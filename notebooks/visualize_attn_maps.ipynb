{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 12 attention map visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from arithmetic_lm.model import TransformerDecoder, generate\n",
    "from arithmetic_lm.tokenizer import CharTokenizer\n",
    "from arithmetic_lm.interp import plot_attn_maps, plot_module\n",
    "from arithmetic_lm.constants import PLOTS_DIR\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path: str) -> tuple[torch.nn.Module, dict]:\n",
    "    # load model\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"mps\")\n",
    "    model = TransformerDecoder(\n",
    "        **ckpt[\"hyper_parameters\"][\"model_hparams\"],\n",
    "        # vocab_size=tokenizer.vocab_size,\n",
    "    )\n",
    "    # state dict has a prefix \"model.\" in the key names\n",
    "    model.load_state_dict({k[6:]: v for k, v in ckpt[\"state_dict\"].items()})\n",
    "    model.eval()\n",
    "    return model, ckpt[\"hyper_parameters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"../checkpoints/addition-generalize-to-longer/trans_dec_6layers_768embd_4head_cot/step670000-train_loss1.4532-val_loss1.4517.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, hparams = load_model(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_names = [\n",
    "    \"transformer_encoder.layers[0].self_attn\",\n",
    "    \"transformer_encoder.layers[1].self_attn\",\n",
    "    \"transformer_encoder.layers[2].self_attn\",\n",
    "    \"transformer_encoder.layers[3].self_attn\",\n",
    "    \"transformer_encoder.layers[4].self_attn\",\n",
    "    \"transformer_encoder.layers[5].self_attn\",\n",
    "]\n",
    "figsize = (14, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: '$123+456=' 3+3\n",
      "true_ans: 579\n",
      "pred_answer: 579$\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of rows must be a positive integer, not 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 209\u001b[0m\n\u001b[1;32m    203\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_maps\n\u001b[0;32m--> 209\u001b[0m attn_maps \u001b[38;5;241m=\u001b[39m \u001b[43mplot_attn_maps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m123\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m456\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfigsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43msavepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfigtitle_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(CoT + finetuned)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreverse_ops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreverse_ans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 197\u001b[0m, in \u001b[0;36mplot_attn_maps\u001b[0;34m(model, tokenizer, a, b, module_names, savepath, pad_zeros, filler_tokens_prompt, save, figsize, reverse_ops, reverse_ans, figtitle_prefix)\u001b[0m\n\u001b[1;32m    191\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstrained\u001b[39m\u001b[38;5;124m\"\u001b[39m, figsize\u001b[38;5;241m=\u001b[39mfigsize)\n\u001b[1;32m    192\u001b[0m fig\u001b[38;5;241m.\u001b[39msuptitle(\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfigtitle_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Attention maps for prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(prompt_str)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(astr)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(bstr)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m predicted answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(pred_answer_str)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mpred_answer_num\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39mtrue_ans\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincorrect, true: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mtrue_ans\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    195\u001b[0m )\n\u001b[0;32m--> 197\u001b[0m subfigs \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubfigures\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattn_maps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (module_name, attn_map) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(attn_maps\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m    199\u001b[0m     plot_module(subfigs[i], module_name, attn_map, ticks)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/matplotlib/figure.py:1585\u001b[0m, in \u001b[0;36mFigureBase.subfigures\u001b[0;34m(self, nrows, ncols, squeeze, wspace, hspace, width_ratios, height_ratios, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubfigures\u001b[39m(\u001b[38;5;28mself\u001b[39m, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, squeeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1546\u001b[0m                wspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1547\u001b[0m                width_ratios\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, height_ratios\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1548\u001b[0m                \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;124;03m    Add a set of subfigures to this figure or subfigure.\u001b[39;00m\n\u001b[1;32m   1551\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;124;03m        If not given, all rows will have the same height.\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1585\u001b[0m     gs \u001b[38;5;241m=\u001b[39m \u001b[43mGridSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mwspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mwidth_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mheight_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbottom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1591\u001b[0m     sfarr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((nrows, ncols), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ncols):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/matplotlib/gridspec.py:378\u001b[0m, in \u001b[0;36mGridSpec.__init__\u001b[0;34m(self, nrows, ncols, figure, left, bottom, right, top, wspace, hspace, width_ratios, height_ratios)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhspace \u001b[38;5;241m=\u001b[39m hspace\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure \u001b[38;5;241m=\u001b[39m figure\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mwidth_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mheight_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight_ratios\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/matplotlib/gridspec.py:48\u001b[0m, in \u001b[0;36mGridSpecBase.__init__\u001b[0;34m(self, nrows, ncols, height_ratios, width_ratios)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    If not given, all rows will have the same height.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(nrows, Integral) \u001b[38;5;129;01mor\u001b[39;00m nrows \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of rows must be a positive integer, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnrows\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ncols, Integral) \u001b[38;5;129;01mor\u001b[39;00m ncols \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of columns must be a positive integer, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mncols\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Number of rows must be a positive integer, not 0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_attention_map(name: str, cache: dict):\n",
    "    def hook(module, inputs, output):\n",
    "        # nn.MultiheadAttention outputs 2 tensors by default:\n",
    "        # - the output of the last linear transformation with shape [bs, tgt_len, embed_dim]\n",
    "        # - the attention map (weights) with shape [bs, n_heads, tgt_len, src_len]\n",
    "        # keeps only last output, which is fine for our purposes\n",
    "        cache[name] = output[1].detach()\n",
    "        print(\"HOOK CALLED\")\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "def set_attn_kwargs_prehook(module, args, kwargs):\n",
    "    \"\"\"\n",
    "    make sure self.attn module is called with need_weights=True and\n",
    "    average_attn_weights=False so that we get per-head attention weights\n",
    "    \"\"\"\n",
    "    kwargs[\"need_weights\"] = True\n",
    "    kwargs[\"average_attn_weights\"] = False\n",
    "    return args, kwargs\n",
    "\n",
    "\n",
    "def generate_hooked(\n",
    "    model: torch.nn.Module,\n",
    "    prompt: torch.Tensor,\n",
    "    stop_token: int,\n",
    "    hook_config: dict[str, dict[str, callable]],\n",
    "    decoder_prompt: torch.Tensor = None,\n",
    ") -> str:\n",
    "    model.eval()\n",
    "\n",
    "    handles = []\n",
    "\n",
    "    for module_name, hook_dict in hook_config.items():\n",
    "        module = eval(f\"model.{module_name}\", {\"model\": model})\n",
    "\n",
    "        if pre_hook := hook_dict.get(\"pre_hook\"):\n",
    "            handles.append(module.register_forward_pre_hook(pre_hook, with_kwargs=True))\n",
    "\n",
    "        if hook := hook_dict.get(\"hook\"):\n",
    "            handles.append(module.register_forward_hook(hook))\n",
    "\n",
    "    # HACK: encode, since just calling generate does not call\n",
    "    # forward hook in the encoder for some weird reason (decoder hooks work fine)\n",
    "    if model.enc_dec:\n",
    "        model.encode(prompt)\n",
    "\n",
    "    pred_tensor = generate(\n",
    "        model,\n",
    "        idx=decoder_prompt if model.enc_dec else prompt,\n",
    "        encoder_source=prompt if model.enc_dec else None,\n",
    "        max_new_tokens=100,\n",
    "        stop_token=stop_token,\n",
    "    )\n",
    "\n",
    "    # remove hooks\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "\n",
    "    return pred_tensor\n",
    "\n",
    "\n",
    "def plot_head(\n",
    "    ax: plt.Axes,\n",
    "    map: torch.Tensor,\n",
    "    title: str,\n",
    "    cmap: str = \"binary\",\n",
    "    xticks: list = None,\n",
    "    yticks: list = None,\n",
    "    colorbar: bool = False,\n",
    "    alpha: float = 1.0,\n",
    "):\n",
    "    ax.imshow(map, cmap=cmap, interpolation=\"none\", alpha=alpha)\n",
    "    if yticks:\n",
    "        ax.set_yticks(np.arange(len(yticks)) - 0.5)\n",
    "        ax.set_yticklabels(yticks, va=\"top\")\n",
    "    if xticks:\n",
    "        ax.set_xticks(np.arange(len(xticks)) - 0.5)\n",
    "        ax.set_xticklabels(xticks, ha=\"left\")\n",
    "    ax.set_title(title)\n",
    "    if colorbar:\n",
    "        ax.figure.colorbar(ax.images[0], ax=ax, shrink=0.3)\n",
    "    # grid\n",
    "    ax.grid(which=\"both\", color=\"k\", linestyle=\":\", linewidth=0.5, alpha=0.5)\n",
    "    ax.set_xlabel(\"source\")\n",
    "    ax.set_ylabel(\"target\")\n",
    "\n",
    "\n",
    "def plot_module(\n",
    "    fig: plt.Figure,\n",
    "    module_name: str,\n",
    "    attn_map: torch.Tensor,\n",
    "    ticks: list[str],\n",
    "    plot_combined: bool = True,\n",
    "):\n",
    "    n_heads = attn_map.shape[1]\n",
    "    axs = fig.subplots(1, n_heads + 1 if plot_combined else n_heads)\n",
    "    fig.suptitle(module_name)\n",
    "\n",
    "    # choose cmaps for combined attn map\n",
    "    cmaps = [\"Reds\", \"Blues\", \"Purples\", \"Greens\", \"Oranges\"]\n",
    "\n",
    "    for i in range(n_heads):\n",
    "        plot_head(\n",
    "            axs[i],\n",
    "            attn_map[0, i],\n",
    "            title=f\"head {i}\",\n",
    "            xticks=ticks,\n",
    "            yticks=ticks,\n",
    "        )\n",
    "        # combined attn map\n",
    "        if plot_combined:\n",
    "            plot_head(\n",
    "                axs[-1],\n",
    "                attn_map[0, i],\n",
    "                title=\"combined\",\n",
    "                cmap=cmaps[i % len(cmaps)],\n",
    "                alpha=0.5,\n",
    "                xticks=ticks,\n",
    "                yticks=ticks,\n",
    "                colorbar=False,\n",
    "            )\n",
    "    # rotate yticks\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis=\"y\", rotation=90)\n",
    "\n",
    "\n",
    "def plot_attn_maps(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer,\n",
    "    a: int,\n",
    "    b: int,\n",
    "    module_names: list[str],\n",
    "    savepath: str,\n",
    "    pad_zeros: int = 0,\n",
    "    filler_tokens_prompt: int = 0,\n",
    "    save: bool = False,\n",
    "    figsize: tuple[int, int] = (8, 8),\n",
    "    reverse_ops: bool = False,\n",
    "    reverse_ans: bool = False,\n",
    "    figtitle_prefix: str = \"\",\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    astr = str(a)\n",
    "    bstr = str(b)\n",
    "\n",
    "    if reverse_ops:\n",
    "        astr = astr[::-1]\n",
    "        bstr = bstr[::-1]\n",
    "\n",
    "    prompt_str = (\n",
    "        f\"${'.' * filler_tokens_prompt}{astr.zfill(pad_zeros)}+{bstr.zfill(pad_zeros)}=\"\n",
    "    )\n",
    "    # prompt_str = \"\\n\" + prompt_str\n",
    "    print(\"prompt:\", repr(prompt_str), f\"{len(astr)}+{len(bstr)}\")\n",
    "    true_ans = str(a + b)\n",
    "    if reverse_ans:\n",
    "        true_ans = true_ans[::-1]\n",
    "    print(\"true_ans:\", true_ans)\n",
    "\n",
    "    prompt = torch.tensor([tokenizer.encode(prompt_str)])\n",
    "    stop_token_id = tokenizer.encode(\"$\")[0]\n",
    "\n",
    "    attn_maps = {}\n",
    "\n",
    "    # generate answer\n",
    "    pred_tensor = generate_hooked(\n",
    "        model,\n",
    "        prompt=prompt,\n",
    "        stop_token=stop_token_id,\n",
    "        hook_config={\n",
    "            mn: {\n",
    "                \"hook\": get_attention_map(mn, attn_maps),\n",
    "                \"pre_hook\": set_attn_kwargs_prehook,\n",
    "            }\n",
    "            for mn in module_names\n",
    "        },\n",
    "    )\n",
    "\n",
    "    pred_answer_str = tokenizer.decode(pred_tensor[0].tolist())\n",
    "    pred_answer_num = \"\".join(c for c in pred_answer_str if c.isdigit())\n",
    "    print(\"pred_answer:\", pred_answer_str)\n",
    "\n",
    "    for mn, matts in attn_maps.items():\n",
    "        print(mn, matts.shape)\n",
    "\n",
    "    # tokens for easier visualization\n",
    "    ticks = list(prompt_str + pred_answer_str)\n",
    "    ticks[0] = \"\\\\n\" if ticks[0] == \"\\n\" else ticks[0]\n",
    "\n",
    "    # for each module, in a subfigure plot heads as subplots\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=figsize)\n",
    "    fig.suptitle(\n",
    "        f\"{figtitle_prefix} Attention maps for prompt: {repr(prompt_str).replace('$', '\\$')}, [{len(astr)}+{len(bstr)}]\"\n",
    "        f\"\\n predicted answer: {repr(pred_answer_str).replace('$', '\\$')} ({'correct' if pred_answer_num == true_ans else 'incorrect, true: ' + true_ans})\",\n",
    "    )\n",
    "\n",
    "    subfigs = fig.subfigures(len(attn_maps), 1, hspace=0, wspace=0)\n",
    "    for i, (module_name, attn_map) in enumerate(attn_maps.items()):\n",
    "        plot_module(subfigs[i], module_name, attn_map, ticks)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(savepath, dpi=90)\n",
    "    plt.show()\n",
    "\n",
    "    return attn_maps\n",
    "\n",
    "\n",
    "attn_maps = plot_attn_maps(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    a=123,\n",
    "    b=456,\n",
    "    module_names=module_names,\n",
    "    figsize=figsize,\n",
    "    savepath=str(\"\"),\n",
    "    figtitle_prefix=\"(CoT + finetuned)\",\n",
    "    reverse_ops=False,\n",
    "    reverse_ans=False,\n",
    "    save=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_answer(model: torch.nn.Module, tokenizer, prompt: str, answer: str) -> bool:\n",
    "#     \"\"\"Return whether the model predicts the correct answer.\"\"\"\n",
    "\n",
    "#     prompt_tokens = torch.tensor(tokenizer.encode(prompt))\n",
    "#     stop_token_id = tokenizer.encode(\"$\")[0]\n",
    "\n",
    "#     pred_ans = generate(\n",
    "#         model, idx=prompt_tokens, max_new_tokens=20, stop_token=stop_token_id\n",
    "#     )\n",
    "\n",
    "#     pred_ans = tokenizer.decode(pred_ans[0])\n",
    "#     pred_ans = pred_ans.strip(\"$\")\n",
    "#     return pred_ans == answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find failure cases\n",
    "# while True:\n",
    "#     a = random.randint(10**5, 10**6)\n",
    "#     b = random.randint(10**5, 10**6)\n",
    "#     prompt = f\"${a}+{b}=\"\n",
    "#     true_ans = str(a + b)\n",
    "#     if not eval_answer(model_before, tokenizer, prompt, true_ans) and not eval_answer(\n",
    "#         model_after, tokenizer, prompt, true_ans\n",
    "#     ):\n",
    "#         break\n",
    "\n",
    "# print(f\"prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdir = PLOTS_DIR / \"exp_15\"\n",
    "subdir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: '$123456+678901=' 6+6\n",
      "true_ans: 802357\n",
      "pred_answer: 802357$\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of rows must be a positive integer, not 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m savepath \u001b[38;5;241m=\u001b[39m subdir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp15_attention_maps_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_cot_finetuned.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m      4\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      5\u001b[0m     a\u001b[38;5;241m=\u001b[39ma,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# save=True,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m attn_maps \u001b[38;5;241m=\u001b[39m \u001b[43mplot_attn_maps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msavepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msavepath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfigtitle_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(CoT + finetuned)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreverse_ops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreverse_ans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/studies/thesis/msc_thesis/arithmetic_lm/interp/attn_maps.py:145\u001b[0m, in \u001b[0;36mplot_attn_maps\u001b[0;34m(model, tokenizer, a, b, module_names, savepath, pad_zeros, filler_tokens_prompt, save, figsize, reverse_ops, reverse_ans, figtitle_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstrained\u001b[39m\u001b[38;5;124m\"\u001b[39m, figsize\u001b[38;5;241m=\u001b[39mfigsize)\n\u001b[1;32m    140\u001b[0m fig\u001b[38;5;241m.\u001b[39msuptitle(\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfigtitle_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Attention maps for prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(prompt_str)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(astr)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(bstr)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m predicted answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(pred_answer_str)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mpred_answer_num\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39mtrue_ans\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincorrect, true: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mtrue_ans\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    143\u001b[0m )\n\u001b[0;32m--> 145\u001b[0m subfigs \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubfigures\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattn_maps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (module_name, attn_map) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(attn_maps\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m    147\u001b[0m     plot_module(subfigs[i], module_name, attn_map, ticks)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/matplotlib/figure.py:1585\u001b[0m, in \u001b[0;36mFigureBase.subfigures\u001b[0;34m(self, nrows, ncols, squeeze, wspace, hspace, width_ratios, height_ratios, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubfigures\u001b[39m(\u001b[38;5;28mself\u001b[39m, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, squeeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1546\u001b[0m                wspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1547\u001b[0m                width_ratios\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, height_ratios\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1548\u001b[0m                \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;124;03m    Add a set of subfigures to this figure or subfigure.\u001b[39;00m\n\u001b[1;32m   1551\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;124;03m        If not given, all rows will have the same height.\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1585\u001b[0m     gs \u001b[38;5;241m=\u001b[39m \u001b[43mGridSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mwspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mwidth_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mheight_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbottom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1591\u001b[0m     sfarr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((nrows, ncols), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ncols):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/matplotlib/gridspec.py:378\u001b[0m, in \u001b[0;36mGridSpec.__init__\u001b[0;34m(self, nrows, ncols, figure, left, bottom, right, top, wspace, hspace, width_ratios, height_ratios)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhspace \u001b[38;5;241m=\u001b[39m hspace\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure \u001b[38;5;241m=\u001b[39m figure\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mwidth_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mheight_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight_ratios\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/matplotlib/gridspec.py:48\u001b[0m, in \u001b[0;36mGridSpecBase.__init__\u001b[0;34m(self, nrows, ncols, height_ratios, width_ratios)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    If not given, all rows will have the same height.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(nrows, Integral) \u001b[38;5;129;01mor\u001b[39;00m nrows \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of rows must be a positive integer, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnrows\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ncols, Integral) \u001b[38;5;129;01mor\u001b[39;00m ncols \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of columns must be a positive integer, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mncols\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Number of rows must be a positive integer, not 0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a, b = 123456, 678901\n",
    "savepath = subdir / f\"exp15_attention_maps_{a}+{b}_cot_finetuned.png\"\n",
    "kwargs = dict(\n",
    "    tokenizer=tokenizer,\n",
    "    a=a,\n",
    "    b=b,\n",
    "    module_names=module_names,\n",
    "    figsize=figsize,\n",
    "    # save=True,\n",
    ")\n",
    "attn_maps = plot_attn_maps(\n",
    "    model=model,\n",
    "    savepath=str(savepath),\n",
    "    figtitle_prefix=\"(CoT + finetuned)\",\n",
    "    reverse_ops=False,\n",
    "    reverse_ans=False,\n",
    "    **kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis-P7I560r2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
