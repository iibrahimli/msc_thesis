{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit and pos encoding intervention\n",
    "\n",
    "Force models to output past end-of-sequence token (`$`) be modyfing its logit directly, or by shifting the positional encoding indices of the answer generated so far to force. Aim is to see if they can correctly predict the answer digits in OOD positions. If yes, then it could be that the position-dependent circuit decides to stop the generation by outputting the EOS token and not the addition circuit itself failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from arithmetic_lm.model import (\n",
    "    TransformerDecoder,\n",
    "    load_model,\n",
    "    find_latest_ckpt,\n",
    "    generate,\n",
    ")\n",
    "from arithmetic_lm.tokenizer import CharTokenizer\n",
    "from arithmetic_lm.interp import plot_attn_maps\n",
    "from arithmetic_lm.constants import PLOTS_DIR, CHECKPOINTS_DIR\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer()\n",
    "stop_token = tokenizer.encode(\"$\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (embedding): Embedding(100, 768)\n",
       "  (pos_encoder): AbsolutePositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "ckpt_path = \"../checkpoints/addition-generalize-to-longer/trans_dec_6layers_768embd_4head_randsp0.5_rev_ansloss/step1000000-train_loss0.0002-val_loss0.0000.ckpt\"\n",
    "model, hparams = load_model(ckpt_path)\n",
    "model.to(\"mps\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:           $659876549321987654999999999999+599876547321987654999999999999= | 30+30\n",
      "True answer:      1597531967428753198999999999991 | 31\n",
      "Predicted answer: 1597874300087540040040400 | 25\n",
      "Carries:          111111101000111110111111111111\n",
      "Correct:          False\n",
      "\n",
      "======\n",
      "\n",
      "Prompt: $659876549321987654999999999999+599876547321987654999999999999=1597531967428753\n",
      "probs:\n",
      "  4: 0.27043\n",
      "  9: 0.15192\n",
      "  5: 0.12490\n",
      "  6: 0.08004\n",
      "  8: 0.07846\n",
      "  3: 0.07484\n",
      "  7: 0.05977\n",
      "  2: 0.05890\n",
      "  0: 0.05427\n",
      "  1: 0.04066\n",
      "stop token:\n",
      "  $: 0.00001\n"
     ]
    }
   ],
   "source": [
    "a = \"999999999999456789123945678956\"\n",
    "b = \"999999999999456789123745678995\"\n",
    "prompt = f\"${a[::-1]}+{b[::-1]}=\"\n",
    "true_ans = str(eval(f\"{a}+{b}\"))[::-1]\n",
    "\n",
    "carries = \"\"\n",
    "for ai, bi in zip(a[::-1], b[::-1]):\n",
    "    if int(ai) + int(bi) >= 10:\n",
    "        carries += \"1\"\n",
    "    else:\n",
    "        carries += \"0\"\n",
    "\n",
    "prompt_idx = torch.tensor(tokenizer.encode(prompt, return_tensors=True)).to(\"mps\")\n",
    "\n",
    "pred_tensor = generate(\n",
    "    model,\n",
    "    idx=prompt_idx,\n",
    "    max_new_tokens=25,\n",
    "    stop_token=stop_token,\n",
    ")\n",
    "pred = tokenizer.decode(pred_tensor[0])\n",
    "\n",
    "len_a, len_b = map(\n",
    "    len, prompt.replace(\"$\", \"\").replace(\"=\", \"\").replace(\" \", \"\").split(\"+\")\n",
    ")\n",
    "print(f\"Prompt:           {prompt} | {len_a}+{len_b}\")\n",
    "print(f\"True answer:      {true_ans} | {len(true_ans)}\")\n",
    "print(f\"Predicted answer: {pred} | {len(pred.replace('$', ''))}\")\n",
    "print(f\"Carries:          {carries}\")\n",
    "print(f\"Correct:          {pred.replace('$', '') == true_ans}\")\n",
    "\n",
    "\n",
    "# logit-level intervention\n",
    "print(\"\\n======\\n\")\n",
    "\n",
    "ans_prompt = prompt + \"1597531967428753\"\n",
    "print(f\"Prompt: {ans_prompt}\")\n",
    "\n",
    "ans_prompt_idx = torch.tensor(tokenizer.encode(ans_prompt, return_tensors=True)).to(\n",
    "    \"mps\"\n",
    ")\n",
    "pred_logits = model(ans_prompt_idx.unsqueeze(0))\n",
    "\n",
    "# print sorted logits and their corresponding tokens\n",
    "print(\"probs:\")\n",
    "logits, indices = torch.sort(pred_logits[0, -1], descending=True)\n",
    "probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"  {tokenizer.decode([indices[i].item()])}: {probs[i].item():.5f}\")\n",
    "\n",
    "# prob of stop token ($)\n",
    "print(\"stop token:\")\n",
    "print(f\"  {tokenizer.decode([stop_token])}: {probs[stop_token].item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(WIP) Hypothesis: for 18 digits OOD it might work (?), but for 20 the model is \"approximating\" the last digits i.e. \n",
    "```\n",
    "Prompt:           $12345678901234567890+92345678901234567899= | 20+20\n",
    "True answer:      104691357802469135789 | 21\n",
    "Predicted answer: 10469135780246913569$ | 20\n",
    "```\n",
    "it predicts ...3569 instead of ...35789, closest in-distribution length is 19 and that would end in ...357, which is approximated to ...369. During training it succesively approximates more and more digits, but there is no pressure to correctly get last OOD digits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis-P7I560r2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
