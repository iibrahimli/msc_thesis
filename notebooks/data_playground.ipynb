{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing data pipeline\n",
    "\n",
    "Get a dataloader based on given experiment config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from hydra import initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from arithmetic_lm.dataset import DATASET_CLASSES\n",
    "from arithmetic_lm.tokenizer import CharTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training.only_answer_loss=true', 'data.format.operand_random_spaces_amount=0.3', '+experiment=15/exp15_trans_dec.yaml', 'data=add_generalize_to_longer_19']\n",
      "data:\n",
      "  train_ds_class: ArithmeticLMSequenceDataset\n",
      "  format:\n",
      "    pad: $\n",
      "    pad_ops_zero: null\n",
      "    pad_ans_zero: null\n",
      "    reverse_ops: false\n",
      "    reverse_ans: false\n",
      "    encdec: false\n",
      "    filler_tokens_prompt: 0\n",
      "    filler_tokens_ans: 0\n",
      "    operand_random_spaces_amount: 0.3\n",
      "    answer_random_spaces_amount: 0\n",
      "    scratchpad: false\n",
      "  train: data/addition/generalize_to_longer_19/train_add_1-19_except18_1M.txt\n",
      "  test:\n",
      "    in_dist: data/addition/generalize_to_longer_19/test_add_in_distribution_2000.txt\n",
      "    ood_18digit: data/addition/generalize_to_longer_19/test_add_ood_18digit_100.txt\n",
      "    ood_20digit: data/addition/generalize_to_longer_19/test_add_ood_20digit_100.txt\n",
      "    ood_21digit: data/addition/generalize_to_longer_19/test_add_ood_21digit_100.txt\n",
      "    ood_22digit: data/addition/generalize_to_longer_19/test_add_ood_22digit_100.txt\n",
      "    ood_23digit: data/addition/generalize_to_longer_19/test_add_ood_23digit_100.txt\n",
      "model:\n",
      "  name: TransformerDecoder\n",
      "  args:\n",
      "    context_len: 256\n",
      "    n_embd: 768\n",
      "    n_head: 4\n",
      "    n_layers: 6\n",
      "    dropout: 0.1\n",
      "    pos_enc: abs\n",
      "    pos_enc_max_shift: 0\n",
      "    emb_type: learned\n",
      "tokenizer:\n",
      "  name: CharTokenizer\n",
      "  args: {}\n",
      "sampling:\n",
      "  temp: 1.0\n",
      "  top_k: 1\n",
      "training:\n",
      "  batch_size: 256\n",
      "  lr: 0.0003\n",
      "  weight_decay: 0.1\n",
      "  warmup_iters: 100\n",
      "  max_iters: 300000\n",
      "  num_dl_workers: 0\n",
      "  val_ratio: 0.1\n",
      "  val_interval: 2000\n",
      "  limit_val_batches: null\n",
      "  limit_test_examples: 1000\n",
      "  reload_dataloaders_every_n_epochs: 0\n",
      "  only_answer_loss: true\n",
      "  devices:\n",
      "  - 0\n",
      "  accumulate_grad_batches: 1\n",
      "  resume_ckpt_path: null\n",
      "  ckpt_weights_only: false\n",
      "  eval_func: numeric\n",
      "  pause_token: .\n",
      "wandb:\n",
      "  enabled: true\n",
      "  entity: compositional-generalization-ut\n",
      "  project: addition-generalize-to-longer\n",
      "  run_name: trans_dec_${model.args.n_layers}layers_${model.args.n_embd}embd_${model.args.n_head}head${.suffix}\n",
      "  grad_log_interval: 100\n",
      "  suffix: ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT = \"15/exp15_trans_dec.yaml\"\n",
    "DATASET = \"add_generalize_to_longer_19\"\n",
    "abs_config_dir = Path(\"..\").resolve() / \"arithmetic_lm/conf\"\n",
    "\n",
    "# add overrides if needed\n",
    "overrides = [\n",
    "    \"training.only_answer_loss=true\",\n",
    "    \"data.format.operand_random_spaces_amount=0.3\",\n",
    "]\n",
    "\n",
    "if EXPERIMENT:\n",
    "    overrides.append(f\"+experiment={EXPERIMENT}\")\n",
    "if DATASET:\n",
    "    overrides.append(f\"data={DATASET}\")\n",
    "print(overrides)\n",
    "\n",
    "with initialize_config_dir(version_base=None, config_dir=str(abs_config_dir)):\n",
    "    cfg = compose(config_name=\"train.yaml\", overrides=overrides)\n",
    "    print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<arithmetic_lm.dataset.arithmetic_dataset.ArithmeticLMSequenceDataset object at 0x130d26e40> 896200 (tensor([65,  0, 72,  0, 80,  0]), tensor([ 0, 72,  0, 80,  0, 65]))\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CharTokenizer()\n",
    "\n",
    "# dataset\n",
    "ds_kwargs = {\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"seq_len\": cfg.model.args.context_len,\n",
    "    \"pad\": cfg.data.format.pad,\n",
    "    \"pad_ops_zero\": cfg.data.format.pad_ops_zero,\n",
    "    \"pad_ans_zero\": cfg.data.format.pad_ans_zero,\n",
    "    \"reverse_ops\": cfg.data.format.reverse_ops,\n",
    "    \"reverse_ans\": cfg.data.format.reverse_ans,\n",
    "    \"filler_tokens_prompt\": cfg.data.format.filler_tokens_prompt,\n",
    "    \"filler_tokens_ans\": cfg.data.format.filler_tokens_ans,\n",
    "    \"equal_in_prompt\": not cfg.data.format.encdec,\n",
    "    \"scratchpad\": cfg.data.format.get(\"scratchpad\", False),\n",
    "    \"operand_random_spaces_amount\": cfg.data.format.get(\n",
    "        \"operand_random_spaces_amount\", 0\n",
    "    ),\n",
    "}\n",
    "ds_class = DATASET_CLASSES[cfg.data.train_ds_class]\n",
    "ds = ds_class(Path(\"..\").resolve() / cfg.data.train, **ds_kwargs)\n",
    "print(ds, len(ds), ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    ds, batch_size=4, shuffle=True, num_workers=0, collate_fn=ds.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a batch of data and do whatever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 62])\n",
      "'6536776816 8+3733  8784 039=102706552207$\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c'\n",
      "'1  38824+301959=440783$\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c'\n",
      "'2425+2887=5312$\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c'\n",
      "'944 60 9009978 41453+516100  87449437 93 2=146070988447279385$'\n",
      "AFTER\n",
      "torch.Size([4, 62])\n",
      "'...........................=102706552207$.....................'\n",
      "'...............=440783$.......................................'\n",
      "'.........=5312$...............................................'\n",
      "'..........................................=146070988447279385$'\n"
     ]
    }
   ],
   "source": [
    "from arithmetic_lm.model.utils import answer_mask\n",
    "\n",
    "for i, batch in enumerate(dl):\n",
    "    tgt = batch[1]\n",
    "    print(tgt.shape)\n",
    "    # decode and print tgt before\n",
    "    for t in tgt:\n",
    "        print(repr(tokenizer.decode(t)))\n",
    "\n",
    "    answer_masked_tgt = answer_mask(\n",
    "        tgt,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        equal_token_id=tokenizer.encode(\"=\")[0],\n",
    "    )\n",
    "\n",
    "    print(\"AFTER\")\n",
    "    print(answer_masked_tgt.shape)\n",
    "\n",
    "    # decode and print tgt after\n",
    "    for t in answer_masked_tgt:\n",
    "        print(repr(tokenizer.decode(t).replace(\"\\x0c\", \".\")))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 params: 163.09M\n"
     ]
    }
   ],
   "source": [
    "# for GPT2\n",
    "context_len = 1024\n",
    "n_tokens = 50257\n",
    "hidden_dim = 768\n",
    "n_layers = 12\n",
    "mlp_mult = 4\n",
    "\n",
    "n_params = transformer_decoder_param_count(\n",
    "    context_len, n_tokens, hidden_dim, n_layers, mlp_mult\n",
    ")\n",
    "\n",
    "print(f\"GPT2 params: {n_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis-P7I560r2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
