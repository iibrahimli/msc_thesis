{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from arithmetic_lm.model import load_model, generate\n",
    "from arithmetic_lm.tokenizer import CharTokenizer\n",
    "from arithmetic_lm.constants import PLOTS_DIR, CHECKPOINTS_DIR\n",
    "from arithmetic_lm.utils import get_carry_str\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.style.use(\"../figure.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer()\n",
    "stop_token = tokenizer.encode(\"$\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(\n",
    "    ckpt_path: str, model_class: str = \"TransformerDecoder\", map_location: str = \"mps\"\n",
    ") -> tuple[torch.nn.Module, dict]:\n",
    "    from arithmetic_lm.model import MODELS\n",
    "\n",
    "    # load model\n",
    "    ckpt = torch.load(ckpt_path, map_location=map_location)\n",
    "    model = MODELS[model_class](\n",
    "        **ckpt[\"hyper_parameters\"][\"model_hparams\"],\n",
    "        # vocab_size=tokenizer.vocab_size,\n",
    "    )\n",
    "    # TODO HACK remove transformer_encoder from prefix\n",
    "    ckpt[\"state_dict\"] = {\n",
    "        k.replace(\"transformer_encoder.\", \"\"): v for k, v in ckpt[\"state_dict\"].items()\n",
    "    }\n",
    "\n",
    "    # state dict has a prefix \"model.\" in the key names\n",
    "    model.load_state_dict(\n",
    "        {k[6:]: v for k, v in ckpt[\"state_dict\"].items() if k.startswith(\"model.\")},\n",
    "        strict=False,\n",
    "    )\n",
    "    model.eval()\n",
    "    return model, ckpt[\"hyper_parameters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (embedding): Embedding(100, 768)\n",
       "  (pos_encoder): AbsolutePositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "ckpt_path = \"../checkpoints/addition-generalize-to-longer/trans_dec_6layers_768embd_4head_randsp0.5_rev_ansloss/step1000000-train_loss0.0002-val_loss0.0000.ckpt\"\n",
    "model, hparams = load_model(ckpt_path)\n",
    "model.to(\"mps\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pad': '$', 'pad_ops_zero': None, 'pad_ans_zero': None, 'reverse_ops': True, 'reverse_ans': True, 'encdec': False, 'filler_tokens_prompt': 0, 'filler_tokens_ans': 0, 'operand_random_spaces_amount': 0.5, 'chain_of_thought': False}\n"
     ]
    }
   ],
   "source": [
    "print(hparams[\"extra_hparams\"][\"data_format\"])\n",
    "reverse_ops = hparams[\"extra_hparams\"][\"data_format\"][\"reverse_ops\"]\n",
    "reverse_ans = hparams[\"extra_hparams\"][\"data_format\"][\"reverse_ans\"]\n",
    "\n",
    "model_name = \"trans_dec_6layers_768embd_4head_randsp0.5_rev_ansloss\"\n",
    "model_max_trained_answer_digit_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:05<00:05,  1.05s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing tensors could not be broadcast together with shapes [28], [27]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Calculate the probability of the correct token at each position\u001b[39;00m\n\u001b[1;32m     77\u001b[0m correct_token_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(ans_str, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m correct_token_prob_seq \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mprobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_generated_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect_token_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_generated_tokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Pad correct token probability sequence\u001b[39;00m\n\u001b[1;32m     85\u001b[0m correct_token_prob_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(correct_token_prob_seq)\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing tensors could not be broadcast together with shapes [28], [27]"
     ]
    }
   ],
   "source": [
    "digit_length = 25\n",
    "n_samples = 10\n",
    "seed = 1337\n",
    "\n",
    "entropies = []\n",
    "eos_probs = []\n",
    "correct_token_probs = []\n",
    "\n",
    "eos_token_id = tokenizer.encode(\"$\")[0]\n",
    "\n",
    "max_new_tokens = digit_length + 5\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "for _ in tqdm(range(n_samples)):\n",
    "    # Generate random numbers\n",
    "    a = random.randint(10 ** (digit_length - 1), 10**digit_length - 1)\n",
    "    b = random.randint(10 ** (digit_length - 1), 10**digit_length - 1)\n",
    "\n",
    "    # Reverse operands if needed\n",
    "    if reverse_ops:\n",
    "        a_str = str(a)[::-1]\n",
    "        b_str = str(b)[::-1]\n",
    "    else:\n",
    "        a_str = str(a)\n",
    "        b_str = str(b)\n",
    "\n",
    "    prompt = f\"${a_str}+{b_str}=\"\n",
    "\n",
    "    # Encode prompt\n",
    "    prompt_idx = tokenizer.encode(prompt, return_tensors=True).to(\"mps\")\n",
    "\n",
    "    # Generate the true answer\n",
    "    ans = a + b\n",
    "    if reverse_ans:\n",
    "        ans_str = str(ans)[::-1]\n",
    "    else:\n",
    "        ans_str = str(ans)\n",
    "    ans_str += \"$\"\n",
    "\n",
    "    # Generate model output with logits\n",
    "    pred_idx, pred_logits = generate(\n",
    "        model,\n",
    "        prompt_idx,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        stop_token=eos_token_id,\n",
    "        return_logits=True,\n",
    "    )\n",
    "\n",
    "    pred_logits = torch.tensor(pred_logits).to(\"mps\")\n",
    "\n",
    "    # Number of generated tokens\n",
    "    num_generated_tokens = pred_idx.shape[1]\n",
    "\n",
    "    # Now compute probabilities and entropy\n",
    "    probs = torch.softmax(\n",
    "        pred_logits, dim=-1\n",
    "    )  # Shape: (num_generated_tokens, vocab_size)\n",
    "\n",
    "    # Calculate entropy for each token\n",
    "    entropy = -(probs * torch.log(probs + 1e-12)).sum(\n",
    "        dim=-1\n",
    "    )  # Shape: (num_generated_tokens,)\n",
    "\n",
    "    # Pad entropy sequence to max_new_tokens\n",
    "    entropy_seq = entropy.cpu().numpy()\n",
    "    entropy_padded = np.full((max_new_tokens,), np.nan)\n",
    "    entropy_padded[:num_generated_tokens] = entropy_seq\n",
    "\n",
    "    # Append entropy sequence to list\n",
    "    entropies.append(entropy_padded)\n",
    "\n",
    "    # Extract logit for EOS token\n",
    "    eos_prob_seq = probs[:, eos_token_id].cpu().numpy()\n",
    "    eos_prob_padded = np.full((max_new_tokens,), np.nan)\n",
    "    eos_prob_padded[:num_generated_tokens] = eos_prob_seq\n",
    "    eos_probs.append(eos_prob_padded)\n",
    "\n",
    "    # Calculate the probability of the correct token at each position\n",
    "    correct_token_ids = tokenizer.encode(ans_str, return_tensors=True).to(\"mps\")\n",
    "    correct_token_prob_seq = (\n",
    "        probs[range(num_generated_tokens), correct_token_ids[:num_generated_tokens]]\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    # Pad correct token probability sequence\n",
    "    correct_token_prob_seq = np.array(correct_token_prob_seq)\n",
    "    correct_token_prob_padded = np.full((max_new_tokens,), np.nan)\n",
    "    correct_token_prob_padded[:num_generated_tokens] = correct_token_prob_seq\n",
    "\n",
    "    # Append correct token probabilities to list\n",
    "    correct_token_probs.append(correct_token_prob_padded)\n",
    "\n",
    "# Now compute mean and std over entropies, EOS logits, and correct token probabilities, ignoring NaNs\n",
    "entropies = np.array(entropies)  # Shape: (n_samples, max_new_tokens)\n",
    "eos_probs = np.array(eos_probs)  # Shape: (n_samples, max_new_tokens)\n",
    "correct_token_probs = np.array(\n",
    "    correct_token_probs\n",
    ")  # Shape: (n_samples, max_new_tokens)\n",
    "\n",
    "mean_entropy = np.nanmean(entropies, axis=0)\n",
    "std_entropy = np.nanstd(entropies, axis=0)\n",
    "\n",
    "mean_eos_prob = np.nanmean(eos_probs, axis=0)\n",
    "std_eos_prob = np.nanstd(eos_probs, axis=0)\n",
    "\n",
    "mean_correct_token_prob = np.nanmean(correct_token_probs, axis=0)\n",
    "std_correct_token_prob = np.nanstd(correct_token_probs, axis=0)\n",
    "\n",
    "# Plot mean entropy with shaded std deviation\n",
    "positions = range(1, max_new_tokens + 1)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# set xticks\n",
    "ax1.set_xticks(positions)\n",
    "\n",
    "ax1.set_xlabel(\"Token position in generated sequence\")\n",
    "ax1.set_ylabel(\"Entropy\")\n",
    "(entropy_line,) = ax1.plot(\n",
    "    positions, mean_entropy, \"-o\", label=\"Mean entropy\", color=\"blue\"\n",
    ")\n",
    "entropy_fill = ax1.fill_between(\n",
    "    positions,\n",
    "    np.clip(mean_entropy - std_entropy, 0, None),\n",
    "    mean_entropy + std_entropy,\n",
    "    color=\"blue\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Add EOS probabilities on second y-axis (ax2)\n",
    "ax2 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis\n",
    "ax2.set_ylabel(\"Probability\")  # We already handled the x-label with ax1\n",
    "(eos_line,) = ax2.plot(\n",
    "    positions, mean_eos_prob, \"-o\", label=\"Mean EOS prob\", color=\"red\"\n",
    ")\n",
    "eos_fill = ax2.fill_between(\n",
    "    positions,\n",
    "    np.clip(mean_eos_prob - std_eos_prob, 0, None),\n",
    "    np.clip(mean_eos_prob + std_eos_prob, None, 1),\n",
    "    color=\"red\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Plot mean probability of correct token on same y-axis (ax1)\n",
    "(correct_token_line,) = ax2.plot(\n",
    "    positions,\n",
    "    mean_correct_token_prob,\n",
    "    \"-o\",\n",
    "    label=\"Mean correct token prob\",\n",
    "    color=\"green\",\n",
    ")\n",
    "correct_token_fill = ax2.fill_between(\n",
    "    positions,\n",
    "    np.clip(mean_correct_token_prob - std_correct_token_prob, 0, None),\n",
    "    np.clip(mean_correct_token_prob + std_correct_token_prob, None, 1),\n",
    "    color=\"green\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# show max answer digit length where the model was trained\n",
    "ax1.axvline(\n",
    "    model_max_trained_answer_digit_len + 1,\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Max answer digit length in training data ({model_max_trained_answer_digit_len})\",\n",
    ")\n",
    "\n",
    "\n",
    "# Combine legends from both axes\n",
    "lines = [entropy_line, correct_token_line, eos_line]\n",
    "fills = [entropy_fill, correct_token_fill, eos_fill]\n",
    "labels = [\"Mean entropy\", \"Mean correct token prob\", \"Mean EOS prob\"]\n",
    "ax1.legend(lines, labels, loc=\"center left\")\n",
    "\n",
    "plt.title(\n",
    "    f\"Entropy, EOS probability, and correct token probability over sequence ({digit_length} digits, {n_samples} samples)\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis-P7I560r2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
