{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial experiment: small vanilla transformer (NanoGPT) trained on plain addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from arithmetic_lm.tokenizer import CharTokenizer, Tokenizer\n",
    "from arithmetic_lm.utils import get_torch_device, set_seed\n",
    "from arithmetic_lm.constants import DATA_DIR, ROOT_DIR, CHECKPOINTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "DEVICE = get_torch_device()\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "set_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "N_LAYERS = 6\n",
    "N_HEAD = 6\n",
    "N_EMBD = 384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoGPT(nn.Module):\n",
    "    \"\"\"Simple small decoder-only transformer model using nn.TransformerDecoder.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_len: int,\n",
    "        n_embd: int,\n",
    "        n_head: int,\n",
    "        n_layers: int,\n",
    "        vocab_size: int,\n",
    "        ff_factor: int = 4,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            context_len: context length, i.e. the number of expected features in the input\n",
    "            n_embd: dimensionality of model embeddings\n",
    "            n_head: number of heads in the multi-head attention\n",
    "            n_layers: number of nn.TransformerDecoderLayer layers\n",
    "            vocab_size: size of the vocabulary\n",
    "            ff_factor: factor by which to scale the hidden layer dimensionality in the feedforward layer\n",
    "            dropout: dropout probability\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ff_factor = ff_factor\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_encoder = PositionalEncoding(n_embd, max_len=context_len, dropout=dropout)\n",
    "\n",
    "        # same as decoder layer essentially, but without cross attention\n",
    "        self.layer = nn.TransformerEncoderLayer(\n",
    "            d_model=n_embd,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=n_embd * ff_factor,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.layer,\n",
    "            num_layers=n_layers,\n",
    "            norm=nn.LayerNorm(n_embd),\n",
    "        )\n",
    "\n",
    "        # output to vocab dim\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        # weight tying\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "\n",
    "        # TODO init weights\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len]``\n",
    "        \n",
    "        Returns:\n",
    "            logits: Tensor, shape ``[batch_size, seq_len, vocab_size]``\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        x = self.transformer_encoder(\n",
    "            x,\n",
    "            is_causal=True,\n",
    "            mask=nn.Transformer.generate_square_subsequent_mask(\n",
    "                self.context_len, device=x.device\n",
    "            ),\n",
    "        )\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "    def param_count(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate(\n",
    "        self,\n",
    "        idx: Tensor,\n",
    "        max_new_tokens: int,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int = None,\n",
    "        stop_token: int = None,\n",
    "        seed: int = 42,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (tensor of shape [batch, seq_len]) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(idx, torch.Tensor), \"idx must be a torch.Tensor\"\n",
    "        assert idx.dim() == 2, \"idx must be a 2D tensor of shape [batch, seq_len]\"\n",
    "        assert idx.size(1) <= self.context_len, \"sequence length must be <= context_len\"\n",
    "        assert idx.size(0) == 1, \"only batch_size=1 is supported for now\"\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop to context_len if necessary\n",
    "            idx_cond = idx if idx.size(1) <= self.context_len else idx[:, -self.context_len:]\n",
    "\n",
    "            # logits shape: [batch, seq_len, vocab_size]\n",
    "            logits = self.forward(idx_cond)\n",
    "\n",
    "            # get logits at final step and apply temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            # optionally apply top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "            # apply softmax\n",
    "            probs = nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "            # sample from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1, generator=torch.Generator(device=DEVICE).manual_seed(seed))\n",
    "\n",
    "            # append to the sequence\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "            # stop if stop_token is generated\n",
    "            if stop_token is not None and next_token.item() == stop_token:\n",
    "                break\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test one batch overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([17, 14, 21, 21, 24, 94, 32, 24, 27, 21, 13], 'len:', 11, 'hello world')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello world\"\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens, \"len:\", len(tokens), tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to tensor\n",
    "tokens = torch.tensor(tokens).unsqueeze(0).to(DEVICE)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try overfitting on one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = NanoGPT(\n",
    "#     context_len=SEQ_LEN,\n",
    "#     n_embd=N_EMBD,\n",
    "#     n_head=N_HEAD,\n",
    "#     n_layers=N_LAYERS,\n",
    "#     vocab_size=tokenizer.vocab_size,\n",
    "# ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # simplest train loop\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# # create target by shifting tokens by 1 and adding a padding token at the end\n",
    "# target = torch.cat([tokens[0, 1:], torch.tensor([65]).to(DEVICE)]).unsqueeze(0)\n",
    "# test_text = \"hel\"\n",
    "# test_tokens = tokenizer.encode(test_text)\n",
    "# test_tokens = torch.tensor(test_tokens).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "# losses = []\n",
    "\n",
    "# for i in range(10000):\n",
    "#     optimizer.zero_grad()\n",
    "#     y = net(tokens)\n",
    "#     # y shape: (batch_size, seq_len, vocab_size)\n",
    "#     loss = criterion(y.view(-1, y.size(-1)), target.view(-1))\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     losses.append(loss.item())\n",
    "\n",
    "#     if i % 100 == 0:\n",
    "#         print(f\"[{i}] loss: {loss.item():.5f}  \", test_text + \" -> \" + tokenizer.decode(net.generate(test_tokens, max_new_tokens=10).squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.plot(losses)\n",
    "# plt.xlabel(\"iteration\")\n",
    "# plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prompt = \"hello w\"\n",
    "# tokens = tokenizer.encode(test_prompt)\n",
    "# tokens = torch.tensor(tokens).unsqueeze(0).to(DEVICE)\n",
    "# print(tokens.shape)\n",
    "\n",
    "# net.eval()\n",
    "# generated_tokens = net.generate(tokens, max_new_tokens=10, temperature=1.0, top_k=1)\n",
    "# print(generated_tokens.shape)\n",
    "\n",
    "# tokenizer.decode(generated_tokens.squeeze(0).cpu().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArithmeticDataset(Dataset):\n",
    "    \"\"\"Concatenate lines in file and split into sequences of length seq_len.\"\"\"\n",
    "    # TODO transforms (adding $, formatting, reversing)\n",
    "    def __init__(self, txtfile: str | Path, tokenizer: Tokenizer, seq_len: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        with open(txtfile, \"r\") as f:\n",
    "            text = f.read()\n",
    "        self.n_examples = text.count(\"\\n\")\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        # make seqs of same length (truncate if necessary)\n",
    "        n_seqs = len(tokens) // seq_len\n",
    "        self.seqs = [tokens[i*seq_len:(i+1)*seq_len] for i in range(n_seqs)]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tensor:\n",
    "        # return tensors\n",
    "        return torch.tensor(self.seqs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArithmeticEvalDataset(Dataset):\n",
    "    \"\"\"Dataset but instead of pure language modeling, we want to evaluate each example (line)\"\"\"\n",
    "    # TODO transforms (adding $, formatting, reversing)\n",
    "    def __init__(self, txtfile: str | Path, tokenizer: Tokenizer, seq_len: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        with open(txtfile, \"r\") as f:\n",
    "            text = f.readlines()\n",
    "        self.n_examples = len(text)\n",
    "        # generate answers\n",
    "        answers = [eval(l.split(\"=\")[0]) for l in text]\n",
    "        lines = [\"\\n\" + l.strip() for l in text]\n",
    "        self.prompts = [self.tokenizer.encode(l)[:seq_len] for l in lines]\n",
    "        self.answers = [self.tokenizer.encode(str(a))[:seq_len] for a in answers]\n",
    "        assert len(self.prompts) == len(self.answers), \"prompts and answers length mismatch\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tensor:\n",
    "        # return tensors\n",
    "        return torch.tensor(self.prompts[idx]), torch.tensor(self.answers[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 468 batches\n",
      "train: 375\n",
      "val: 93\n",
      "type(train_ds[0]): <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# 10k balanced dataset\n",
    "total_ds = ArithmeticDataset(DATA_DIR / \"add_3digit_bal\" / \"add_3digit_10k_bal.txt\" , CharTokenizer(), seq_len=SEQ_LEN)\n",
    "print(\"total:\", len(total_ds), \"batches\")\n",
    "\n",
    "# train/val split\n",
    "train_ds, val_ds = torch.utils.data.random_split(total_ds, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "print(\"train:\", len(train_ds))\n",
    "print(\"val:\", len(val_ds))\n",
    "print(\"type(train_ds[0]):\", type(train_ds[0]))\n",
    "\n",
    "del total_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldm = L.LightningDataModule.from_datasets(train_dataset=train_ds, val_dataset=val_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning module wrapper for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_cosine_annealing_with_warmup(it: int, learning_rate: float, warmup_iters: int, lr_decay_iters: int, min_lr: float = None):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    \n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    \n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    \n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningNanoGPT(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_len: int,\n",
    "        n_embd: int,\n",
    "        n_head: int,\n",
    "        n_layers: int,\n",
    "        vocab_size: int,\n",
    "        ff_factor: int = 4,\n",
    "        dropout: float = 0.1,\n",
    "        lr: float = 0.001,\n",
    "        betas: tuple[float, float] = (0.9, 0.99),\n",
    "        weight_decay: float = 0.1,\n",
    "        warmup_iters: int = 100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = NanoGPT(\n",
    "            context_len=context_len,\n",
    "            n_embd=n_embd,\n",
    "            n_head=n_head,\n",
    "            n_layers=n_layers,\n",
    "            vocab_size=vocab_size,\n",
    "            ff_factor=ff_factor,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.weight_decay = weight_decay\n",
    "        self.warmup_iters = warmup_iters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
    "        # batch: (batch_size, seq_len)\n",
    "        # split into input and target (shifted by 1)\n",
    "        x, y = batch[:, :-1], batch[:, 1:]\n",
    "        # forward pass\n",
    "        logits = self.model(x)\n",
    "        # calculate loss\n",
    "        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y.reshape(-1))\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
    "        x, y = batch[:, :-1], batch[:, 1:]\n",
    "        logits = self.model(x)\n",
    "        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y.reshape(-1))\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
    "        # TODO accuracy\n",
    "        raise NotImplementedError(\"Test step not implemented\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.model.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = f\"{mn}.{pn}\" if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, torch.nn.MultiheadAttention):\n",
    "                    # special case for multihead attention\n",
    "                    decay.add(fpn)\n",
    "\n",
    "        # subtle: 'embedding.weight' and 'lm_head.weight' are tied, so they\n",
    "        # will appear in the no_decay and decay sets respectively after the above.\n",
    "        # In addition, because named_parameters() doesn't return duplicates, it\n",
    "        # will only return the first occurence, key'd by 'embedding.weight', below.\n",
    "        # so let's manually remove 'lm_head.weight' from decay set. This will include\n",
    "        # this tensor into optimization via embedding.weight only, and not decayed.\n",
    "        decay.remove('lm_head.weight')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.model.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": self.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        use_fused = str(DEVICE) == \"cuda\"\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=self.lr, betas=self.betas, **extra_args)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lambda i: lr_cosine_annealing_with_warmup(\n",
    "                i, self.lr, warmup_iters=self.warmup_iters, lr_decay_iters=self.trainer.max_steps\n",
    "            )\n",
    "        )\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "\n",
    "    def param_count(self) -> int:\n",
    "        return self.model.param_count()\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate(self,\n",
    "        idx: Tensor,\n",
    "        max_new_tokens: int,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int = None,\n",
    "        stop_token: int = None,\n",
    "        seed: int = 42,\n",
    "    ) -> Tensor:\n",
    "        return self.model.generate(idx, max_new_tokens, temperature, top_k, stop_token, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel = LightningNanoGPT(\n",
    "    context_len=SEQ_LEN,\n",
    "    n_embd=N_EMBD,\n",
    "    n_head=N_HEAD,\n",
    "    n_layers=N_LAYERS,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    dropout=0.2,\n",
    "    lr=0.001,\n",
    "    betas=(0.9, 0.99),\n",
    "    weight_decay=0.1,\n",
    "    warmup_iters=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "run_name = \"nanogpt_add_3digit_10k_bal_with_lr_sched\"\n",
    "run_dir = CHECKPOINTS_DIR / run_name\n",
    "run_dir.mkdir(exist_ok=True, parents=True)\n",
    "checkpoint_callback = L.pytorch.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_top_k=1, mode=\"min\", dirpath=run_dir, filename=\"{step}-{train_loss:.4f}-{val_loss:.4f}\")\n",
    "trainer = L.Trainer(\n",
    "    logger=L.pytorch.loggers.WandbLogger(project=\"msc-thesis-pilot\", name=run_name, save_dir=ROOT_DIR, log_model=True),\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_steps=1000,\n",
    "    val_check_interval=10,\n",
    "    log_every_n_steps=1,\n",
    "    gradient_clip_val=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miibrahimli\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/imran/Desktop/studies/thesis/msc_thesis/wandb/run-20240222_100416-nis2v2rh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iibrahimli/msc-thesis-pilot/runs/nis2v2rh' target=\"_blank\">nanogpt_add_3digit_10k_bal_with_lr_sched</a></strong> to <a href='https://wandb.ai/iibrahimli/msc-thesis-pilot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iibrahimli/msc-thesis-pilot' target=\"_blank\">https://wandb.ai/iibrahimli/msc-thesis-pilot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iibrahimli/msc-thesis-pilot/runs/nis2v2rh' target=\"_blank\">https://wandb.ai/iibrahimli/msc-thesis-pilot/runs/nis2v2rh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imran/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /Users/imran/Desktop/studies/thesis/msc_thesis/checkpoints/nanogpt_add_3digit_10k_bal_with_lr_sched exists and is not empty.\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | NanoGPT | 12.5 M\n",
      "----------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "49.842    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imran/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imran/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83:  33%|███▎      | 4/12 [00:04<00:08,  0.95it/s, v_num=v2rh] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83:  33%|███▎      | 4/12 [00:04<00:08,  0.95it/s, v_num=v2rh]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(lmodel, ldm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example 0:\n",
      "input: 654+114=\n",
      "target: 768\n",
      "output: 77777\n",
      "\n",
      "example 1:\n",
      "input: 25+759=\n",
      "target: 784\n",
      "output: =====\n",
      "\n",
      "example 2:\n",
      "input: 281+250=\n",
      "target: 531\n",
      "output: =====\n",
      "\n",
      "example 3:\n",
      "input: 228+142=\n",
      "target: 370\n",
      "output: =7777\n",
      "\n",
      "example 4:\n",
      "input: 754+104=\n",
      "target: 858\n",
      "output: =====\n",
      "\n",
      "example 5:\n",
      "input: 692+758=\n",
      "target: 1450\n",
      "output: ===99\n",
      "\n",
      "example 6:\n",
      "input: 913+558=\n",
      "target: 1471\n",
      "output: 77777\n",
      "\n",
      "example 7:\n",
      "input: 89+604=\n",
      "target: 693\n",
      "output: =9999\n",
      "\n",
      "example 8:\n",
      "input: 432+32=\n",
      "target: 464\n",
      "output: 77777\n",
      "\n",
      "example 9:\n",
      "input: 30+95=\n",
      "target: 125\n",
      "output: 99999\n",
      "\n",
      "example 10:\n",
      "input: 223+238=\n",
      "target: 461\n",
      "output: =9999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load test dataset\n",
    "test_ds = ArithmeticEvalDataset(DATA_DIR / \"add_3digit_bal\" / \"add_3digit_10k_test.txt\", tokenizer, seq_len=SEQ_LEN)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# load best model\n",
    "best_lmodel = LightningNanoGPT.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "best_lmodel.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i, (x, y) in enumerate(test_dl):\n",
    "        if i > 10:\n",
    "            break\n",
    "        output = best_lmodel.generate(x.to(DEVICE), max_new_tokens=5).squeeze().tolist()\n",
    "        output_text = tokenizer.decode(output)\n",
    "        output_text = output_text[output_text.index(\"=\")+1:]\n",
    "        print(f\"example {i}:\")\n",
    "        print(\"input:\", tokenizer.decode(x.squeeze().tolist()).strip())\n",
    "        print(\"target:\", tokenizer.decode(y.squeeze().tolist()).strip())\n",
    "        print(\"output:\", output_text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis-P7I560r2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
