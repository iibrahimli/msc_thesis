{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial experiment: small vanilla transformer (NanoGPT) trained on plain addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from arithmetic_lm.utils import get_torch_device, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "DEVICE = get_torch_device()\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "set_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyTransformer(nn.Module):\n",
    "    \"\"\"Simple small decoder-only transformer model using nn.TransformerDecoder.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_len: int,\n",
    "        n_embd: int,\n",
    "        n_head: int,\n",
    "        n_layers: int,\n",
    "        vocab_size: int,\n",
    "        ff_factor: int = 4,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            context_len: context length, i.e. the number of expected features in the input\n",
    "            n_embd: dimensionality of model embeddings\n",
    "            n_head: number of heads in the multi-head attention\n",
    "            n_layers: number of nn.TransformerDecoderLayer layers\n",
    "            vocab_size: size of the vocabulary\n",
    "            ff_factor: factor by which to scale the hidden layer dimensionality in the feedforward layer\n",
    "            dropout: dropout probability\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ff_factor = ff_factor\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_encoder = PositionalEncoding(n_embd, max_len=context_len, dropout=dropout)\n",
    "\n",
    "        # Same as decoder layer essentially, but without cross attention\n",
    "        self.layer = nn.TransformerEncoderLayer(\n",
    "            d_model=n_embd,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=n_embd * ff_factor,\n",
    "            dropout=dropout,\n",
    "\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.layer,\n",
    "            num_layers=n_layers,\n",
    "            norm=nn.LayerNorm(n_embd),\n",
    "        )\n",
    "\n",
    "        # Output\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size]``\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x, is_causal=True, mask=torch.nn.Transformer.generate_square_subsequent_mask(self.context_len))\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int, temperature: float = 1.0, top_k: int = None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (seq_len, batch)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop to context_len if necessary\n",
    "            idx_cond = idx if idx.size(0) <= self.context_len else idx[-self.context_len:, :]\n",
    "            logits = self.forward(idx_cond)\n",
    "            # get logits at final step and apply temperature\n",
    "            logits = logits[-1:, :, :] / temperature\n",
    "            # optionally apply top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(0)), dim=-1)\n",
    "                logits[logits < v[:, :, [-1]]] = -float(\"inf\")\n",
    "            # apply softmax\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            next_token = torch.multinomial(probs[0], num_samples=1)\n",
    "            # append to the sequence\n",
    "            idx = torch.cat([idx, next_token], dim=0)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imran/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8, 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = TinyTransformer(\n",
    "    context_len=10,\n",
    "    n_embd=32,\n",
    "    n_head=4,\n",
    "    n_layers=2,\n",
    "    vocab_size=100,\n",
    "    ff_factor=4,\n",
    "    dropout=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "# test forward pass\n",
    "x = torch.randint(0, 100, (10, 8)).to(DEVICE) # (seq_len, batch_size)\n",
    "y = net(x)\n",
    "y.shape # (seq_len, batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test one batch overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arithmetic_lm.tokenizer import CharTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([17, 14, 21, 21, 24, 94, 32, 24, 27, 21], 'len:', 10, 'hello worl')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello worl\"\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens, \"len:\", len(tokens), tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to tensor\n",
    "tokens = torch.tensor(tokens).unsqueeze(1).to(DEVICE)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([80, 100]),\n",
       " tensor([17, 14, 21, 21, 24, 94, 32, 24, 27, 21], device='mps:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(-1, y.size(-1)).shape, tokens.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplest train loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    y = net(tokens)\n",
    "    # create target by shifting tokens by 1 and adding a padding token at the end\n",
    "    target = torch.cat([tokens[1:, 0], torch.tensor([65]).to(DEVICE)]).unsqueeze(-1)\n",
    "    loss = criterion(y.view(-1, y.size(-1)), target.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"[{i}] loss: {loss.item():.5f}\")\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello worl$l$$'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompt = \"hell\"\n",
    "tokens = tokenizer.encode(test_prompt)\n",
    "tokens = torch.tensor(tokens).unsqueeze(1).to(DEVICE)\n",
    "print(tokens.shape)\n",
    "\n",
    "net.eval()\n",
    "generated_tokens = net.generate(tokens, max_new_tokens=10, temperature=1.0, top_k=10)\n",
    "\n",
    "tokenizer.decode(generated_tokens.squeeze(1).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis-P7I560r2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
