{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial experiment: small vanilla transformer (NanoGPT) trained on plain addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from arithmetic_lm.utils import get_torch_device, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "DEVICE = get_torch_device()\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "set_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyTransformer(nn.Module):\n",
    "    \"\"\"Simple small decoder-only transformer model using nn.TransformerDecoder.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_len: int,\n",
    "        n_embd: int,\n",
    "        n_head: int,\n",
    "        n_layers: int,\n",
    "        vocab_size: int,\n",
    "        ff_factor: int = 4,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            context_len: context length, i.e. the number of expected features in the input\n",
    "            n_embd: dimensionality of model embeddings\n",
    "            n_head: number of heads in the multi-head attention\n",
    "            n_layers: number of nn.TransformerDecoderLayer layers\n",
    "            vocab_size: size of the vocabulary\n",
    "            ff_factor: factor by which to scale the hidden layer dimensionality in the feedforward layer\n",
    "            dropout: dropout probability\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ff_factor = ff_factor\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_encoder = PositionalEncoding(n_embd, max_len=context_len, dropout=dropout)\n",
    "\n",
    "        # Same as decoder layer essentially, but without cross attention\n",
    "        self.layer = nn.TransformerEncoderLayer(\n",
    "            d_model=n_embd,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=n_embd * ff_factor,\n",
    "            dropout=dropout,\n",
    "\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.layer,\n",
    "            num_layers=n_layers,\n",
    "            norm=nn.LayerNorm(n_embd),\n",
    "        )\n",
    "\n",
    "        # Output\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size]``\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x, is_causal=True, mask=torch.nn.Transformer.generate_square_subsequent_mask(self.context_len))\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int, temperature: float = 1.0, top_k: int = None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (seq_len, batch)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop to context_len if necessary\n",
    "            idx_cond = idx if idx.size(0) <= self.context_len else idx[-self.context_len:, :]\n",
    "            logits = self.forward(idx_cond)\n",
    "            # get logits at final step and apply temperature\n",
    "            logits = logits[-1:, :, :] / temperature\n",
    "            # optionally apply top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(0)), dim=-1)\n",
    "                logits[logits < v[:, :, [-1]]] = -float(\"inf\")\n",
    "            # apply softmax\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            next_token = torch.multinomial(probs[0], num_samples=1)\n",
    "            # append to the sequence\n",
    "            idx = torch.cat([idx, next_token], dim=0)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imran/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8, 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = TinyTransformer(\n",
    "    context_len=10,\n",
    "    n_embd=32,\n",
    "    n_head=4,\n",
    "    n_layers=2,\n",
    "    vocab_size=100,\n",
    "    ff_factor=4,\n",
    "    dropout=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "# test forward pass\n",
    "x = torch.randint(0, 100, (10, 8)).to(DEVICE) # (seq_len, batch_size)\n",
    "y = net(x)\n",
    "y.shape # (seq_len, batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test one batch overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arithmetic_lm.tokenizer import CharTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([17, 14, 21, 21, 24, 94, 32, 24, 27, 21], 'len:', 10, 'hello worl')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello worl\"\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens, \"len:\", len(tokens), tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to tensor\n",
    "tokens = torch.tensor(tokens).unsqueeze(1).to(DEVICE)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([80, 100]),\n",
       " tensor([17, 14, 21, 21, 24, 94, 32, 24, 27, 21], device='mps:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(-1, y.size(-1)).shape, tokens.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss: 4.75054\n",
      "[1] loss: 4.84614\n",
      "[2] loss: 4.44716\n",
      "[3] loss: 4.58616\n",
      "[4] loss: 4.19507\n",
      "[5] loss: 4.10802\n",
      "[6] loss: 3.93403\n",
      "[7] loss: 3.71056\n",
      "[8] loss: 3.73026\n",
      "[9] loss: 3.62177\n",
      "[10] loss: 3.49763\n",
      "[11] loss: 3.39667\n",
      "[12] loss: 3.31937\n",
      "[13] loss: 3.23069\n",
      "[14] loss: 3.19946\n",
      "[15] loss: 3.01413\n",
      "[16] loss: 3.04749\n",
      "[17] loss: 2.98443\n",
      "[18] loss: 2.93788\n",
      "[19] loss: 2.84480\n",
      "[20] loss: 2.77242\n",
      "[21] loss: 2.74176\n",
      "[22] loss: 2.71742\n",
      "[23] loss: 2.65807\n",
      "[24] loss: 2.60152\n",
      "[25] loss: 2.56191\n",
      "[26] loss: 2.46438\n",
      "[27] loss: 2.33367\n",
      "[28] loss: 2.21808\n",
      "[29] loss: 2.26809\n",
      "[30] loss: 2.14971\n",
      "[31] loss: 2.16049\n",
      "[32] loss: 2.10479\n",
      "[33] loss: 2.03912\n",
      "[34] loss: 2.05508\n",
      "[35] loss: 1.92739\n",
      "[36] loss: 1.92850\n",
      "[37] loss: 1.90222\n",
      "[38] loss: 1.83010\n",
      "[39] loss: 1.75390\n",
      "[40] loss: 1.82884\n",
      "[41] loss: 1.69540\n",
      "[42] loss: 1.65533\n",
      "[43] loss: 1.66301\n",
      "[44] loss: 1.57256\n",
      "[45] loss: 1.58017\n",
      "[46] loss: 1.51742\n",
      "[47] loss: 1.51938\n",
      "[48] loss: 1.45002\n",
      "[49] loss: 1.37712\n",
      "[50] loss: 1.34487\n",
      "[51] loss: 1.33578\n",
      "[52] loss: 1.34706\n",
      "[53] loss: 1.30183\n",
      "[54] loss: 1.26369\n",
      "[55] loss: 1.20557\n",
      "[56] loss: 1.11354\n",
      "[57] loss: 1.14423\n",
      "[58] loss: 1.08116\n",
      "[59] loss: 1.02922\n",
      "[60] loss: 1.02441\n",
      "[61] loss: 1.00603\n",
      "[62] loss: 0.97505\n",
      "[63] loss: 0.89312\n",
      "[64] loss: 0.84723\n",
      "[65] loss: 0.89534\n",
      "[66] loss: 0.88553\n",
      "[67] loss: 0.86411\n",
      "[68] loss: 0.81595\n",
      "[69] loss: 0.85675\n",
      "[70] loss: 0.78914\n",
      "[71] loss: 0.73983\n",
      "[72] loss: 0.76744\n",
      "[73] loss: 0.72220\n",
      "[74] loss: 0.76059\n",
      "[75] loss: 0.69212\n",
      "[76] loss: 0.63112\n",
      "[77] loss: 0.68321\n",
      "[78] loss: 0.74253\n",
      "[79] loss: 0.64582\n",
      "[80] loss: 0.59267\n",
      "[81] loss: 0.55217\n",
      "[82] loss: 0.55931\n",
      "[83] loss: 0.54099\n",
      "[84] loss: 0.61027\n",
      "[85] loss: 0.54512\n",
      "[86] loss: 0.46890\n",
      "[87] loss: 0.53926\n",
      "[88] loss: 0.58423\n",
      "[89] loss: 0.54685\n",
      "[90] loss: 0.42624\n",
      "[91] loss: 0.45931\n",
      "[92] loss: 0.44296\n",
      "[93] loss: 0.48379\n",
      "[94] loss: 0.42451\n",
      "[95] loss: 0.37357\n",
      "[96] loss: 0.39641\n",
      "[97] loss: 0.33638\n",
      "[98] loss: 0.38942\n",
      "[99] loss: 0.44121\n",
      "[100] loss: 0.33855\n",
      "[101] loss: 0.53471\n",
      "[102] loss: 0.32310\n",
      "[103] loss: 0.31195\n",
      "[104] loss: 0.30442\n",
      "[105] loss: 0.29683\n",
      "[106] loss: 0.30651\n",
      "[107] loss: 0.29468\n",
      "[108] loss: 0.28323\n",
      "[109] loss: 0.30064\n",
      "[110] loss: 0.39239\n",
      "[111] loss: 0.29712\n",
      "[112] loss: 0.32562\n",
      "[113] loss: 0.28148\n",
      "[114] loss: 0.25996\n",
      "[115] loss: 0.23935\n",
      "[116] loss: 0.23903\n",
      "[117] loss: 0.22986\n",
      "[118] loss: 0.24033\n",
      "[119] loss: 0.22473\n",
      "[120] loss: 0.21878\n",
      "[121] loss: 0.22134\n",
      "[122] loss: 0.24075\n",
      "[123] loss: 0.40519\n",
      "[124] loss: 0.24951\n",
      "[125] loss: 0.21675\n",
      "[126] loss: 0.18424\n",
      "[127] loss: 0.22936\n",
      "[128] loss: 0.17275\n",
      "[129] loss: 0.19640\n",
      "[130] loss: 0.16314\n",
      "[131] loss: 0.25264\n",
      "[132] loss: 0.22020\n",
      "[133] loss: 0.17324\n",
      "[134] loss: 0.16780\n",
      "[135] loss: 0.18032\n",
      "[136] loss: 0.18113\n",
      "[137] loss: 0.15973\n",
      "[138] loss: 0.16266\n",
      "[139] loss: 0.15991\n",
      "[140] loss: 0.14642\n",
      "[141] loss: 0.21344\n",
      "[142] loss: 0.14333\n",
      "[143] loss: 0.13759\n",
      "[144] loss: 0.15374\n",
      "[145] loss: 0.16801\n",
      "[146] loss: 0.19065\n",
      "[147] loss: 0.17755\n",
      "[148] loss: 0.13896\n",
      "[149] loss: 0.13810\n",
      "[150] loss: 0.12280\n",
      "[151] loss: 0.14373\n",
      "[152] loss: 0.14148\n",
      "[153] loss: 0.18086\n",
      "[154] loss: 0.15436\n",
      "[155] loss: 0.12860\n",
      "[156] loss: 0.12697\n",
      "[157] loss: 0.11410\n",
      "[158] loss: 0.12290\n",
      "[159] loss: 0.11402\n",
      "[160] loss: 0.21543\n",
      "[161] loss: 0.12575\n",
      "[162] loss: 0.10829\n",
      "[163] loss: 0.12761\n",
      "[164] loss: 0.09939\n",
      "[165] loss: 0.11937\n",
      "[166] loss: 0.12838\n",
      "[167] loss: 0.10833\n",
      "[168] loss: 0.08732\n",
      "[169] loss: 0.10603\n",
      "[170] loss: 0.10587\n",
      "[171] loss: 0.09043\n",
      "[172] loss: 0.10605\n",
      "[173] loss: 0.09990\n",
      "[174] loss: 0.11250\n",
      "[175] loss: 0.08635\n",
      "[176] loss: 0.09372\n",
      "[177] loss: 0.07942\n",
      "[178] loss: 0.08569\n",
      "[179] loss: 0.07414\n",
      "[180] loss: 0.09165\n",
      "[181] loss: 0.08442\n",
      "[182] loss: 0.09482\n",
      "[183] loss: 0.07883\n",
      "[184] loss: 0.08873\n",
      "[185] loss: 0.08568\n",
      "[186] loss: 0.08678\n",
      "[187] loss: 0.08041\n",
      "[188] loss: 0.07082\n",
      "[189] loss: 0.08953\n",
      "[190] loss: 0.09041\n",
      "[191] loss: 0.08714\n",
      "[192] loss: 0.07244\n",
      "[193] loss: 0.07314\n",
      "[194] loss: 0.07863\n",
      "[195] loss: 0.07329\n",
      "[196] loss: 0.07100\n",
      "[197] loss: 0.07276\n",
      "[198] loss: 0.07327\n",
      "[199] loss: 0.07719\n",
      "[200] loss: 0.08334\n",
      "[201] loss: 0.06743\n",
      "[202] loss: 0.07140\n",
      "[203] loss: 0.07419\n",
      "[204] loss: 0.06602\n",
      "[205] loss: 0.06342\n",
      "[206] loss: 0.06176\n",
      "[207] loss: 0.06809\n",
      "[208] loss: 0.07179\n",
      "[209] loss: 0.07738\n",
      "[210] loss: 0.05643\n",
      "[211] loss: 0.06599\n",
      "[212] loss: 0.06703\n",
      "[213] loss: 0.12686\n",
      "[214] loss: 0.05683\n",
      "[215] loss: 0.05324\n",
      "[216] loss: 0.06173\n",
      "[217] loss: 0.06225\n",
      "[218] loss: 0.22604\n",
      "[219] loss: 0.17149\n",
      "[220] loss: 0.10331\n",
      "[221] loss: 0.06654\n",
      "[222] loss: 0.07002\n",
      "[223] loss: 0.06607\n",
      "[224] loss: 0.09113\n",
      "[225] loss: 0.04851\n",
      "[226] loss: 0.08663\n",
      "[227] loss: 0.05403\n",
      "[228] loss: 0.05073\n",
      "[229] loss: 0.05620\n",
      "[230] loss: 0.06147\n",
      "[231] loss: 0.05246\n",
      "[232] loss: 0.04841\n",
      "[233] loss: 0.05163\n",
      "[234] loss: 0.04798\n",
      "[235] loss: 0.33789\n",
      "[236] loss: 0.05718\n",
      "[237] loss: 0.04686\n",
      "[238] loss: 0.05800\n",
      "[239] loss: 0.05171\n",
      "[240] loss: 0.04981\n",
      "[241] loss: 0.06111\n",
      "[242] loss: 0.08529\n",
      "[243] loss: 0.04611\n",
      "[244] loss: 0.04942\n",
      "[245] loss: 0.06991\n",
      "[246] loss: 0.04577\n",
      "[247] loss: 0.04300\n",
      "[248] loss: 0.04207\n",
      "[249] loss: 0.04112\n",
      "[250] loss: 0.04513\n",
      "[251] loss: 0.32927\n",
      "[252] loss: 0.04396\n",
      "[253] loss: 0.04638\n",
      "[254] loss: 0.04439\n",
      "[255] loss: 0.05079\n",
      "[256] loss: 0.04326\n",
      "[257] loss: 0.04690\n",
      "[258] loss: 0.04902\n",
      "[259] loss: 0.05657\n",
      "[260] loss: 0.04785\n",
      "[261] loss: 0.05217\n",
      "[262] loss: 0.41956\n",
      "[263] loss: 0.12920\n",
      "[264] loss: 0.04303\n",
      "[265] loss: 0.03623\n",
      "[266] loss: 0.04559\n",
      "[267] loss: 0.03674\n",
      "[268] loss: 0.03915\n",
      "[269] loss: 0.03983\n",
      "[270] loss: 0.04263\n",
      "[271] loss: 0.12919\n",
      "[272] loss: 0.04184\n",
      "[273] loss: 0.04101\n",
      "[274] loss: 0.03836\n",
      "[275] loss: 0.20468\n",
      "[276] loss: 0.05585\n",
      "[277] loss: 0.05038\n",
      "[278] loss: 0.09945\n",
      "[279] loss: 0.14858\n",
      "[280] loss: 0.03274\n",
      "[281] loss: 0.09492\n",
      "[282] loss: 0.22541\n",
      "[283] loss: 0.04311\n",
      "[284] loss: 0.08214\n",
      "[285] loss: 0.06720\n",
      "[286] loss: 0.03753\n",
      "[287] loss: 0.03946\n",
      "[288] loss: 0.03288\n",
      "[289] loss: 0.03371\n",
      "[290] loss: 0.03307\n",
      "[291] loss: 0.08823\n",
      "[292] loss: 0.03407\n",
      "[293] loss: 0.03923\n",
      "[294] loss: 0.03755\n",
      "[295] loss: 0.04016\n",
      "[296] loss: 0.03824\n",
      "[297] loss: 0.28535\n",
      "[298] loss: 0.04015\n",
      "[299] loss: 0.03186\n",
      "[300] loss: 0.04218\n",
      "[301] loss: 0.03956\n",
      "[302] loss: 0.04518\n",
      "[303] loss: 0.06727\n",
      "[304] loss: 0.04209\n",
      "[305] loss: 0.17316\n",
      "[306] loss: 0.15234\n",
      "[307] loss: 0.03191\n",
      "[308] loss: 0.10426\n",
      "[309] loss: 0.03794\n",
      "[310] loss: 0.03674\n",
      "[311] loss: 0.03244\n",
      "[312] loss: 0.03059\n",
      "[313] loss: 0.05067\n",
      "[314] loss: 0.03365\n",
      "[315] loss: 0.03134\n",
      "[316] loss: 0.09040\n",
      "[317] loss: 0.04821\n",
      "[318] loss: 0.03802\n",
      "[319] loss: 0.11022\n",
      "[320] loss: 0.03411\n",
      "[321] loss: 0.03003\n",
      "[322] loss: 0.03932\n",
      "[323] loss: 0.04447\n",
      "[324] loss: 0.02685\n",
      "[325] loss: 0.02924\n",
      "[326] loss: 0.02971\n",
      "[327] loss: 0.02647\n",
      "[328] loss: 0.02591\n",
      "[329] loss: 0.02750\n",
      "[330] loss: 0.03032\n",
      "[331] loss: 0.03049\n",
      "[332] loss: 0.03095\n",
      "[333] loss: 0.03211\n",
      "[334] loss: 0.02756\n",
      "[335] loss: 0.02725\n",
      "[336] loss: 0.03222\n",
      "[337] loss: 0.02893\n",
      "[338] loss: 0.02632\n",
      "[339] loss: 0.02988\n",
      "[340] loss: 0.02613\n",
      "[341] loss: 0.02748\n",
      "[342] loss: 0.02581\n",
      "[343] loss: 0.02391\n",
      "[344] loss: 0.02417\n",
      "[345] loss: 0.02748\n",
      "[346] loss: 0.04012\n",
      "[347] loss: 0.03429\n",
      "[348] loss: 0.02981\n",
      "[349] loss: 0.03359\n",
      "[350] loss: 0.03139\n",
      "[351] loss: 0.02577\n",
      "[352] loss: 0.02848\n",
      "[353] loss: 0.03236\n",
      "[354] loss: 0.02219\n",
      "[355] loss: 0.02300\n",
      "[356] loss: 0.02861\n",
      "[357] loss: 0.03243\n",
      "[358] loss: 0.02457\n",
      "[359] loss: 0.02843\n",
      "[360] loss: 0.02424\n",
      "[361] loss: 0.02372\n",
      "[362] loss: 0.02179\n",
      "[363] loss: 0.02275\n",
      "[364] loss: 0.02083\n",
      "[365] loss: 0.02188\n",
      "[366] loss: 0.12377\n",
      "[367] loss: 0.02311\n",
      "[368] loss: 0.02039\n",
      "[369] loss: 0.02132\n",
      "[370] loss: 0.02569\n",
      "[371] loss: 0.07723\n",
      "[372] loss: 0.02882\n",
      "[373] loss: 0.02266\n",
      "[374] loss: 0.02076\n",
      "[375] loss: 0.06402\n",
      "[376] loss: 0.02325\n",
      "[377] loss: 0.02156\n",
      "[378] loss: 0.01939\n",
      "[379] loss: 0.02028\n",
      "[380] loss: 0.02112\n",
      "[381] loss: 0.01845\n",
      "[382] loss: 0.01937\n",
      "[383] loss: 0.01882\n",
      "[384] loss: 0.02190\n",
      "[385] loss: 0.02137\n",
      "[386] loss: 0.02027\n",
      "[387] loss: 0.01976\n",
      "[388] loss: 0.01813\n",
      "[389] loss: 0.02328\n",
      "[390] loss: 0.01831\n",
      "[391] loss: 0.02107\n",
      "[392] loss: 0.01802\n",
      "[393] loss: 0.02322\n",
      "[394] loss: 0.08325\n",
      "[395] loss: 0.01980\n",
      "[396] loss: 0.01740\n",
      "[397] loss: 0.01894\n",
      "[398] loss: 0.03777\n",
      "[399] loss: 0.01887\n",
      "[400] loss: 0.01946\n",
      "[401] loss: 0.01961\n",
      "[402] loss: 0.01713\n",
      "[403] loss: 0.03609\n",
      "[404] loss: 0.01639\n",
      "[405] loss: 0.01690\n",
      "[406] loss: 0.01871\n",
      "[407] loss: 0.04040\n",
      "[408] loss: 0.01631\n",
      "[409] loss: 0.01562\n",
      "[410] loss: 0.01816\n",
      "[411] loss: 0.01934\n",
      "[412] loss: 0.01651\n",
      "[413] loss: 0.01571\n",
      "[414] loss: 0.01718\n",
      "[415] loss: 0.01600\n",
      "[416] loss: 0.01835\n",
      "[417] loss: 0.01796\n",
      "[418] loss: 0.01738\n",
      "[419] loss: 0.01639\n",
      "[420] loss: 0.01831\n",
      "[421] loss: 0.01575\n",
      "[422] loss: 0.01617\n",
      "[423] loss: 0.01591\n",
      "[424] loss: 0.02173\n",
      "[425] loss: 0.01989\n",
      "[426] loss: 0.01507\n",
      "[427] loss: 0.01568\n",
      "[428] loss: 0.01531\n",
      "[429] loss: 0.01754\n",
      "[430] loss: 0.01586\n",
      "[431] loss: 0.01747\n",
      "[432] loss: 0.02235\n",
      "[433] loss: 0.01400\n",
      "[434] loss: 0.02145\n",
      "[435] loss: 0.01509\n",
      "[436] loss: 0.01821\n",
      "[437] loss: 0.01393\n",
      "[438] loss: 0.01761\n",
      "[439] loss: 0.01538\n",
      "[440] loss: 0.01377\n",
      "[441] loss: 0.01797\n",
      "[442] loss: 0.01732\n",
      "[443] loss: 0.01534\n",
      "[444] loss: 0.04845\n",
      "[445] loss: 0.01427\n",
      "[446] loss: 0.01606\n",
      "[447] loss: 0.01495\n",
      "[448] loss: 0.01756\n",
      "[449] loss: 0.01782\n",
      "[450] loss: 0.01507\n",
      "[451] loss: 0.01548\n",
      "[452] loss: 0.13708\n",
      "[453] loss: 0.01438\n",
      "[454] loss: 0.02853\n",
      "[455] loss: 0.01604\n",
      "[456] loss: 0.01246\n",
      "[457] loss: 0.01409\n",
      "[458] loss: 0.01315\n",
      "[459] loss: 0.01273\n",
      "[460] loss: 0.01357\n",
      "[461] loss: 0.01311\n",
      "[462] loss: 0.01339\n",
      "[463] loss: 0.01706\n",
      "[464] loss: 0.01380\n",
      "[465] loss: 0.01607\n",
      "[466] loss: 0.03811\n",
      "[467] loss: 0.01218\n",
      "[468] loss: 0.01606\n",
      "[469] loss: 0.01475\n",
      "[470] loss: 0.01544\n",
      "[471] loss: 0.01465\n",
      "[472] loss: 0.01804\n",
      "[473] loss: 0.01401\n",
      "[474] loss: 0.01622\n",
      "[475] loss: 0.01196\n",
      "[476] loss: 0.01253\n",
      "[477] loss: 0.01375\n",
      "[478] loss: 0.01354\n",
      "[479] loss: 0.01260\n",
      "[480] loss: 0.02336\n",
      "[481] loss: 0.49597\n",
      "[482] loss: 0.01240\n",
      "[483] loss: 0.01894\n",
      "[484] loss: 0.01324\n",
      "[485] loss: 0.01199\n",
      "[486] loss: 0.01519\n",
      "[487] loss: 0.01257\n",
      "[488] loss: 0.19079\n",
      "[489] loss: 0.01419\n",
      "[490] loss: 0.01248\n",
      "[491] loss: 0.01313\n",
      "[492] loss: 0.48027\n",
      "[493] loss: 0.01527\n",
      "[494] loss: 0.01344\n",
      "[495] loss: 0.01723\n",
      "[496] loss: 0.01733\n",
      "[497] loss: 0.01242\n",
      "[498] loss: 0.04691\n",
      "[499] loss: 0.01550\n",
      "[500] loss: 0.07239\n",
      "[501] loss: 0.01557\n",
      "[502] loss: 0.27971\n",
      "[503] loss: 0.01425\n",
      "[504] loss: 0.01448\n",
      "[505] loss: 0.01410\n",
      "[506] loss: 0.01555\n",
      "[507] loss: 0.01990\n",
      "[508] loss: 0.01296\n",
      "[509] loss: 0.01227\n",
      "[510] loss: 0.01226\n",
      "[511] loss: 0.01475\n",
      "[512] loss: 0.01761\n",
      "[513] loss: 0.11522\n",
      "[514] loss: 0.01530\n",
      "[515] loss: 0.01348\n",
      "[516] loss: 0.01482\n",
      "[517] loss: 0.01668\n",
      "[518] loss: 0.01416\n",
      "[519] loss: 0.01456\n",
      "[520] loss: 0.01375\n",
      "[521] loss: 0.02029\n",
      "[522] loss: 0.01616\n",
      "[523] loss: 0.01467\n",
      "[524] loss: 0.10742\n",
      "[525] loss: 0.01243\n",
      "[526] loss: 0.02006\n",
      "[527] loss: 0.01260\n",
      "[528] loss: 0.06223\n",
      "[529] loss: 0.01311\n",
      "[530] loss: 0.01619\n",
      "[531] loss: 0.01095\n",
      "[532] loss: 0.01365\n",
      "[533] loss: 0.01097\n",
      "[534] loss: 0.01316\n",
      "[535] loss: 0.01201\n",
      "[536] loss: 0.01132\n",
      "[537] loss: 0.01242\n",
      "[538] loss: 0.01053\n",
      "[539] loss: 0.01154\n",
      "[540] loss: 0.01120\n",
      "[541] loss: 0.01255\n",
      "[542] loss: 0.01179\n",
      "[543] loss: 0.00981\n",
      "[544] loss: 0.01269\n",
      "[545] loss: 0.01066\n",
      "[546] loss: 0.01198\n",
      "[547] loss: 0.01000\n",
      "[548] loss: 0.01006\n",
      "[549] loss: 0.00967\n",
      "[550] loss: 0.01269\n",
      "[551] loss: 0.00957\n",
      "[552] loss: 0.01210\n",
      "[553] loss: 0.01195\n",
      "[554] loss: 0.01006\n",
      "[555] loss: 0.01483\n",
      "[556] loss: 0.01179\n",
      "[557] loss: 0.16525\n",
      "[558] loss: 0.00970\n",
      "[559] loss: 0.01455\n",
      "[560] loss: 0.01137\n",
      "[561] loss: 0.01097\n",
      "[562] loss: 0.01062\n",
      "[563] loss: 0.00991\n",
      "[564] loss: 0.01167\n",
      "[565] loss: 0.01474\n",
      "[566] loss: 0.01070\n",
      "[567] loss: 0.01117\n",
      "[568] loss: 0.19261\n",
      "[569] loss: 0.01288\n",
      "[570] loss: 0.01324\n",
      "[571] loss: 0.00982\n",
      "[572] loss: 0.01010\n",
      "[573] loss: 0.01297\n",
      "[574] loss: 0.01208\n",
      "[575] loss: 0.04344\n",
      "[576] loss: 0.01110\n",
      "[577] loss: 0.01112\n",
      "[578] loss: 0.01209\n",
      "[579] loss: 0.01077\n",
      "[580] loss: 0.02779\n",
      "[581] loss: 0.01308\n",
      "[582] loss: 0.01162\n",
      "[583] loss: 0.00902\n",
      "[584] loss: 0.00912\n",
      "[585] loss: 0.00958\n",
      "[586] loss: 0.00950\n",
      "[587] loss: 0.02693\n",
      "[588] loss: 0.01086\n",
      "[589] loss: 0.01371\n",
      "[590] loss: 0.01057\n",
      "[591] loss: 0.01010\n",
      "[592] loss: 0.01288\n",
      "[593] loss: 0.01042\n",
      "[594] loss: 0.00948\n",
      "[595] loss: 0.01133\n",
      "[596] loss: 0.02853\n",
      "[597] loss: 0.01191\n",
      "[598] loss: 0.00985\n",
      "[599] loss: 0.01299\n",
      "[600] loss: 0.01921\n",
      "[601] loss: 0.00851\n",
      "[602] loss: 0.01165\n",
      "[603] loss: 0.00964\n",
      "[604] loss: 0.00885\n",
      "[605] loss: 0.01000\n",
      "[606] loss: 0.00834\n",
      "[607] loss: 0.01367\n",
      "[608] loss: 0.00866\n",
      "[609] loss: 0.00864\n",
      "[610] loss: 0.00929\n",
      "[611] loss: 0.00847\n",
      "[612] loss: 0.00977\n",
      "[613] loss: 0.00788\n",
      "[614] loss: 0.01230\n",
      "[615] loss: 0.01016\n",
      "[616] loss: 0.00830\n",
      "[617] loss: 0.00775\n",
      "[618] loss: 0.00796\n",
      "[619] loss: 0.01570\n",
      "[620] loss: 0.00998\n",
      "[621] loss: 0.00913\n",
      "[622] loss: 0.00855\n",
      "[623] loss: 0.00866\n",
      "[624] loss: 0.00844\n",
      "[625] loss: 0.06506\n",
      "[626] loss: 0.00895\n",
      "[627] loss: 0.00836\n",
      "[628] loss: 0.01000\n",
      "[629] loss: 0.05264\n",
      "[630] loss: 0.00854\n",
      "[631] loss: 0.00820\n",
      "[632] loss: 0.00715\n",
      "[633] loss: 0.00840\n",
      "[634] loss: 0.00972\n",
      "[635] loss: 0.01035\n",
      "[636] loss: 0.00857\n",
      "[637] loss: 0.00821\n",
      "[638] loss: 0.00946\n",
      "[639] loss: 0.00805\n",
      "[640] loss: 0.00940\n",
      "[641] loss: 0.00836\n",
      "[642] loss: 0.00821\n",
      "[643] loss: 0.00832\n",
      "[644] loss: 0.00731\n",
      "[645] loss: 0.00757\n",
      "[646] loss: 0.00747\n",
      "[647] loss: 0.00897\n",
      "[648] loss: 0.00887\n",
      "[649] loss: 0.00855\n",
      "[650] loss: 0.00767\n",
      "[651] loss: 0.27532\n",
      "[652] loss: 0.00744\n",
      "[653] loss: 0.00772\n",
      "[654] loss: 0.00737\n",
      "[655] loss: 0.01969\n",
      "[656] loss: 0.00673\n",
      "[657] loss: 0.00955\n",
      "[658] loss: 0.01316\n",
      "[659] loss: 0.00864\n",
      "[660] loss: 0.01137\n",
      "[661] loss: 0.00804\n",
      "[662] loss: 0.01002\n",
      "[663] loss: 0.00850\n",
      "[664] loss: 0.00842\n",
      "[665] loss: 0.01032\n",
      "[666] loss: 0.00839\n",
      "[667] loss: 0.02344\n",
      "[668] loss: 0.00771\n",
      "[669] loss: 0.01078\n",
      "[670] loss: 0.00692\n",
      "[671] loss: 0.44178\n",
      "[672] loss: 0.00748\n",
      "[673] loss: 0.00705\n",
      "[674] loss: 0.00660\n",
      "[675] loss: 0.00809\n",
      "[676] loss: 0.00914\n",
      "[677] loss: 0.00811\n",
      "[678] loss: 0.00912\n",
      "[679] loss: 0.00911\n",
      "[680] loss: 0.00957\n",
      "[681] loss: 0.00785\n",
      "[682] loss: 0.00749\n",
      "[683] loss: 0.00893\n",
      "[684] loss: 0.00756\n",
      "[685] loss: 0.00776\n",
      "[686] loss: 0.00929\n",
      "[687] loss: 0.00945\n",
      "[688] loss: 0.00983\n",
      "[689] loss: 0.00752\n",
      "[690] loss: 0.00858\n",
      "[691] loss: 0.11599\n",
      "[692] loss: 0.00732\n",
      "[693] loss: 0.00740\n",
      "[694] loss: 0.00877\n",
      "[695] loss: 0.00670\n",
      "[696] loss: 0.06181\n",
      "[697] loss: 0.00684\n",
      "[698] loss: 0.04328\n",
      "[699] loss: 0.00677\n",
      "[700] loss: 0.00783\n",
      "[701] loss: 0.05367\n",
      "[702] loss: 0.00766\n",
      "[703] loss: 0.00681\n",
      "[704] loss: 0.00733\n",
      "[705] loss: 0.00810\n",
      "[706] loss: 0.01435\n",
      "[707] loss: 0.00722\n",
      "[708] loss: 0.01345\n",
      "[709] loss: 0.00622\n",
      "[710] loss: 0.00744\n",
      "[711] loss: 0.00790\n",
      "[712] loss: 0.01402\n",
      "[713] loss: 0.00696\n",
      "[714] loss: 0.00892\n",
      "[715] loss: 0.00802\n",
      "[716] loss: 0.00992\n",
      "[717] loss: 0.00633\n",
      "[718] loss: 0.00739\n",
      "[719] loss: 0.00678\n",
      "[720] loss: 0.00686\n",
      "[721] loss: 0.00675\n",
      "[722] loss: 0.00770\n",
      "[723] loss: 0.00761\n",
      "[724] loss: 0.00581\n",
      "[725] loss: 0.00678\n",
      "[726] loss: 0.00706\n",
      "[727] loss: 0.00745\n",
      "[728] loss: 0.00658\n",
      "[729] loss: 0.00902\n",
      "[730] loss: 0.00660\n",
      "[731] loss: 0.00627\n",
      "[732] loss: 0.00690\n",
      "[733] loss: 0.00752\n",
      "[734] loss: 0.00667\n",
      "[735] loss: 0.00756\n",
      "[736] loss: 0.00727\n",
      "[737] loss: 0.00590\n",
      "[738] loss: 0.00657\n",
      "[739] loss: 0.00670\n",
      "[740] loss: 0.00742\n",
      "[741] loss: 0.00558\n",
      "[742] loss: 0.00714\n",
      "[743] loss: 0.00739\n",
      "[744] loss: 0.00649\n",
      "[745] loss: 0.00609\n",
      "[746] loss: 0.00624\n",
      "[747] loss: 0.00615\n",
      "[748] loss: 0.00642\n",
      "[749] loss: 0.00665\n",
      "[750] loss: 0.00638\n",
      "[751] loss: 0.00651\n",
      "[752] loss: 0.00662\n",
      "[753] loss: 0.00706\n",
      "[754] loss: 0.00795\n",
      "[755] loss: 0.00692\n",
      "[756] loss: 0.00542\n",
      "[757] loss: 0.01004\n",
      "[758] loss: 0.00568\n",
      "[759] loss: 0.00563\n",
      "[760] loss: 0.00722\n",
      "[761] loss: 0.00731\n",
      "[762] loss: 0.00853\n",
      "[763] loss: 0.00577\n",
      "[764] loss: 0.00604\n",
      "[765] loss: 0.00673\n",
      "[766] loss: 0.00511\n",
      "[767] loss: 0.00919\n",
      "[768] loss: 0.00606\n",
      "[769] loss: 0.00664\n",
      "[770] loss: 0.00574\n",
      "[771] loss: 0.00579\n",
      "[772] loss: 0.00622\n",
      "[773] loss: 0.00574\n",
      "[774] loss: 0.00554\n",
      "[775] loss: 0.00714\n",
      "[776] loss: 0.00547\n",
      "[777] loss: 0.00612\n",
      "[778] loss: 0.00537\n",
      "[779] loss: 0.00581\n",
      "[780] loss: 0.00559\n",
      "[781] loss: 0.00626\n",
      "[782] loss: 0.00535\n",
      "[783] loss: 0.00581\n",
      "[784] loss: 0.00515\n",
      "[785] loss: 0.00555\n",
      "[786] loss: 0.00501\n",
      "[787] loss: 0.00575\n",
      "[788] loss: 0.00558\n",
      "[789] loss: 0.00580\n",
      "[790] loss: 0.00549\n",
      "[791] loss: 0.00475\n",
      "[792] loss: 0.00473\n",
      "[793] loss: 0.00617\n",
      "[794] loss: 0.00452\n",
      "[795] loss: 0.00623\n",
      "[796] loss: 0.00480\n",
      "[797] loss: 0.00527\n",
      "[798] loss: 0.00553\n",
      "[799] loss: 0.00490\n",
      "[800] loss: 0.00580\n",
      "[801] loss: 0.00527\n",
      "[802] loss: 0.00477\n",
      "[803] loss: 0.00593\n",
      "[804] loss: 0.00502\n",
      "[805] loss: 0.00677\n",
      "[806] loss: 0.00588\n",
      "[807] loss: 0.00525\n",
      "[808] loss: 0.00536\n",
      "[809] loss: 0.00495\n",
      "[810] loss: 0.00648\n",
      "[811] loss: 0.00636\n",
      "[812] loss: 0.00846\n",
      "[813] loss: 0.00607\n",
      "[814] loss: 0.00498\n",
      "[815] loss: 0.00483\n",
      "[816] loss: 0.00596\n",
      "[817] loss: 0.00507\n",
      "[818] loss: 0.00479\n",
      "[819] loss: 0.01085\n",
      "[820] loss: 0.00804\n",
      "[821] loss: 0.00598\n",
      "[822] loss: 0.00463\n",
      "[823] loss: 0.00455\n",
      "[824] loss: 0.00419\n",
      "[825] loss: 0.00577\n",
      "[826] loss: 0.00629\n",
      "[827] loss: 0.00499\n",
      "[828] loss: 0.00464\n",
      "[829] loss: 0.00434\n",
      "[830] loss: 0.00475\n",
      "[831] loss: 0.00431\n",
      "[832] loss: 0.00780\n",
      "[833] loss: 0.00860\n",
      "[834] loss: 0.00604\n",
      "[835] loss: 0.00430\n",
      "[836] loss: 0.00567\n",
      "[837] loss: 0.00504\n",
      "[838] loss: 0.00595\n",
      "[839] loss: 0.00522\n",
      "[840] loss: 0.00479\n",
      "[841] loss: 0.07608\n",
      "[842] loss: 0.00511\n",
      "[843] loss: 0.00561\n",
      "[844] loss: 0.00569\n",
      "[845] loss: 0.00532\n",
      "[846] loss: 0.00417\n",
      "[847] loss: 0.00447\n",
      "[848] loss: 0.00504\n",
      "[849] loss: 0.00475\n",
      "[850] loss: 0.00475\n",
      "[851] loss: 0.00447\n",
      "[852] loss: 0.00515\n",
      "[853] loss: 0.00516\n",
      "[854] loss: 0.00554\n",
      "[855] loss: 0.00637\n",
      "[856] loss: 0.00434\n",
      "[857] loss: 0.00669\n",
      "[858] loss: 0.00587\n",
      "[859] loss: 0.00721\n",
      "[860] loss: 0.00464\n",
      "[861] loss: 0.00495\n",
      "[862] loss: 0.00762\n",
      "[863] loss: 0.00663\n",
      "[864] loss: 0.00517\n",
      "[865] loss: 0.00515\n",
      "[866] loss: 0.23598\n",
      "[867] loss: 0.00435\n",
      "[868] loss: 0.00502\n",
      "[869] loss: 0.00575\n",
      "[870] loss: 0.00573\n",
      "[871] loss: 0.00520\n",
      "[872] loss: 0.06227\n",
      "[873] loss: 0.00408\n",
      "[874] loss: 0.00475\n",
      "[875] loss: 0.00469\n",
      "[876] loss: 0.00454\n",
      "[877] loss: 0.00457\n",
      "[878] loss: 0.22323\n",
      "[879] loss: 0.34602\n",
      "[880] loss: 0.00441\n",
      "[881] loss: 0.00601\n",
      "[882] loss: 0.00483\n",
      "[883] loss: 0.00620\n",
      "[884] loss: 0.00472\n",
      "[885] loss: 0.00561\n",
      "[886] loss: 0.00458\n",
      "[887] loss: 0.25192\n",
      "[888] loss: 0.00580\n",
      "[889] loss: 0.00547\n",
      "[890] loss: 0.00544\n",
      "[891] loss: 0.00471\n",
      "[892] loss: 0.01547\n",
      "[893] loss: 0.02666\n",
      "[894] loss: 0.01243\n",
      "[895] loss: 0.00603\n",
      "[896] loss: 0.00667\n",
      "[897] loss: 0.00494\n",
      "[898] loss: 0.00534\n",
      "[899] loss: 0.00586\n",
      "[900] loss: 0.00670\n",
      "[901] loss: 0.00956\n",
      "[902] loss: 0.00576\n",
      "[903] loss: 0.00542\n",
      "[904] loss: 0.00500\n",
      "[905] loss: 0.01925\n",
      "[906] loss: 0.00610\n",
      "[907] loss: 0.00573\n",
      "[908] loss: 0.00532\n",
      "[909] loss: 0.00846\n",
      "[910] loss: 0.00655\n",
      "[911] loss: 0.00456\n",
      "[912] loss: 0.00665\n",
      "[913] loss: 0.00903\n",
      "[914] loss: 0.00742\n",
      "[915] loss: 0.01983\n",
      "[916] loss: 0.00400\n",
      "[917] loss: 0.00494\n",
      "[918] loss: 0.00868\n",
      "[919] loss: 0.00556\n",
      "[920] loss: 0.00446\n",
      "[921] loss: 0.00700\n",
      "[922] loss: 0.00477\n",
      "[923] loss: 0.00603\n",
      "[924] loss: 0.00454\n",
      "[925] loss: 0.00459\n",
      "[926] loss: 0.02723\n",
      "[927] loss: 0.00501\n",
      "[928] loss: 0.00404\n",
      "[929] loss: 0.05121\n",
      "[930] loss: 0.00405\n",
      "[931] loss: 0.00788\n",
      "[932] loss: 0.00708\n",
      "[933] loss: 0.00616\n",
      "[934] loss: 0.00574\n",
      "[935] loss: 0.00676\n",
      "[936] loss: 0.00428\n",
      "[937] loss: 0.01238\n",
      "[938] loss: 0.02907\n",
      "[939] loss: 0.00567\n",
      "[940] loss: 0.01027\n",
      "[941] loss: 0.00440\n",
      "[942] loss: 0.00498\n",
      "[943] loss: 0.00876\n",
      "[944] loss: 0.01510\n",
      "[945] loss: 0.00597\n",
      "[946] loss: 0.00551\n",
      "[947] loss: 0.00452\n",
      "[948] loss: 0.00440\n",
      "[949] loss: 0.00570\n",
      "[950] loss: 0.00427\n",
      "[951] loss: 0.00941\n",
      "[952] loss: 0.00762\n",
      "[953] loss: 0.00397\n",
      "[954] loss: 0.00392\n",
      "[955] loss: 0.00473\n",
      "[956] loss: 0.00397\n",
      "[957] loss: 0.00414\n",
      "[958] loss: 0.00484\n",
      "[959] loss: 0.00472\n",
      "[960] loss: 0.00411\n",
      "[961] loss: 0.00353\n",
      "[962] loss: 0.00393\n",
      "[963] loss: 0.00415\n",
      "[964] loss: 0.00431\n",
      "[965] loss: 0.00404\n",
      "[966] loss: 0.00468\n",
      "[967] loss: 0.00560\n",
      "[968] loss: 0.00461\n",
      "[969] loss: 0.00399\n",
      "[970] loss: 0.00420\n",
      "[971] loss: 0.00465\n",
      "[972] loss: 0.00348\n",
      "[973] loss: 0.00504\n",
      "[974] loss: 0.00425\n",
      "[975] loss: 0.00340\n",
      "[976] loss: 0.00339\n",
      "[977] loss: 0.00328\n",
      "[978] loss: 0.00561\n",
      "[979] loss: 0.00347\n",
      "[980] loss: 0.00416\n",
      "[981] loss: 0.00412\n",
      "[982] loss: 0.00358\n",
      "[983] loss: 0.00618\n",
      "[984] loss: 0.00364\n",
      "[985] loss: 0.00334\n",
      "[986] loss: 0.00387\n",
      "[987] loss: 0.00407\n",
      "[988] loss: 0.00437\n",
      "[989] loss: 0.00485\n",
      "[990] loss: 0.00307\n",
      "[991] loss: 0.00380\n",
      "[992] loss: 0.00335\n",
      "[993] loss: 0.00339\n",
      "[994] loss: 0.00360\n",
      "[995] loss: 0.00354\n",
      "[996] loss: 0.00398\n",
      "[997] loss: 0.00616\n",
      "[998] loss: 0.00327\n",
      "[999] loss: 0.00416\n"
     ]
    }
   ],
   "source": [
    "# simplest train loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    y = net(tokens)\n",
    "    # create target by shifting tokens by 1 and adding a padding token at the end\n",
    "    target = torch.cat([tokens[1:, 0], torch.tensor([65]).to(DEVICE)]).unsqueeze(-1)\n",
    "    loss = criterion(y.view(-1, y.size(-1)), target.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"[{i}] loss: {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello worl$l$$'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompt = \"hell\"\n",
    "tokens = tokenizer.encode(test_prompt)\n",
    "tokens = torch.tensor(tokens).unsqueeze(1).to(DEVICE)\n",
    "print(tokens.shape)\n",
    "\n",
    "net.eval()\n",
    "generated_tokens = net.generate(tokens, max_new_tokens=10, temperature=1.0, top_k=10)\n",
    "\n",
    "tokenizer.decode(generated_tokens.squeeze(1).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis-P7I560r2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
