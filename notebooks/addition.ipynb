{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial experiment: small vanilla transformer (NanoGPT) trained on plain addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from arithmetic_lm.tokenizer import CharTokenizer, Tokenizer\n",
    "from arithmetic_lm.utils import get_torch_device, set_seed\n",
    "from arithmetic_lm.constants import DATA_DIR, ROOT_DIR, CHECKPOINTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "DEVICE = get_torch_device()\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "set_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "N_LAYERS = 6\n",
    "N_HEAD = 6\n",
    "N_EMBD = 384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoGPT(nn.Module):\n",
    "    \"\"\"Simple small decoder-only transformer model using nn.TransformerDecoder.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_len: int,\n",
    "        n_embd: int,\n",
    "        n_head: int,\n",
    "        n_layers: int,\n",
    "        vocab_size: int,\n",
    "        ff_factor: int = 4,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            context_len: context length, i.e. the number of expected features in the input\n",
    "            n_embd: dimensionality of model embeddings\n",
    "            n_head: number of heads in the multi-head attention\n",
    "            n_layers: number of nn.TransformerDecoderLayer layers\n",
    "            vocab_size: size of the vocabulary\n",
    "            ff_factor: factor by which to scale the hidden layer dimensionality in the feedforward layer\n",
    "            dropout: dropout probability\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ff_factor = ff_factor\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_encoder = PositionalEncoding(n_embd, max_len=context_len, dropout=dropout)\n",
    "\n",
    "        # same as decoder layer essentially, but without cross attention\n",
    "        self.layer = nn.TransformerEncoderLayer(\n",
    "            d_model=n_embd,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=n_embd * ff_factor,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.layer,\n",
    "            num_layers=n_layers,\n",
    "            norm=nn.LayerNorm(n_embd),\n",
    "        )\n",
    "\n",
    "        # output to vocab dim\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        # weight tying\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "\n",
    "        # TODO init weights\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len]``\n",
    "        \n",
    "        Returns:\n",
    "            logits: Tensor, shape ``[batch_size, seq_len, vocab_size]``\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        x = self.transformer_encoder(\n",
    "            x,\n",
    "            is_causal=True,\n",
    "            mask=nn.Transformer.generate_square_subsequent_mask(\n",
    "                self.context_len, device=x.device\n",
    "            ),\n",
    "        )\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "    def param_count(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        idx: Tensor,\n",
    "        max_new_tokens: int,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int = None,\n",
    "        stop_token: int = None\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (tensor of shape [batch, seq_len]) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(idx, torch.Tensor), \"idx must be a torch.Tensor\"\n",
    "        assert idx.dim() == 2, \"idx must be a 2D tensor of shape [batch, seq_len]\"\n",
    "        assert idx.size(1) <= self.context_len, \"sequence length must be <= context_len\"\n",
    "        assert idx.size(0) == 1, \"only batch_size=1 is supported for now\"\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop to context_len if necessary\n",
    "            idx_cond = idx if idx.size(1) <= self.context_len else idx[:, -self.context_len:]\n",
    "\n",
    "            # logits shape: [batch, seq_len, vocab_size]\n",
    "            logits = self.forward(idx_cond)\n",
    "\n",
    "            # get logits at final step and apply temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            # optionally apply top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "            # apply softmax\n",
    "            probs = nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "            # sample from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # append to the sequence\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "            # stop if stop_token is generated\n",
    "            if stop_token is not None and next_token.item() == stop_token:\n",
    "                break\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test one batch overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([17, 14, 21, 21, 24, 94, 32, 24, 27, 21, 13], 'len:', 11, 'hello world')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello world\"\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens, \"len:\", len(tokens), tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to tensor\n",
    "tokens = torch.tensor(tokens).unsqueeze(0).to(DEVICE)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try overfitting on one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = NanoGPT(\n",
    "#     context_len=SEQ_LEN,\n",
    "#     n_embd=N_EMBD,\n",
    "#     n_head=N_HEAD,\n",
    "#     n_layers=N_LAYERS,\n",
    "#     vocab_size=tokenizer.vocab_size,\n",
    "# ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # simplest train loop\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# # create target by shifting tokens by 1 and adding a padding token at the end\n",
    "# target = torch.cat([tokens[0, 1:], torch.tensor([65]).to(DEVICE)]).unsqueeze(0)\n",
    "# test_text = \"hel\"\n",
    "# test_tokens = tokenizer.encode(test_text)\n",
    "# test_tokens = torch.tensor(test_tokens).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "# losses = []\n",
    "\n",
    "# for i in range(10000):\n",
    "#     optimizer.zero_grad()\n",
    "#     y = net(tokens)\n",
    "#     # y shape: (batch_size, seq_len, vocab_size)\n",
    "#     loss = criterion(y.view(-1, y.size(-1)), target.view(-1))\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     losses.append(loss.item())\n",
    "\n",
    "#     if i % 100 == 0:\n",
    "#         print(f\"[{i}] loss: {loss.item():.5f}  \", test_text + \" -> \" + tokenizer.decode(net.generate(test_tokens, max_new_tokens=10).squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.plot(losses)\n",
    "# plt.xlabel(\"iteration\")\n",
    "# plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prompt = \"hello w\"\n",
    "# tokens = tokenizer.encode(test_prompt)\n",
    "# tokens = torch.tensor(tokens).unsqueeze(0).to(DEVICE)\n",
    "# print(tokens.shape)\n",
    "\n",
    "# net.eval()\n",
    "# generated_tokens = net.generate(tokens, max_new_tokens=10, temperature=1.0, top_k=1)\n",
    "# print(generated_tokens.shape)\n",
    "\n",
    "# tokenizer.decode(generated_tokens.squeeze(0).cpu().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArithmeticDataset(Dataset):\n",
    "    \"\"\"Concatenate lines in file and split into sequences of length seq_len.\"\"\"\n",
    "    # TODO transforms (adding $, formatting, reversing)\n",
    "    def __init__(self, txtfile: str | Path, tokenizer: Tokenizer, seq_len: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        with open(txtfile, \"r\") as f:\n",
    "            text = f.read()\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        # make batches of same length (truncate if necessary)\n",
    "        n_batches = len(tokens) // seq_len\n",
    "        self.batches = [tokens[i*seq_len:(i+1)*seq_len] for i in range(n_batches)]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.batches)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> list[int]:\n",
    "        # return tensors\n",
    "        return torch.tensor(self.batches[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArithmeticEvalDataset(Dataset):\n",
    "    \"\"\"Dataset but instead of pure language modeling, we want to evaluate each example (line)\"\"\"\n",
    "    # TODO transforms (adding $, formatting, reversing)\n",
    "    def __init__(self, txtfile: str | Path, tokenizer: Tokenizer, seq_len: int, format_str: str = None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        with open(txtfile, \"r\") as f:\n",
    "            text = f.readlines()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.batches)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> list[int]:\n",
    "        # return tensors\n",
    "        return torch.tensor(self.batches[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 468 batches\n",
      "train: 375\n",
      "val: 93\n",
      "type(train_ds[0]): <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# 10k balanced dataset\n",
    "total_ds = ArithmeticDataset(DATA_DIR / \"add_3digit_bal\" / \"add_3digit_10k_bal.txt\" , CharTokenizer(), seq_len=SEQ_LEN)\n",
    "print(\"total:\", len(total_ds), \"batches\")\n",
    "\n",
    "# train/val split\n",
    "train_ds, val_ds = torch.utils.data.random_split(total_ds, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "print(\"train:\", len(train_ds))\n",
    "print(\"val:\", len(val_ds))\n",
    "print(\"type(train_ds[0]):\", type(train_ds[0]))\n",
    "\n",
    "del total_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldm = L.LightningDataModule.from_datasets(train_dataset=train_ds, val_dataset=val_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning module wrapper for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningNanoGPT(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_len: int,\n",
    "        n_embd: int,\n",
    "        n_head: int,\n",
    "        n_layers: int,\n",
    "        vocab_size: int,\n",
    "        ff_factor: int = 4,\n",
    "        dropout: float = 0.1,\n",
    "        lr: float = 0.001,\n",
    "        betas: tuple[float, float] = (0.9, 0.99),\n",
    "        weight_decay: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = NanoGPT(\n",
    "            context_len=context_len,\n",
    "            n_embd=n_embd,\n",
    "            n_head=n_head,\n",
    "            n_layers=n_layers,\n",
    "            vocab_size=vocab_size,\n",
    "            ff_factor=ff_factor,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.weight_decay = weight_decay\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
    "        # batch: (batch_size, seq_len)\n",
    "        # split into input and target (shifted by 1)\n",
    "        x, y = batch[:, :-1], batch[:, 1:]\n",
    "        # forward pass\n",
    "        logits = self.model(x)\n",
    "        # calculate loss\n",
    "        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y.reshape(-1))\n",
    "        self.log(\"train_loss\", loss, on_step=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
    "        x, y = batch[:, :-1], batch[:, 1:]\n",
    "        logits = self.model(x)\n",
    "        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y.reshape(-1))\n",
    "        self.log(\"val_loss\", loss, on_step=True)\n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def test_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
    "        # TODO accuracy\n",
    "        raise NotImplementedError(\"Test step not implemented\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.model.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = f\"{mn}.{pn}\" if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, torch.nn.MultiheadAttention):\n",
    "                    # special case for multihead attention\n",
    "                    decay.add(fpn)\n",
    "\n",
    "        # subtle: 'embedding.weight' and 'lm_head.weight' are tied, so they\n",
    "        # will appear in the no_decay and decay sets respectively after the above.\n",
    "        # In addition, because named_parameters() doesn't return duplicates, it\n",
    "        # will only return the first occurence, key'd by 'embedding.weight', below.\n",
    "        # so let's manually remove 'lm_head.weight' from decay set. This will include\n",
    "        # this tensor into optimization via embedding.weight only, and not decayed.\n",
    "        decay.remove('lm_head.weight')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.model.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": self.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        use_fused = str(DEVICE) == \"cuda\"\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=self.lr, betas=self.betas, **extra_args)\n",
    "        return optimizer\n",
    "\n",
    "    def param_count(self) -> int:\n",
    "        return self.model.param_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel = LightningNanoGPT(\n",
    "    context_len=SEQ_LEN,\n",
    "    n_embd=N_EMBD,\n",
    "    n_head=N_HEAD,\n",
    "    n_layers=N_LAYERS,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    dropout=0.2,\n",
    "    lr=0.001,\n",
    "    betas=(0.9, 0.99),\n",
    "    weight_decay=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "run_name = \"nanogpt_add_3digit_10k_bal\"\n",
    "run_dir = CHECKPOINTS_DIR / run_name\n",
    "run_dir.mkdir(exist_ok=True, parents=True)\n",
    "checkpoint_callback = L.pytorch.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_top_k=1, mode=\"min\", dirpath=run_dir, filename=\"{step}-{train_loss:.4f}-{val_loss:.4f}\")\n",
    "trainer = L.Trainer(\n",
    "    logger=L.pytorch.loggers.WandbLogger(project=\"msc-thesis-pilot\", name=run_name, save_dir=ROOT_DIR, log_model=True),\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_steps=1000,\n",
    "    val_check_interval=10,\n",
    "    log_every_n_steps=1,\n",
    "    gradient_clip_val=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miibrahimli\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/imran/Desktop/studies/thesis/msc_thesis/wandb/run-20240222_012254-2t1lcmoj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iibrahimli/msc-thesis-pilot/runs/2t1lcmoj' target=\"_blank\">nanogpt_add_3digit_10k_bal</a></strong> to <a href='https://wandb.ai/iibrahimli/msc-thesis-pilot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iibrahimli/msc-thesis-pilot' target=\"_blank\">https://wandb.ai/iibrahimli/msc-thesis-pilot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iibrahimli/msc-thesis-pilot/runs/2t1lcmoj' target=\"_blank\">https://wandb.ai/iibrahimli/msc-thesis-pilot/runs/2t1lcmoj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | NanoGPT | 12.5 M\n",
      "----------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "49.842    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imran/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imran/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42:  42%|████▏     | 5/12 [00:06<00:09,  0.72it/s, v_num=cmoj] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imran/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(lmodel, ldm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis-P7I560r2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
