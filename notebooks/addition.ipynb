{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial experiment: small vanilla transformer (NanoGPT) trained on plain addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from arithmetic_lm.utils import get_torch_device, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "DEVICE = get_torch_device()\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "set_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyTransformer(nn.Module):\n",
    "    \"\"\"Simple small decoder-only transformer model using nn.TransformerDecoder.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_len: int,\n",
    "        n_embd: int,\n",
    "        n_head: int,\n",
    "        n_layers: int,\n",
    "        vocab_size: int,\n",
    "        ff_factor: int = 4,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            context_len: context length, i.e. the number of expected features in the input\n",
    "            n_embd: dimensionality of model embeddings\n",
    "            n_head: number of heads in the multi-head attention\n",
    "            n_layers: number of nn.TransformerDecoderLayer layers\n",
    "            vocab_size: size of the vocabulary\n",
    "            ff_factor: factor by which to scale the hidden layer dimensionality in the feedforward layer\n",
    "            dropout: dropout probability\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ff_factor = ff_factor\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_encoder = PositionalEncoding(n_embd, max_len=context_len, dropout=dropout)\n",
    "\n",
    "        # Same as decoder layer essentially, but without cross attention\n",
    "        self.layer = nn.TransformerEncoderLayer(\n",
    "            d_model=n_embd,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=n_embd * ff_factor,\n",
    "            dropout=dropout,\n",
    "\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.layer,\n",
    "            num_layers=n_layers,\n",
    "            norm=nn.LayerNorm(n_embd),\n",
    "        )\n",
    "\n",
    "        # Output\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size]``\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x, is_causal=True, mask=torch.nn.Transformer.generate_square_subsequent_mask(self.context_len))\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int, temperature: float = 1.0, top_k: int = None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (seq_len, batch)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop to context_len if necessary\n",
    "            idx_cond = idx if idx.size(0) <= self.context_len else idx[-self.context_len:, :]\n",
    "            logits = self.forward(idx_cond)\n",
    "            # get logits at final step and apply temperature\n",
    "            logits = logits[-1:, :, :] / temperature\n",
    "            # optionally apply top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(0)), dim=-1)\n",
    "                logits[logits < v[:, :, [-1]]] = -float(\"inf\")\n",
    "            # apply softmax\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            next_token = torch.multinomial(probs[0], num_samples=1)\n",
    "            # append to the sequence\n",
    "            idx = torch.cat([idx, next_token], dim=0)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imran/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8, 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = TinyTransformer(\n",
    "    context_len=10,\n",
    "    n_embd=32,\n",
    "    n_head=4,\n",
    "    n_layers=2,\n",
    "    vocab_size=100,\n",
    "    ff_factor=4,\n",
    "    dropout=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "# test forward pass\n",
    "x = torch.randint(0, 100, (10, 8)).to(DEVICE) # (seq_len, batch_size)\n",
    "y = net(x)\n",
    "y.shape # (seq_len, batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test one batch overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arithmetic_lm.tokenizer import CharTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([17, 14, 21, 21, 24, 94, 32, 24, 27, 21], 'len:', 10, 'hello worl')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello worl\"\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens, \"len:\", len(tokens), tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to tensor\n",
    "tokens = torch.tensor(tokens).unsqueeze(1).to(DEVICE)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([80, 100]),\n",
       " tensor([17, 14, 21, 21, 24, 94, 32, 24, 27, 21], device='mps:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(-1, y.size(-1)).shape, tokens.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss: 4.75054\n",
      "[1] loss: 4.84614\n",
      "[2] loss: 4.44716\n",
      "[3] loss: 4.58616\n",
      "[4] loss: 4.19507\n",
      "[5] loss: 4.10802\n",
      "[6] loss: 3.93403\n",
      "[7] loss: 3.71056\n",
      "[8] loss: 3.73026\n",
      "[9] loss: 3.62177\n",
      "[10] loss: 3.49763\n",
      "[11] loss: 3.39667\n",
      "[12] loss: 3.31937\n",
      "[13] loss: 3.23069\n",
      "[14] loss: 3.19946\n",
      "[15] loss: 3.01413\n",
      "[16] loss: 3.04749\n",
      "[17] loss: 2.98443\n",
      "[18] loss: 2.93788\n",
      "[19] loss: 2.84480\n",
      "[20] loss: 2.77242\n",
      "[21] loss: 2.74176\n",
      "[22] loss: 2.71742\n",
      "[23] loss: 2.65807\n",
      "[24] loss: 2.60152\n",
      "[25] loss: 2.56191\n",
      "[26] loss: 2.46438\n",
      "[27] loss: 2.33367\n",
      "[28] loss: 2.21808\n",
      "[29] loss: 2.26809\n",
      "[30] loss: 2.14971\n",
      "[31] loss: 2.16049\n",
      "[32] loss: 2.10479\n",
      "[33] loss: 2.03912\n",
      "[34] loss: 2.05508\n",
      "[35] loss: 1.92739\n",
      "[36] loss: 1.92850\n",
      "[37] loss: 1.90222\n",
      "[38] loss: 1.83010\n",
      "[39] loss: 1.75390\n",
      "[40] loss: 1.82884\n",
      "[41] loss: 1.69540\n",
      "[42] loss: 1.65533\n",
      "[43] loss: 1.66301\n",
      "[44] loss: 1.57256\n",
      "[45] loss: 1.58017\n",
      "[46] loss: 1.51742\n",
      "[47] loss: 1.51938\n",
      "[48] loss: 1.45002\n",
      "[49] loss: 1.37712\n",
      "[50] loss: 1.34487\n",
      "[51] loss: 1.33578\n",
      "[52] loss: 1.34706\n",
      "[53] loss: 1.30183\n",
      "[54] loss: 1.26369\n",
      "[55] loss: 1.20557\n",
      "[56] loss: 1.11354\n",
      "[57] loss: 1.14423\n",
      "[58] loss: 1.08116\n",
      "[59] loss: 1.02922\n",
      "[60] loss: 1.02441\n",
      "[61] loss: 1.00603\n",
      "[62] loss: 0.97505\n",
      "[63] loss: 0.89312\n",
      "[64] loss: 0.84723\n",
      "[65] loss: 0.89534\n",
      "[66] loss: 0.88553\n",
      "[67] loss: 0.86411\n",
      "[68] loss: 0.81595\n",
      "[69] loss: 0.85675\n",
      "[70] loss: 0.78914\n",
      "[71] loss: 0.73983\n",
      "[72] loss: 0.76744\n",
      "[73] loss: 0.72220\n",
      "[74] loss: 0.76059\n",
      "[75] loss: 0.69212\n",
      "[76] loss: 0.63112\n",
      "[77] loss: 0.68321\n",
      "[78] loss: 0.74253\n",
      "[79] loss: 0.64582\n",
      "[80] loss: 0.59267\n",
      "[81] loss: 0.55217\n",
      "[82] loss: 0.55931\n",
      "[83] loss: 0.54099\n",
      "[84] loss: 0.61027\n",
      "[85] loss: 0.54512\n",
      "[86] loss: 0.46890\n",
      "[87] loss: 0.53926\n",
      "[88] loss: 0.58423\n",
      "[89] loss: 0.54685\n",
      "[90] loss: 0.42624\n",
      "[91] loss: 0.45931\n",
      "[92] loss: 0.44296\n",
      "[93] loss: 0.48379\n",
      "[94] loss: 0.42451\n",
      "[95] loss: 0.37357\n",
      "[96] loss: 0.39641\n",
      "[97] loss: 0.33638\n",
      "[98] loss: 0.38942\n",
      "[99] loss: 0.44121\n",
      "[100] loss: 0.33855\n",
      "[101] loss: 0.53471\n",
      "[102] loss: 0.32310\n",
      "[103] loss: 0.31195\n",
      "[104] loss: 0.30442\n",
      "[105] loss: 0.29683\n",
      "[106] loss: 0.30651\n",
      "[107] loss: 0.29468\n",
      "[108] loss: 0.28323\n",
      "[109] loss: 0.30064\n",
      "[110] loss: 0.39239\n",
      "[111] loss: 0.29712\n",
      "[112] loss: 0.32562\n",
      "[113] loss: 0.28148\n",
      "[114] loss: 0.25996\n",
      "[115] loss: 0.23935\n",
      "[116] loss: 0.23903\n",
      "[117] loss: 0.22986\n",
      "[118] loss: 0.24033\n",
      "[119] loss: 0.22473\n",
      "[120] loss: 0.21878\n",
      "[121] loss: 0.22134\n",
      "[122] loss: 0.24075\n",
      "[123] loss: 0.40519\n",
      "[124] loss: 0.24951\n",
      "[125] loss: 0.21675\n",
      "[126] loss: 0.18424\n",
      "[127] loss: 0.22936\n",
      "[128] loss: 0.17275\n",
      "[129] loss: 0.19640\n",
      "[130] loss: 0.16314\n",
      "[131] loss: 0.25264\n",
      "[132] loss: 0.22020\n",
      "[133] loss: 0.17324\n",
      "[134] loss: 0.16780\n",
      "[135] loss: 0.18032\n",
      "[136] loss: 0.18113\n",
      "[137] loss: 0.15973\n",
      "[138] loss: 0.16266\n",
      "[139] loss: 0.15991\n",
      "[140] loss: 0.14642\n",
      "[141] loss: 0.21344\n",
      "[142] loss: 0.14333\n",
      "[143] loss: 0.13759\n",
      "[144] loss: 0.15374\n",
      "[145] loss: 0.16801\n",
      "[146] loss: 0.19065\n",
      "[147] loss: 0.17755\n",
      "[148] loss: 0.13896\n",
      "[149] loss: 0.13810\n",
      "[150] loss: 0.12280\n",
      "[151] loss: 0.14373\n",
      "[152] loss: 0.14148\n",
      "[153] loss: 0.18086\n",
      "[154] loss: 0.15436\n",
      "[155] loss: 0.12860\n",
      "[156] loss: 0.12697\n",
      "[157] loss: 0.11410\n",
      "[158] loss: 0.12290\n",
      "[159] loss: 0.11402\n",
      "[160] loss: 0.21543\n",
      "[161] loss: 0.12575\n",
      "[162] loss: 0.10829\n",
      "[163] loss: 0.12761\n",
      "[164] loss: 0.09939\n",
      "[165] loss: 0.11937\n",
      "[166] loss: 0.12838\n",
      "[167] loss: 0.10833\n",
      "[168] loss: 0.08732\n",
      "[169] loss: 0.10603\n",
      "[170] loss: 0.10587\n",
      "[171] loss: 0.09043\n",
      "[172] loss: 0.10605\n",
      "[173] loss: 0.09990\n",
      "[174] loss: 0.11250\n",
      "[175] loss: 0.08635\n",
      "[176] loss: 0.09372\n",
      "[177] loss: 0.07942\n",
      "[178] loss: 0.08569\n",
      "[179] loss: 0.07414\n",
      "[180] loss: 0.09165\n",
      "[181] loss: 0.08442\n",
      "[182] loss: 0.09482\n",
      "[183] loss: 0.07883\n",
      "[184] loss: 0.08873\n",
      "[185] loss: 0.08568\n",
      "[186] loss: 0.08678\n",
      "[187] loss: 0.08041\n",
      "[188] loss: 0.07082\n",
      "[189] loss: 0.08953\n",
      "[190] loss: 0.09041\n",
      "[191] loss: 0.08714\n",
      "[192] loss: 0.07244\n",
      "[193] loss: 0.07314\n",
      "[194] loss: 0.07863\n",
      "[195] loss: 0.07329\n",
      "[196] loss: 0.07100\n",
      "[197] loss: 0.07276\n",
      "[198] loss: 0.07327\n",
      "[199] loss: 0.07719\n",
      "[200] loss: 0.08334\n",
      "[201] loss: 0.06743\n",
      "[202] loss: 0.07140\n",
      "[203] loss: 0.07419\n",
      "[204] loss: 0.06602\n",
      "[205] loss: 0.06342\n",
      "[206] loss: 0.06176\n",
      "[207] loss: 0.06809\n",
      "[208] loss: 0.07179\n",
      "[209] loss: 0.07738\n",
      "[210] loss: 0.05643\n",
      "[211] loss: 0.06599\n",
      "[212] loss: 0.06703\n",
      "[213] loss: 0.12686\n",
      "[214] loss: 0.05683\n",
      "[215] loss: 0.05324\n",
      "[216] loss: 0.06173\n",
      "[217] loss: 0.06225\n",
      "[218] loss: 0.22604\n",
      "[219] loss: 0.17149\n",
      "[220] loss: 0.10331\n",
      "[221] loss: 0.06654\n",
      "[222] loss: 0.07002\n",
      "[223] loss: 0.06607\n",
      "[224] loss: 0.09113\n",
      "[225] loss: 0.04851\n",
      "[226] loss: 0.08663\n",
      "[227] loss: 0.05403\n",
      "[228] loss: 0.05073\n",
      "[229] loss: 0.05620\n",
      "[230] loss: 0.06147\n",
      "[231] loss: 0.05246\n",
      "[232] loss: 0.04841\n",
      "[233] loss: 0.05163\n",
      "[234] loss: 0.04798\n",
      "[235] loss: 0.33789\n",
      "[236] loss: 0.05718\n",
      "[237] loss: 0.04686\n",
      "[238] loss: 0.05800\n",
      "[239] loss: 0.05171\n",
      "[240] loss: 0.04981\n",
      "[241] loss: 0.06111\n",
      "[242] loss: 0.08529\n",
      "[243] loss: 0.04611\n",
      "[244] loss: 0.04942\n",
      "[245] loss: 0.06991\n",
      "[246] loss: 0.04577\n",
      "[247] loss: 0.04300\n",
      "[248] loss: 0.04207\n",
      "[249] loss: 0.04112\n",
      "[250] loss: 0.04513\n",
      "[251] loss: 0.32927\n",
      "[252] loss: 0.04396\n",
      "[253] loss: 0.04638\n",
      "[254] loss: 0.04439\n",
      "[255] loss: 0.05079\n",
      "[256] loss: 0.04326\n",
      "[257] loss: 0.04690\n",
      "[258] loss: 0.04902\n",
      "[259] loss: 0.05657\n",
      "[260] loss: 0.04785\n",
      "[261] loss: 0.05217\n",
      "[262] loss: 0.41956\n",
      "[263] loss: 0.12920\n",
      "[264] loss: 0.04303\n",
      "[265] loss: 0.03623\n",
      "[266] loss: 0.04559\n",
      "[267] loss: 0.03674\n",
      "[268] loss: 0.03915\n",
      "[269] loss: 0.03983\n",
      "[270] loss: 0.04263\n",
      "[271] loss: 0.12919\n",
      "[272] loss: 0.04184\n",
      "[273] loss: 0.04101\n",
      "[274] loss: 0.03836\n",
      "[275] loss: 0.20468\n",
      "[276] loss: 0.05585\n",
      "[277] loss: 0.05038\n",
      "[278] loss: 0.09945\n",
      "[279] loss: 0.14858\n",
      "[280] loss: 0.03274\n",
      "[281] loss: 0.09492\n",
      "[282] loss: 0.22541\n",
      "[283] loss: 0.04311\n",
      "[284] loss: 0.08214\n",
      "[285] loss: 0.06720\n",
      "[286] loss: 0.03753\n",
      "[287] loss: 0.03946\n",
      "[288] loss: 0.03288\n",
      "[289] loss: 0.03371\n",
      "[290] loss: 0.03307\n",
      "[291] loss: 0.08823\n",
      "[292] loss: 0.03407\n",
      "[293] loss: 0.03923\n",
      "[294] loss: 0.03755\n",
      "[295] loss: 0.04016\n",
      "[296] loss: 0.03824\n",
      "[297] loss: 0.28535\n",
      "[298] loss: 0.04015\n",
      "[299] loss: 0.03186\n",
      "[300] loss: 0.04218\n",
      "[301] loss: 0.03956\n",
      "[302] loss: 0.04518\n",
      "[303] loss: 0.06727\n",
      "[304] loss: 0.04209\n",
      "[305] loss: 0.17316\n",
      "[306] loss: 0.15234\n",
      "[307] loss: 0.03191\n",
      "[308] loss: 0.10426\n",
      "[309] loss: 0.03794\n",
      "[310] loss: 0.03674\n",
      "[311] loss: 0.03244\n",
      "[312] loss: 0.03059\n",
      "[313] loss: 0.05067\n",
      "[314] loss: 0.03365\n",
      "[315] loss: 0.03134\n",
      "[316] loss: 0.09040\n",
      "[317] loss: 0.04821\n",
      "[318] loss: 0.03802\n",
      "[319] loss: 0.11022\n",
      "[320] loss: 0.03411\n",
      "[321] loss: 0.03003\n",
      "[322] loss: 0.03932\n",
      "[323] loss: 0.04447\n",
      "[324] loss: 0.02685\n",
      "[325] loss: 0.02924\n",
      "[326] loss: 0.02971\n",
      "[327] loss: 0.02647\n",
      "[328] loss: 0.02591\n",
      "[329] loss: 0.02750\n",
      "[330] loss: 0.03032\n",
      "[331] loss: 0.03049\n",
      "[332] loss: 0.03095\n",
      "[333] loss: 0.03211\n",
      "[334] loss: 0.02756\n",
      "[335] loss: 0.02725\n",
      "[336] loss: 0.03222\n",
      "[337] loss: 0.02893\n",
      "[338] loss: 0.02632\n",
      "[339] loss: 0.02988\n",
      "[340] loss: 0.02613\n",
      "[341] loss: 0.02748\n",
      "[342] loss: 0.02581\n",
      "[343] loss: 0.02391\n",
      "[344] loss: 0.02417\n",
      "[345] loss: 0.02748\n",
      "[346] loss: 0.04012\n",
      "[347] loss: 0.03429\n",
      "[348] loss: 0.02981\n",
      "[349] loss: 0.03359\n",
      "[350] loss: 0.03139\n",
      "[351] loss: 0.02577\n",
      "[352] loss: 0.02848\n",
      "[353] loss: 0.03236\n",
      "[354] loss: 0.02219\n",
      "[355] loss: 0.02300\n",
      "[356] loss: 0.02861\n",
      "[357] loss: 0.03243\n",
      "[358] loss: 0.02457\n",
      "[359] loss: 0.02843\n",
      "[360] loss: 0.02424\n",
      "[361] loss: 0.02372\n",
      "[362] loss: 0.02179\n",
      "[363] loss: 0.02275\n",
      "[364] loss: 0.02083\n",
      "[365] loss: 0.02188\n",
      "[366] loss: 0.12377\n",
      "[367] loss: 0.02311\n",
      "[368] loss: 0.02039\n",
      "[369] loss: 0.02132\n",
      "[370] loss: 0.02569\n",
      "[371] loss: 0.07723\n",
      "[372] loss: 0.02882\n",
      "[373] loss: 0.02266\n",
      "[374] loss: 0.02076\n",
      "[375] loss: 0.06402\n",
      "[376] loss: 0.02325\n",
      "[377] loss: 0.02156\n",
      "[378] loss: 0.01939\n",
      "[379] loss: 0.02028\n",
      "[380] loss: 0.02112\n",
      "[381] loss: 0.01845\n",
      "[382] loss: 0.01937\n",
      "[383] loss: 0.01882\n",
      "[384] loss: 0.02190\n",
      "[385] loss: 0.02137\n",
      "[386] loss: 0.02027\n",
      "[387] loss: 0.01976\n",
      "[388] loss: 0.01813\n",
      "[389] loss: 0.02328\n",
      "[390] loss: 0.01831\n",
      "[391] loss: 0.02107\n",
      "[392] loss: 0.01802\n",
      "[393] loss: 0.02322\n",
      "[394] loss: 0.08325\n",
      "[395] loss: 0.01980\n",
      "[396] loss: 0.01740\n",
      "[397] loss: 0.01894\n",
      "[398] loss: 0.03777\n",
      "[399] loss: 0.01887\n",
      "[400] loss: 0.01946\n",
      "[401] loss: 0.01961\n",
      "[402] loss: 0.01713\n",
      "[403] loss: 0.03609\n",
      "[404] loss: 0.01639\n",
      "[405] loss: 0.01690\n",
      "[406] loss: 0.01871\n",
      "[407] loss: 0.04040\n",
      "[408] loss: 0.01631\n",
      "[409] loss: 0.01562\n",
      "[410] loss: 0.01816\n",
      "[411] loss: 0.01934\n",
      "[412] loss: 0.01651\n",
      "[413] loss: 0.01571\n",
      "[414] loss: 0.01718\n",
      "[415] loss: 0.01600\n",
      "[416] loss: 0.01835\n",
      "[417] loss: 0.01796\n",
      "[418] loss: 0.01738\n",
      "[419] loss: 0.01639\n",
      "[420] loss: 0.01831\n",
      "[421] loss: 0.01575\n",
      "[422] loss: 0.01617\n",
      "[423] loss: 0.01591\n",
      "[424] loss: 0.02173\n",
      "[425] loss: 0.01989\n",
      "[426] loss: 0.01507\n",
      "[427] loss: 0.01568\n",
      "[428] loss: 0.01531\n",
      "[429] loss: 0.01754\n",
      "[430] loss: 0.01586\n",
      "[431] loss: 0.01747\n",
      "[432] loss: 0.02235\n",
      "[433] loss: 0.01400\n",
      "[434] loss: 0.02145\n",
      "[435] loss: 0.01509\n",
      "[436] loss: 0.01821\n",
      "[437] loss: 0.01393\n",
      "[438] loss: 0.01761\n",
      "[439] loss: 0.01538\n",
      "[440] loss: 0.01377\n",
      "[441] loss: 0.01797\n",
      "[442] loss: 0.01732\n",
      "[443] loss: 0.01534\n",
      "[444] loss: 0.04845\n",
      "[445] loss: 0.01427\n",
      "[446] loss: 0.01606\n",
      "[447] loss: 0.01495\n",
      "[448] loss: 0.01756\n",
      "[449] loss: 0.01782\n",
      "[450] loss: 0.01507\n",
      "[451] loss: 0.01548\n",
      "[452] loss: 0.13708\n",
      "[453] loss: 0.01438\n",
      "[454] loss: 0.02853\n",
      "[455] loss: 0.01604\n",
      "[456] loss: 0.01246\n",
      "[457] loss: 0.01409\n",
      "[458] loss: 0.01315\n",
      "[459] loss: 0.01273\n",
      "[460] loss: 0.01357\n",
      "[461] loss: 0.01311\n",
      "[462] loss: 0.01339\n",
      "[463] loss: 0.01706\n",
      "[464] loss: 0.01380\n",
      "[465] loss: 0.01607\n",
      "[466] loss: 0.03811\n",
      "[467] loss: 0.01218\n",
      "[468] loss: 0.01606\n",
      "[469] loss: 0.01475\n",
      "[470] loss: 0.01544\n",
      "[471] loss: 0.01465\n",
      "[472] loss: 0.01804\n",
      "[473] loss: 0.01401\n",
      "[474] loss: 0.01622\n",
      "[475] loss: 0.01196\n",
      "[476] loss: 0.01253\n",
      "[477] loss: 0.01375\n",
      "[478] loss: 0.01354\n",
      "[479] loss: 0.01260\n",
      "[480] loss: 0.02336\n",
      "[481] loss: 0.49597\n",
      "[482] loss: 0.01240\n",
      "[483] loss: 0.01894\n",
      "[484] loss: 0.01324\n",
      "[485] loss: 0.01199\n",
      "[486] loss: 0.01519\n",
      "[487] loss: 0.01257\n",
      "[488] loss: 0.19079\n",
      "[489] loss: 0.01419\n",
      "[490] loss: 0.01248\n",
      "[491] loss: 0.01313\n",
      "[492] loss: 0.48027\n",
      "[493] loss: 0.01527\n",
      "[494] loss: 0.01344\n",
      "[495] loss: 0.01723\n",
      "[496] loss: 0.01733\n",
      "[497] loss: 0.01242\n",
      "[498] loss: 0.04691\n",
      "[499] loss: 0.01550\n",
      "[500] loss: 0.07239\n",
      "[501] loss: 0.01557\n",
      "[502] loss: 0.27971\n",
      "[503] loss: 0.01425\n",
      "[504] loss: 0.01448\n",
      "[505] loss: 0.01410\n",
      "[506] loss: 0.01555\n",
      "[507] loss: 0.01990\n",
      "[508] loss: 0.01296\n",
      "[509] loss: 0.01227\n",
      "[510] loss: 0.01226\n",
      "[511] loss: 0.01475\n",
      "[512] loss: 0.01761\n",
      "[513] loss: 0.11522\n",
      "[514] loss: 0.01530\n",
      "[515] loss: 0.01348\n",
      "[516] loss: 0.01482\n",
      "[517] loss: 0.01668\n",
      "[518] loss: 0.01416\n",
      "[519] loss: 0.01456\n",
      "[520] loss: 0.01375\n",
      "[521] loss: 0.02029\n",
      "[522] loss: 0.01616\n",
      "[523] loss: 0.01467\n",
      "[524] loss: 0.10742\n",
      "[525] loss: 0.01243\n",
      "[526] loss: 0.02006\n",
      "[527] loss: 0.01260\n",
      "[528] loss: 0.06223\n",
      "[529] loss: 0.01311\n",
      "[530] loss: 0.01619\n",
      "[531] loss: 0.01095\n",
      "[532] loss: 0.01365\n",
      "[533] loss: 0.01097\n",
      "[534] loss: 0.01316\n",
      "[535] loss: 0.01201\n",
      "[536] loss: 0.01132\n",
      "[537] loss: 0.01242\n",
      "[538] loss: 0.01053\n",
      "[539] loss: 0.01154\n",
      "[540] loss: 0.01120\n",
      "[541] loss: 0.01255\n",
      "[542] loss: 0.01179\n",
      "[543] loss: 0.00981\n",
      "[544] loss: 0.01269\n",
      "[545] loss: 0.01066\n",
      "[546] loss: 0.01198\n",
      "[547] loss: 0.01000\n",
      "[548] loss: 0.01006\n",
      "[549] loss: 0.00967\n",
      "[550] loss: 0.01269\n",
      "[551] loss: 0.00957\n",
      "[552] loss: 0.01210\n",
      "[553] loss: 0.01195\n",
      "[554] loss: 0.01006\n",
      "[555] loss: 0.01483\n",
      "[556] loss: 0.01179\n",
      "[557] loss: 0.16525\n",
      "[558] loss: 0.00970\n",
      "[559] loss: 0.01455\n",
      "[560] loss: 0.01137\n",
      "[561] loss: 0.01097\n",
      "[562] loss: 0.01062\n",
      "[563] loss: 0.00991\n",
      "[564] loss: 0.01167\n",
      "[565] loss: 0.01474\n",
      "[566] loss: 0.01070\n",
      "[567] loss: 0.01117\n",
      "[568] loss: 0.19261\n",
      "[569] loss: 0.01288\n",
      "[570] loss: 0.01324\n",
      "[571] loss: 0.00982\n",
      "[572] loss: 0.01010\n",
      "[573] loss: 0.01297\n",
      "[574] loss: 0.01208\n",
      "[575] loss: 0.04344\n",
      "[576] loss: 0.01110\n",
      "[577] loss: 0.01112\n",
      "[578] loss: 0.01209\n",
      "[579] loss: 0.01077\n",
      "[580] loss: 0.02779\n",
      "[581] loss: 0.01308\n",
      "[582] loss: 0.01162\n",
      "[583] loss: 0.00902\n",
      "[584] loss: 0.00912\n",
      "[585] loss: 0.00958\n",
      "[586] loss: 0.00950\n",
      "[587] loss: 0.02693\n",
      "[588] loss: 0.01086\n",
      "[589] loss: 0.01371\n",
      "[590] loss: 0.01057\n",
      "[591] loss: 0.01010\n",
      "[592] loss: 0.01288\n",
      "[593] loss: 0.01042\n",
      "[594] loss: 0.00948\n",
      "[595] loss: 0.01133\n",
      "[596] loss: 0.02853\n",
      "[597] loss: 0.01191\n",
      "[598] loss: 0.00985\n",
      "[599] loss: 0.01299\n",
      "[600] loss: 0.01921\n",
      "[601] loss: 0.00851\n",
      "[602] loss: 0.01165\n",
      "[603] loss: 0.00964\n",
      "[604] loss: 0.00885\n",
      "[605] loss: 0.01000\n",
      "[606] loss: 0.00834\n",
      "[607] loss: 0.01367\n",
      "[608] loss: 0.00866\n",
      "[609] loss: 0.00864\n",
      "[610] loss: 0.00929\n",
      "[611] loss: 0.00847\n",
      "[612] loss: 0.00977\n",
      "[613] loss: 0.00788\n",
      "[614] loss: 0.01230\n",
      "[615] loss: 0.01016\n",
      "[616] loss: 0.00830\n",
      "[617] loss: 0.00775\n",
      "[618] loss: 0.00796\n",
      "[619] loss: 0.01570\n",
      "[620] loss: 0.00998\n",
      "[621] loss: 0.00913\n",
      "[622] loss: 0.00855\n",
      "[623] loss: 0.00866\n",
      "[624] loss: 0.00844\n",
      "[625] loss: 0.06506\n",
      "[626] loss: 0.00895\n",
      "[627] loss: 0.00836\n",
      "[628] loss: 0.01000\n",
      "[629] loss: 0.05264\n",
      "[630] loss: 0.00854\n",
      "[631] loss: 0.00820\n",
      "[632] loss: 0.00715\n",
      "[633] loss: 0.00840\n",
      "[634] loss: 0.00972\n",
      "[635] loss: 0.01035\n",
      "[636] loss: 0.00857\n",
      "[637] loss: 0.00821\n",
      "[638] loss: 0.00946\n",
      "[639] loss: 0.00805\n",
      "[640] loss: 0.00940\n",
      "[641] loss: 0.00836\n",
      "[642] loss: 0.00821\n",
      "[643] loss: 0.00832\n",
      "[644] loss: 0.00731\n",
      "[645] loss: 0.00757\n",
      "[646] loss: 0.00747\n",
      "[647] loss: 0.00897\n",
      "[648] loss: 0.00887\n",
      "[649] loss: 0.00855\n",
      "[650] loss: 0.00767\n",
      "[651] loss: 0.27532\n",
      "[652] loss: 0.00744\n",
      "[653] loss: 0.00772\n",
      "[654] loss: 0.00737\n",
      "[655] loss: 0.01969\n",
      "[656] loss: 0.00673\n",
      "[657] loss: 0.00955\n",
      "[658] loss: 0.01316\n",
      "[659] loss: 0.00864\n",
      "[660] loss: 0.01137\n",
      "[661] loss: 0.00804\n",
      "[662] loss: 0.01002\n",
      "[663] loss: 0.00850\n",
      "[664] loss: 0.00842\n",
      "[665] loss: 0.01032\n",
      "[666] loss: 0.00839\n",
      "[667] loss: 0.02344\n",
      "[668] loss: 0.00771\n",
      "[669] loss: 0.01078\n",
      "[670] loss: 0.00692\n",
      "[671] loss: 0.44178\n",
      "[672] loss: 0.00748\n",
      "[673] loss: 0.00705\n",
      "[674] loss: 0.00660\n",
      "[675] loss: 0.00809\n",
      "[676] loss: 0.00914\n",
      "[677] loss: 0.00811\n",
      "[678] loss: 0.00912\n",
      "[679] loss: 0.00911\n",
      "[680] loss: 0.00957\n",
      "[681] loss: 0.00785\n",
      "[682] loss: 0.00749\n",
      "[683] loss: 0.00893\n",
      "[684] loss: 0.00756\n",
      "[685] loss: 0.00776\n",
      "[686] loss: 0.00929\n",
      "[687] loss: 0.00945\n",
      "[688] loss: 0.00983\n",
      "[689] loss: 0.00752\n",
      "[690] loss: 0.00858\n",
      "[691] loss: 0.11599\n",
      "[692] loss: 0.00732\n",
      "[693] loss: 0.00740\n",
      "[694] loss: 0.00877\n",
      "[695] loss: 0.00670\n",
      "[696] loss: 0.06181\n",
      "[697] loss: 0.00684\n",
      "[698] loss: 0.04328\n",
      "[699] loss: 0.00677\n",
      "[700] loss: 0.00783\n",
      "[701] loss: 0.05367\n",
      "[702] loss: 0.00766\n",
      "[703] loss: 0.00681\n",
      "[704] loss: 0.00733\n",
      "[705] loss: 0.00810\n",
      "[706] loss: 0.01435\n",
      "[707] loss: 0.00722\n",
      "[708] loss: 0.01345\n",
      "[709] loss: 0.00622\n",
      "[710] loss: 0.00744\n",
      "[711] loss: 0.00790\n",
      "[712] loss: 0.01402\n",
      "[713] loss: 0.00696\n",
      "[714] loss: 0.00892\n",
      "[715] loss: 0.00802\n",
      "[716] loss: 0.00992\n",
      "[717] loss: 0.00633\n",
      "[718] loss: 0.00739\n",
      "[719] loss: 0.00678\n",
      "[720] loss: 0.00686\n",
      "[721] loss: 0.00675\n",
      "[722] loss: 0.00770\n",
      "[723] loss: 0.00761\n",
      "[724] loss: 0.00581\n",
      "[725] loss: 0.00678\n",
      "[726] loss: 0.00706\n",
      "[727] loss: 0.00745\n",
      "[728] loss: 0.00658\n",
      "[729] loss: 0.00902\n",
      "[730] loss: 0.00660\n",
      "[731] loss: 0.00627\n",
      "[732] loss: 0.00690\n",
      "[733] loss: 0.00752\n",
      "[734] loss: 0.00667\n",
      "[735] loss: 0.00756\n",
      "[736] loss: 0.00727\n",
      "[737] loss: 0.00590\n",
      "[738] loss: 0.00657\n",
      "[739] loss: 0.00670\n",
      "[740] loss: 0.00742\n",
      "[741] loss: 0.00558\n",
      "[742] loss: 0.00714\n",
      "[743] loss: 0.00739\n",
      "[744] loss: 0.00649\n",
      "[745] loss: 0.00609\n",
      "[746] loss: 0.00624\n",
      "[747] loss: 0.00615\n",
      "[748] loss: 0.00642\n",
      "[749] loss: 0.00665\n",
      "[750] loss: 0.00638\n",
      "[751] loss: 0.00651\n",
      "[752] loss: 0.00662\n",
      "[753] loss: 0.00706\n",
      "[754] loss: 0.00795\n",
      "[755] loss: 0.00692\n",
      "[756] loss: 0.00542\n",
      "[757] loss: 0.01004\n",
      "[758] loss: 0.00568\n",
      "[759] loss: 0.00563\n",
      "[760] loss: 0.00722\n",
      "[761] loss: 0.00731\n",
      "[762] loss: 0.00853\n",
      "[763] loss: 0.00577\n",
      "[764] loss: 0.00604\n",
      "[765] loss: 0.00673\n",
      "[766] loss: 0.00511\n",
      "[767] loss: 0.00919\n",
      "[768] loss: 0.00606\n",
      "[769] loss: 0.00664\n",
      "[770] loss: 0.00574\n",
      "[771] loss: 0.00579\n",
      "[772] loss: 0.00622\n",
      "[773] loss: 0.00574\n",
      "[774] loss: 0.00554\n",
      "[775] loss: 0.00714\n",
      "[776] loss: 0.00547\n",
      "[777] loss: 0.00612\n",
      "[778] loss: 0.00537\n",
      "[779] loss: 0.00581\n",
      "[780] loss: 0.00559\n",
      "[781] loss: 0.00626\n",
      "[782] loss: 0.00535\n",
      "[783] loss: 0.00581\n",
      "[784] loss: 0.00515\n",
      "[785] loss: 0.00555\n",
      "[786] loss: 0.00501\n",
      "[787] loss: 0.00575\n",
      "[788] loss: 0.00558\n",
      "[789] loss: 0.00580\n",
      "[790] loss: 0.00549\n",
      "[791] loss: 0.00475\n",
      "[792] loss: 0.00473\n",
      "[793] loss: 0.00617\n",
      "[794] loss: 0.00452\n",
      "[795] loss: 0.00623\n",
      "[796] loss: 0.00480\n",
      "[797] loss: 0.00527\n",
      "[798] loss: 0.00553\n",
      "[799] loss: 0.00490\n",
      "[800] loss: 0.00580\n",
      "[801] loss: 0.00527\n",
      "[802] loss: 0.00477\n",
      "[803] loss: 0.00593\n",
      "[804] loss: 0.00502\n",
      "[805] loss: 0.00677\n",
      "[806] loss: 0.00588\n",
      "[807] loss: 0.00525\n",
      "[808] loss: 0.00536\n",
      "[809] loss: 0.00495\n",
      "[810] loss: 0.00648\n",
      "[811] loss: 0.00636\n",
      "[812] loss: 0.00846\n",
      "[813] loss: 0.00607\n",
      "[814] loss: 0.00498\n",
      "[815] loss: 0.00483\n",
      "[816] loss: 0.00596\n",
      "[817] loss: 0.00507\n",
      "[818] loss: 0.00479\n",
      "[819] loss: 0.01085\n",
      "[820] loss: 0.00804\n",
      "[821] loss: 0.00598\n",
      "[822] loss: 0.00463\n",
      "[823] loss: 0.00455\n",
      "[824] loss: 0.00419\n",
      "[825] loss: 0.00577\n",
      "[826] loss: 0.00629\n",
      "[827] loss: 0.00499\n",
      "[828] loss: 0.00464\n",
      "[829] loss: 0.00434\n",
      "[830] loss: 0.00475\n",
      "[831] loss: 0.00431\n",
      "[832] loss: 0.00780\n",
      "[833] loss: 0.00860\n",
      "[834] loss: 0.00604\n",
      "[835] loss: 0.00430\n",
      "[836] loss: 0.00567\n",
      "[837] loss: 0.00504\n",
      "[838] loss: 0.00595\n",
      "[839] loss: 0.00522\n",
      "[840] loss: 0.00479\n",
      "[841] loss: 0.07608\n",
      "[842] loss: 0.00511\n",
      "[843] loss: 0.00561\n",
      "[844] loss: 0.00569\n",
      "[845] loss: 0.00532\n",
      "[846] loss: 0.00417\n",
      "[847] loss: 0.00447\n",
      "[848] loss: 0.00504\n",
      "[849] loss: 0.00475\n",
      "[850] loss: 0.00475\n",
      "[851] loss: 0.00447\n",
      "[852] loss: 0.00515\n",
      "[853] loss: 0.00516\n",
      "[854] loss: 0.00554\n",
      "[855] loss: 0.00637\n",
      "[856] loss: 0.00434\n",
      "[857] loss: 0.00669\n",
      "[858] loss: 0.00587\n",
      "[859] loss: 0.00721\n",
      "[860] loss: 0.00464\n",
      "[861] loss: 0.00495\n",
      "[862] loss: 0.00762\n",
      "[863] loss: 0.00663\n",
      "[864] loss: 0.00517\n",
      "[865] loss: 0.00515\n",
      "[866] loss: 0.23598\n",
      "[867] loss: 0.00435\n",
      "[868] loss: 0.00502\n",
      "[869] loss: 0.00575\n",
      "[870] loss: 0.00573\n",
      "[871] loss: 0.00520\n",
      "[872] loss: 0.06227\n",
      "[873] loss: 0.00408\n",
      "[874] loss: 0.00475\n",
      "[875] loss: 0.00469\n",
      "[876] loss: 0.00454\n",
      "[877] loss: 0.00457\n",
      "[878] loss: 0.22323\n",
      "[879] loss: 0.34602\n",
      "[880] loss: 0.00441\n",
      "[881] loss: 0.00601\n",
      "[882] loss: 0.00483\n",
      "[883] loss: 0.00620\n",
      "[884] loss: 0.00472\n",
      "[885] loss: 0.00561\n",
      "[886] loss: 0.00458\n",
      "[887] loss: 0.25192\n",
      "[888] loss: 0.00580\n",
      "[889] loss: 0.00547\n",
      "[890] loss: 0.00544\n",
      "[891] loss: 0.00471\n",
      "[892] loss: 0.01547\n",
      "[893] loss: 0.02666\n",
      "[894] loss: 0.01243\n",
      "[895] loss: 0.00603\n",
      "[896] loss: 0.00667\n",
      "[897] loss: 0.00494\n",
      "[898] loss: 0.00534\n",
      "[899] loss: 0.00586\n",
      "[900] loss: 0.00670\n",
      "[901] loss: 0.00956\n",
      "[902] loss: 0.00576\n",
      "[903] loss: 0.00542\n",
      "[904] loss: 0.00500\n",
      "[905] loss: 0.01925\n",
      "[906] loss: 0.00610\n",
      "[907] loss: 0.00573\n",
      "[908] loss: 0.00532\n",
      "[909] loss: 0.00846\n",
      "[910] loss: 0.00655\n",
      "[911] loss: 0.00456\n",
      "[912] loss: 0.00665\n",
      "[913] loss: 0.00903\n",
      "[914] loss: 0.00742\n",
      "[915] loss: 0.01983\n",
      "[916] loss: 0.00400\n",
      "[917] loss: 0.00494\n",
      "[918] loss: 0.00868\n",
      "[919] loss: 0.00556\n",
      "[920] loss: 0.00446\n",
      "[921] loss: 0.00700\n",
      "[922] loss: 0.00477\n",
      "[923] loss: 0.00603\n",
      "[924] loss: 0.00454\n",
      "[925] loss: 0.00459\n",
      "[926] loss: 0.02723\n",
      "[927] loss: 0.00501\n",
      "[928] loss: 0.00404\n",
      "[929] loss: 0.05121\n",
      "[930] loss: 0.00405\n",
      "[931] loss: 0.00788\n",
      "[932] loss: 0.00708\n",
      "[933] loss: 0.00616\n",
      "[934] loss: 0.00574\n",
      "[935] loss: 0.00676\n",
      "[936] loss: 0.00428\n",
      "[937] loss: 0.01238\n",
      "[938] loss: 0.02907\n",
      "[939] loss: 0.00567\n",
      "[940] loss: 0.01027\n",
      "[941] loss: 0.00440\n",
      "[942] loss: 0.00498\n",
      "[943] loss: 0.00876\n",
      "[944] loss: 0.01510\n",
      "[945] loss: 0.00597\n",
      "[946] loss: 0.00551\n",
      "[947] loss: 0.00452\n",
      "[948] loss: 0.00440\n",
      "[949] loss: 0.00570\n",
      "[950] loss: 0.00427\n",
      "[951] loss: 0.00941\n",
      "[952] loss: 0.00762\n",
      "[953] loss: 0.00397\n",
      "[954] loss: 0.00392\n",
      "[955] loss: 0.00473\n",
      "[956] loss: 0.00397\n",
      "[957] loss: 0.00414\n",
      "[958] loss: 0.00484\n",
      "[959] loss: 0.00472\n",
      "[960] loss: 0.00411\n",
      "[961] loss: 0.00353\n",
      "[962] loss: 0.00393\n",
      "[963] loss: 0.00415\n",
      "[964] loss: 0.00431\n",
      "[965] loss: 0.00404\n",
      "[966] loss: 0.00468\n",
      "[967] loss: 0.00560\n",
      "[968] loss: 0.00461\n",
      "[969] loss: 0.00399\n",
      "[970] loss: 0.00420\n",
      "[971] loss: 0.00465\n",
      "[972] loss: 0.00348\n",
      "[973] loss: 0.00504\n",
      "[974] loss: 0.00425\n",
      "[975] loss: 0.00340\n",
      "[976] loss: 0.00339\n",
      "[977] loss: 0.00328\n",
      "[978] loss: 0.00561\n",
      "[979] loss: 0.00347\n",
      "[980] loss: 0.00416\n",
      "[981] loss: 0.00412\n",
      "[982] loss: 0.00358\n",
      "[983] loss: 0.00618\n",
      "[984] loss: 0.00364\n",
      "[985] loss: 0.00334\n",
      "[986] loss: 0.00387\n",
      "[987] loss: 0.00407\n",
      "[988] loss: 0.00437\n",
      "[989] loss: 0.00485\n",
      "[990] loss: 0.00307\n",
      "[991] loss: 0.00380\n",
      "[992] loss: 0.00335\n",
      "[993] loss: 0.00339\n",
      "[994] loss: 0.00360\n",
      "[995] loss: 0.00354\n",
      "[996] loss: 0.00398\n",
      "[997] loss: 0.00616\n",
      "[998] loss: 0.00327\n",
      "[999] loss: 0.00416\n"
     ]
    }
   ],
   "source": [
    "# simplest train loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# create target by shifting tokens by 1 and adding a padding token at the end\n",
    "target = torch.cat([tokens[1:, 0], torch.tensor([65]).to(DEVICE)]).unsqueeze(-1)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    y = net(tokens)\n",
    "    loss = criterion(y.view(-1, y.size(-1)), target.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"[{i}] loss: {loss.item():.5f}\")\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello worl$l$$'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompt = \"hell\"\n",
    "tokens = tokenizer.encode(test_prompt)\n",
    "tokens = torch.tensor(tokens).unsqueeze(1).to(DEVICE)\n",
    "print(tokens.shape)\n",
    "\n",
    "net.eval()\n",
    "generated_tokens = net.generate(tokens, max_new_tokens=10, temperature=1.0, top_k=10)\n",
    "\n",
    "tokenizer.decode(generated_tokens.squeeze(1).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis-P7I560r2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
