{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial experiment: small vanilla transformer (NanoGPT) trained on plain addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from arithmetic_lm.utils import get_torch_device, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "DEVICE = get_torch_device()\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "set_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyTransformer(nn.Module):\n",
    "    \"\"\"Simple small decoder-only transformer model using nn.TransformerDecoder.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_len: int,\n",
    "        n_embd: int,\n",
    "        n_head: int,\n",
    "        n_layers: int,\n",
    "        vocab_size: int,\n",
    "        ff_factor: int = 4,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            context_len: context length, i.e. the number of expected features in the input\n",
    "            n_embd: dimensionality of model embeddings\n",
    "            n_head: number of heads in the multi-head attention\n",
    "            n_layers: number of nn.TransformerDecoderLayer layers\n",
    "            vocab_size: size of the vocabulary\n",
    "            ff_factor: factor by which to scale the hidden layer dimensionality in the feedforward layer\n",
    "            dropout: dropout probability\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ff_factor = ff_factor\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_encoder = PositionalEncoding(n_embd, max_len=context_len, dropout=dropout)\n",
    "\n",
    "        # Same as decoder layer essentially, but without cross attention\n",
    "        self.layer = nn.TransformerEncoderLayer(\n",
    "            d_model=n_embd,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=n_embd * ff_factor,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.layer,\n",
    "            num_layers=n_layers,\n",
    "            norm=nn.LayerNorm(n_embd),\n",
    "        )\n",
    "\n",
    "        # Output\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size]``\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx: Tensor, max_new_tokens: int, temperature: float = 1.0, top_k: int = None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (seq_len, batch)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop to context_len if necessary\n",
    "            idx_cond = idx if idx.size(0) <= self.context_len else idx[-self.context_len:, :]\n",
    "            logits = self.forward(idx_cond)\n",
    "            # get logits at final step and apply temperature\n",
    "            logits = logits[-1:, :, :] / temperature\n",
    "            # optionally apply top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(0)), dim=-1)\n",
    "                logits[logits < v[:, :, [-1]]] = -float(\"inf\")\n",
    "            # apply softmax\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            next_token = torch.multinomial(probs[0], num_samples=1)\n",
    "            # append to the sequence\n",
    "            idx = torch.cat([idx, next_token], dim=0)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imran/Library/Caches/pypoetry/virtualenvs/msc-thesis-P7I560r2-py3.12/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8, 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = TinyTransformer(\n",
    "    context_len=10,\n",
    "    n_embd=32,\n",
    "    n_head=4,\n",
    "    n_layers=2,\n",
    "    vocab_size=100,\n",
    "    ff_factor=4,\n",
    "    dropout=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "# test forward pass\n",
    "x = torch.randint(0, 100, (10, 8)).to(DEVICE) # (seq_len, batch_size)\n",
    "y = net(x)\n",
    "y.shape # (seq_len, batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test one batch overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arithmetic_lm.tokenizer import CharTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([17, 14, 21, 21, 24, 94, 32, 24, 27, 21], 'len:', 10, 'hello worl')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello worl\"\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens, \"len:\", len(tokens), tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to tensor\n",
    "tokens = torch.tensor(tokens).unsqueeze(1).to(DEVICE)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss: 4.83043\n",
      "[1] loss: 4.67797\n",
      "[2] loss: 4.67896\n",
      "[3] loss: 4.57869\n",
      "[4] loss: 4.26835\n",
      "[5] loss: 4.10940\n",
      "[6] loss: 3.84363\n",
      "[7] loss: 3.75419\n",
      "[8] loss: 3.67240\n",
      "[9] loss: 3.45884\n",
      "[10] loss: 3.36571\n",
      "[11] loss: 3.28346\n",
      "[12] loss: 3.17458\n",
      "[13] loss: 3.06351\n",
      "[14] loss: 2.95267\n",
      "[15] loss: 2.85019\n",
      "[16] loss: 2.79848\n",
      "[17] loss: 2.75286\n",
      "[18] loss: 2.65473\n",
      "[19] loss: 2.52025\n",
      "[20] loss: 2.63414\n",
      "[21] loss: 2.43003\n",
      "[22] loss: 2.43552\n",
      "[23] loss: 2.38354\n",
      "[24] loss: 2.29613\n",
      "[25] loss: 2.24732\n",
      "[26] loss: 2.20975\n",
      "[27] loss: 2.16064\n",
      "[28] loss: 1.95921\n",
      "[29] loss: 2.06231\n",
      "[30] loss: 1.84376\n",
      "[31] loss: 1.89070\n",
      "[32] loss: 1.85551\n",
      "[33] loss: 1.80413\n",
      "[34] loss: 1.71771\n",
      "[35] loss: 1.68575\n",
      "[36] loss: 1.59673\n",
      "[37] loss: 1.63618\n",
      "[38] loss: 1.52146\n",
      "[39] loss: 1.42685\n",
      "[40] loss: 1.46680\n",
      "[41] loss: 1.37645\n",
      "[42] loss: 1.34905\n",
      "[43] loss: 1.29195\n",
      "[44] loss: 1.25102\n",
      "[45] loss: 1.20397\n",
      "[46] loss: 1.19220\n",
      "[47] loss: 1.12903\n",
      "[48] loss: 1.14362\n",
      "[49] loss: 1.04575\n",
      "[50] loss: 1.13638\n",
      "[51] loss: 0.99771\n",
      "[52] loss: 1.00256\n",
      "[53] loss: 0.96742\n",
      "[54] loss: 0.94646\n",
      "[55] loss: 0.91000\n",
      "[56] loss: 0.87368\n",
      "[57] loss: 0.86149\n",
      "[58] loss: 0.83014\n",
      "[59] loss: 0.79952\n",
      "[60] loss: 0.74078\n",
      "[61] loss: 0.78444\n",
      "[62] loss: 0.74381\n",
      "[63] loss: 0.68721\n",
      "[64] loss: 0.72738\n",
      "[65] loss: 0.68005\n",
      "[66] loss: 0.65680\n",
      "[67] loss: 0.61538\n",
      "[68] loss: 0.62122\n",
      "[69] loss: 0.61461\n",
      "[70] loss: 0.58879\n",
      "[71] loss: 0.54897\n",
      "[72] loss: 0.54554\n",
      "[73] loss: 0.54522\n",
      "[74] loss: 0.52033\n",
      "[75] loss: 0.49559\n",
      "[76] loss: 0.50903\n",
      "[77] loss: 0.50443\n",
      "[78] loss: 0.46497\n",
      "[79] loss: 0.46376\n",
      "[80] loss: 0.46692\n",
      "[81] loss: 0.43249\n",
      "[82] loss: 0.43102\n",
      "[83] loss: 0.43540\n",
      "[84] loss: 0.42730\n",
      "[85] loss: 0.41007\n",
      "[86] loss: 0.39532\n",
      "[87] loss: 0.39361\n",
      "[88] loss: 0.37559\n",
      "[89] loss: 0.38102\n",
      "[90] loss: 0.34090\n",
      "[91] loss: 0.36043\n",
      "[92] loss: 0.32948\n",
      "[93] loss: 0.38331\n",
      "[94] loss: 0.35821\n",
      "[95] loss: 0.31213\n",
      "[96] loss: 0.32504\n",
      "[97] loss: 0.29724\n",
      "[98] loss: 0.29997\n",
      "[99] loss: 0.29866\n",
      "[100] loss: 0.27172\n",
      "[101] loss: 0.28546\n",
      "[102] loss: 0.26913\n",
      "[103] loss: 0.25231\n",
      "[104] loss: 0.25058\n",
      "[105] loss: 0.25948\n",
      "[106] loss: 0.23789\n",
      "[107] loss: 0.23654\n",
      "[108] loss: 0.26194\n",
      "[109] loss: 0.22695\n",
      "[110] loss: 0.22663\n",
      "[111] loss: 0.22197\n",
      "[112] loss: 0.21320\n",
      "[113] loss: 0.21895\n",
      "[114] loss: 0.22138\n",
      "[115] loss: 0.19831\n",
      "[116] loss: 0.20427\n",
      "[117] loss: 0.19674\n",
      "[118] loss: 0.21817\n",
      "[119] loss: 0.18885\n",
      "[120] loss: 0.18512\n",
      "[121] loss: 0.17636\n",
      "[122] loss: 0.19161\n",
      "[123] loss: 0.17496\n",
      "[124] loss: 0.16983\n",
      "[125] loss: 0.18181\n",
      "[126] loss: 0.16207\n",
      "[127] loss: 0.17263\n",
      "[128] loss: 0.17736\n",
      "[129] loss: 0.16587\n",
      "[130] loss: 0.16148\n",
      "[131] loss: 0.17706\n",
      "[132] loss: 0.16250\n",
      "[133] loss: 0.15271\n",
      "[134] loss: 0.16012\n",
      "[135] loss: 0.15757\n",
      "[136] loss: 0.16169\n",
      "[137] loss: 0.15651\n",
      "[138] loss: 0.14129\n",
      "[139] loss: 0.14202\n",
      "[140] loss: 0.13093\n",
      "[141] loss: 0.13055\n",
      "[142] loss: 0.13268\n",
      "[143] loss: 0.13946\n",
      "[144] loss: 0.13049\n",
      "[145] loss: 0.13368\n",
      "[146] loss: 0.13317\n",
      "[147] loss: 0.14140\n",
      "[148] loss: 0.11902\n",
      "[149] loss: 0.13194\n",
      "[150] loss: 0.11232\n",
      "[151] loss: 0.13836\n",
      "[152] loss: 0.11977\n",
      "[153] loss: 0.11865\n",
      "[154] loss: 0.11742\n",
      "[155] loss: 0.11552\n",
      "[156] loss: 0.11286\n",
      "[157] loss: 0.11611\n",
      "[158] loss: 0.09927\n",
      "[159] loss: 0.09823\n",
      "[160] loss: 0.10750\n",
      "[161] loss: 0.09900\n",
      "[162] loss: 0.09881\n",
      "[163] loss: 0.09993\n",
      "[164] loss: 0.10061\n",
      "[165] loss: 0.10507\n",
      "[166] loss: 0.11649\n",
      "[167] loss: 0.10284\n",
      "[168] loss: 0.09364\n",
      "[169] loss: 0.09969\n",
      "[170] loss: 0.09233\n",
      "[171] loss: 0.09167\n",
      "[172] loss: 0.09161\n",
      "[173] loss: 0.09861\n",
      "[174] loss: 0.08625\n",
      "[175] loss: 0.09023\n",
      "[176] loss: 0.09373\n",
      "[177] loss: 0.08409\n",
      "[178] loss: 0.08126\n",
      "[179] loss: 0.07663\n",
      "[180] loss: 0.08105\n",
      "[181] loss: 0.07887\n",
      "[182] loss: 0.08325\n",
      "[183] loss: 0.07287\n",
      "[184] loss: 0.07827\n",
      "[185] loss: 0.07812\n",
      "[186] loss: 0.07327\n",
      "[187] loss: 0.08505\n",
      "[188] loss: 0.07374\n",
      "[189] loss: 0.07861\n",
      "[190] loss: 0.09481\n",
      "[191] loss: 0.07335\n",
      "[192] loss: 0.07813\n",
      "[193] loss: 0.06979\n",
      "[194] loss: 0.07545\n",
      "[195] loss: 0.07002\n",
      "[196] loss: 0.08269\n",
      "[197] loss: 0.08548\n",
      "[198] loss: 0.07862\n",
      "[199] loss: 0.07029\n",
      "[200] loss: 0.06742\n",
      "[201] loss: 0.06600\n",
      "[202] loss: 0.06406\n",
      "[203] loss: 0.06634\n",
      "[204] loss: 0.06482\n",
      "[205] loss: 0.07180\n",
      "[206] loss: 0.06182\n",
      "[207] loss: 0.06776\n",
      "[208] loss: 0.06508\n",
      "[209] loss: 0.06734\n",
      "[210] loss: 0.06261\n",
      "[211] loss: 0.06205\n",
      "[212] loss: 0.05717\n",
      "[213] loss: 0.06583\n",
      "[214] loss: 0.05857\n",
      "[215] loss: 0.05575\n",
      "[216] loss: 0.05805\n",
      "[217] loss: 0.06726\n",
      "[218] loss: 0.06164\n",
      "[219] loss: 0.05949\n",
      "[220] loss: 0.05313\n",
      "[221] loss: 0.06231\n",
      "[222] loss: 0.05573\n",
      "[223] loss: 0.05778\n",
      "[224] loss: 0.06443\n",
      "[225] loss: 0.04989\n",
      "[226] loss: 0.05489\n",
      "[227] loss: 0.05106\n",
      "[228] loss: 0.05038\n",
      "[229] loss: 0.05114\n",
      "[230] loss: 0.05219\n",
      "[231] loss: 0.05331\n",
      "[232] loss: 0.05465\n",
      "[233] loss: 0.05232\n",
      "[234] loss: 0.05551\n",
      "[235] loss: 0.04859\n",
      "[236] loss: 0.05247\n",
      "[237] loss: 0.04968\n",
      "[238] loss: 0.04781\n",
      "[239] loss: 0.05159\n",
      "[240] loss: 0.04857\n",
      "[241] loss: 0.04948\n",
      "[242] loss: 0.04591\n",
      "[243] loss: 0.04858\n",
      "[244] loss: 0.04519\n",
      "[245] loss: 0.04330\n",
      "[246] loss: 0.05075\n",
      "[247] loss: 0.04266\n",
      "[248] loss: 0.04590\n",
      "[249] loss: 0.04635\n",
      "[250] loss: 0.04661\n",
      "[251] loss: 0.04208\n",
      "[252] loss: 0.05073\n",
      "[253] loss: 0.04526\n",
      "[254] loss: 0.04406\n",
      "[255] loss: 0.04165\n",
      "[256] loss: 0.03999\n",
      "[257] loss: 0.04525\n",
      "[258] loss: 0.04393\n",
      "[259] loss: 0.04080\n",
      "[260] loss: 0.04247\n",
      "[261] loss: 0.04092\n",
      "[262] loss: 0.04610\n",
      "[263] loss: 0.04375\n",
      "[264] loss: 0.03935\n",
      "[265] loss: 0.03634\n",
      "[266] loss: 0.03776\n",
      "[267] loss: 0.03647\n",
      "[268] loss: 0.03742\n",
      "[269] loss: 0.03691\n",
      "[270] loss: 0.03733\n",
      "[271] loss: 0.04119\n",
      "[272] loss: 0.03622\n",
      "[273] loss: 0.03750\n",
      "[274] loss: 0.03883\n",
      "[275] loss: 0.04344\n",
      "[276] loss: 0.03438\n",
      "[277] loss: 0.03660\n",
      "[278] loss: 0.03539\n",
      "[279] loss: 0.03592\n",
      "[280] loss: 0.03601\n",
      "[281] loss: 0.03417\n",
      "[282] loss: 0.03827\n",
      "[283] loss: 0.03553\n",
      "[284] loss: 0.03588\n",
      "[285] loss: 0.03689\n",
      "[286] loss: 0.03533\n",
      "[287] loss: 0.03468\n",
      "[288] loss: 0.03195\n",
      "[289] loss: 0.03125\n",
      "[290] loss: 0.03400\n",
      "[291] loss: 0.03564\n",
      "[292] loss: 0.03348\n",
      "[293] loss: 0.03296\n",
      "[294] loss: 0.03074\n",
      "[295] loss: 0.03207\n",
      "[296] loss: 0.03855\n",
      "[297] loss: 0.03056\n",
      "[298] loss: 0.03259\n",
      "[299] loss: 0.03468\n",
      "[300] loss: 0.03125\n",
      "[301] loss: 0.02880\n",
      "[302] loss: 0.03199\n",
      "[303] loss: 0.03037\n",
      "[304] loss: 0.03124\n",
      "[305] loss: 0.03284\n",
      "[306] loss: 0.03147\n",
      "[307] loss: 0.03192\n",
      "[308] loss: 0.02997\n",
      "[309] loss: 0.03108\n",
      "[310] loss: 0.02922\n",
      "[311] loss: 0.02939\n",
      "[312] loss: 0.02952\n",
      "[313] loss: 0.03151\n",
      "[314] loss: 0.03627\n",
      "[315] loss: 0.02954\n",
      "[316] loss: 0.03126\n",
      "[317] loss: 0.02944\n",
      "[318] loss: 0.02742\n",
      "[319] loss: 0.03118\n",
      "[320] loss: 0.03140\n",
      "[321] loss: 0.03110\n",
      "[322] loss: 0.02776\n",
      "[323] loss: 0.02651\n",
      "[324] loss: 0.02757\n",
      "[325] loss: 0.02760\n",
      "[326] loss: 0.02775\n",
      "[327] loss: 0.02673\n",
      "[328] loss: 0.02592\n",
      "[329] loss: 0.02777\n",
      "[330] loss: 0.03063\n",
      "[331] loss: 0.02649\n",
      "[332] loss: 0.02780\n",
      "[333] loss: 0.02443\n",
      "[334] loss: 0.02866\n",
      "[335] loss: 0.02563\n",
      "[336] loss: 0.02940\n",
      "[337] loss: 0.02372\n",
      "[338] loss: 0.02509\n",
      "[339] loss: 0.02497\n",
      "[340] loss: 0.02547\n",
      "[341] loss: 0.02375\n",
      "[342] loss: 0.02556\n",
      "[343] loss: 0.02344\n",
      "[344] loss: 0.02325\n",
      "[345] loss: 0.02409\n",
      "[346] loss: 0.02474\n",
      "[347] loss: 0.02374\n",
      "[348] loss: 0.02541\n",
      "[349] loss: 0.02479\n",
      "[350] loss: 0.02417\n",
      "[351] loss: 0.02239\n",
      "[352] loss: 0.02564\n",
      "[353] loss: 0.02507\n",
      "[354] loss: 0.02317\n",
      "[355] loss: 0.02271\n",
      "[356] loss: 0.02342\n",
      "[357] loss: 0.02628\n",
      "[358] loss: 0.02279\n",
      "[359] loss: 0.02753\n",
      "[360] loss: 0.02208\n",
      "[361] loss: 0.02178\n",
      "[362] loss: 0.02252\n",
      "[363] loss: 0.02254\n",
      "[364] loss: 0.02072\n",
      "[365] loss: 0.02057\n",
      "[366] loss: 0.02304\n",
      "[367] loss: 0.02355\n",
      "[368] loss: 0.02041\n",
      "[369] loss: 0.02112\n",
      "[370] loss: 0.02330\n",
      "[371] loss: 0.02046\n",
      "[372] loss: 0.02123\n",
      "[373] loss: 0.01856\n",
      "[374] loss: 0.02108\n",
      "[375] loss: 0.02087\n",
      "[376] loss: 0.02144\n",
      "[377] loss: 0.02091\n",
      "[378] loss: 0.02130\n",
      "[379] loss: 0.01958\n",
      "[380] loss: 0.02189\n",
      "[381] loss: 0.02161\n",
      "[382] loss: 0.01896\n",
      "[383] loss: 0.02000\n",
      "[384] loss: 0.01894\n",
      "[385] loss: 0.01909\n",
      "[386] loss: 0.02035\n",
      "[387] loss: 0.01900\n",
      "[388] loss: 0.02013\n",
      "[389] loss: 0.02060\n",
      "[390] loss: 0.01790\n",
      "[391] loss: 0.01812\n",
      "[392] loss: 0.01857\n",
      "[393] loss: 0.01867\n",
      "[394] loss: 0.02109\n",
      "[395] loss: 0.01861\n",
      "[396] loss: 0.01833\n",
      "[397] loss: 0.01848\n",
      "[398] loss: 0.02113\n",
      "[399] loss: 0.01955\n",
      "[400] loss: 0.01919\n",
      "[401] loss: 0.01872\n",
      "[402] loss: 0.01823\n",
      "[403] loss: 0.01848\n",
      "[404] loss: 0.01883\n",
      "[405] loss: 0.01751\n",
      "[406] loss: 0.01856\n",
      "[407] loss: 0.01768\n",
      "[408] loss: 0.01824\n",
      "[409] loss: 0.01718\n",
      "[410] loss: 0.01804\n",
      "[411] loss: 0.01970\n",
      "[412] loss: 0.02083\n",
      "[413] loss: 0.01913\n",
      "[414] loss: 0.01675\n",
      "[415] loss: 0.01687\n",
      "[416] loss: 0.01714\n",
      "[417] loss: 0.01901\n",
      "[418] loss: 0.01669\n",
      "[419] loss: 0.01715\n",
      "[420] loss: 0.01964\n",
      "[421] loss: 0.01755\n",
      "[422] loss: 0.01601\n",
      "[423] loss: 0.01553\n",
      "[424] loss: 0.01587\n",
      "[425] loss: 0.01706\n",
      "[426] loss: 0.01582\n",
      "[427] loss: 0.01539\n",
      "[428] loss: 0.01581\n",
      "[429] loss: 0.01812\n",
      "[430] loss: 0.01626\n",
      "[431] loss: 0.01963\n",
      "[432] loss: 0.02711\n",
      "[433] loss: 0.01474\n",
      "[434] loss: 0.01644\n",
      "[435] loss: 0.01548\n",
      "[436] loss: 0.01749\n",
      "[437] loss: 0.01550\n",
      "[438] loss: 0.01622\n",
      "[439] loss: 0.01561\n",
      "[440] loss: 0.01465\n",
      "[441] loss: 0.01560\n",
      "[442] loss: 0.01793\n",
      "[443] loss: 0.01458\n",
      "[444] loss: 0.01399\n",
      "[445] loss: 0.01559\n",
      "[446] loss: 0.01448\n",
      "[447] loss: 0.01734\n",
      "[448] loss: 0.01434\n",
      "[449] loss: 0.01723\n",
      "[450] loss: 0.01560\n",
      "[451] loss: 0.01439\n",
      "[452] loss: 0.01574\n",
      "[453] loss: 0.01579\n",
      "[454] loss: 0.01583\n",
      "[455] loss: 0.01637\n",
      "[456] loss: 0.01283\n",
      "[457] loss: 0.01435\n",
      "[458] loss: 0.01335\n",
      "[459] loss: 0.01448\n",
      "[460] loss: 0.01523\n",
      "[461] loss: 0.01350\n",
      "[462] loss: 0.01499\n",
      "[463] loss: 0.01754\n",
      "[464] loss: 0.01354\n",
      "[465] loss: 0.01428\n",
      "[466] loss: 0.01380\n",
      "[467] loss: 0.01487\n",
      "[468] loss: 0.01307\n",
      "[469] loss: 0.01504\n",
      "[470] loss: 0.01414\n",
      "[471] loss: 0.01428\n",
      "[472] loss: 0.01666\n",
      "[473] loss: 0.01445\n",
      "[474] loss: 0.01272\n",
      "[475] loss: 0.01373\n",
      "[476] loss: 0.01268\n",
      "[477] loss: 0.01466\n",
      "[478] loss: 0.01468\n",
      "[479] loss: 0.01369\n",
      "[480] loss: 0.01344\n",
      "[481] loss: 0.01314\n",
      "[482] loss: 0.01403\n",
      "[483] loss: 0.01293\n",
      "[484] loss: 0.01343\n",
      "[485] loss: 0.01189\n",
      "[486] loss: 0.01220\n",
      "[487] loss: 0.01384\n",
      "[488] loss: 0.01270\n",
      "[489] loss: 0.01197\n",
      "[490] loss: 0.01214\n",
      "[491] loss: 0.01171\n",
      "[492] loss: 0.01301\n",
      "[493] loss: 0.01325\n",
      "[494] loss: 0.01302\n",
      "[495] loss: 0.01165\n",
      "[496] loss: 0.01227\n",
      "[497] loss: 0.01218\n",
      "[498] loss: 0.01145\n",
      "[499] loss: 0.01187\n",
      "[500] loss: 0.01281\n",
      "[501] loss: 0.01211\n",
      "[502] loss: 0.01265\n",
      "[503] loss: 0.01186\n",
      "[504] loss: 0.01579\n",
      "[505] loss: 0.01444\n",
      "[506] loss: 0.01617\n",
      "[507] loss: 0.01232\n",
      "[508] loss: 0.01123\n",
      "[509] loss: 0.01307\n",
      "[510] loss: 0.01154\n",
      "[511] loss: 0.01224\n",
      "[512] loss: 0.01271\n",
      "[513] loss: 0.01102\n",
      "[514] loss: 0.01121\n",
      "[515] loss: 0.01160\n",
      "[516] loss: 0.01125\n",
      "[517] loss: 0.01163\n",
      "[518] loss: 0.01078\n",
      "[519] loss: 0.01370\n",
      "[520] loss: 0.00997\n",
      "[521] loss: 0.01169\n",
      "[522] loss: 0.01111\n",
      "[523] loss: 0.01418\n",
      "[524] loss: 0.01128\n",
      "[525] loss: 0.01197\n",
      "[526] loss: 0.01070\n",
      "[527] loss: 0.01075\n",
      "[528] loss: 0.01060\n",
      "[529] loss: 0.01023\n",
      "[530] loss: 0.01066\n",
      "[531] loss: 0.01082\n",
      "[532] loss: 0.01423\n",
      "[533] loss: 0.01096\n",
      "[534] loss: 0.01167\n",
      "[535] loss: 0.01105\n",
      "[536] loss: 0.01172\n",
      "[537] loss: 0.01156\n",
      "[538] loss: 0.01069\n",
      "[539] loss: 0.00993\n",
      "[540] loss: 0.01082\n",
      "[541] loss: 0.01103\n",
      "[542] loss: 0.01151\n",
      "[543] loss: 0.01003\n",
      "[544] loss: 0.01177\n",
      "[545] loss: 0.00963\n",
      "[546] loss: 0.01098\n",
      "[547] loss: 0.01287\n",
      "[548] loss: 0.01043\n",
      "[549] loss: 0.00925\n",
      "[550] loss: 0.01166\n",
      "[551] loss: 0.00955\n",
      "[552] loss: 0.01146\n",
      "[553] loss: 0.01009\n",
      "[554] loss: 0.01110\n",
      "[555] loss: 0.00996\n",
      "[556] loss: 0.00902\n",
      "[557] loss: 0.01002\n",
      "[558] loss: 0.01003\n",
      "[559] loss: 0.01082\n",
      "[560] loss: 0.00971\n",
      "[561] loss: 0.00984\n",
      "[562] loss: 0.00879\n",
      "[563] loss: 0.01118\n",
      "[564] loss: 0.00930\n",
      "[565] loss: 0.00977\n",
      "[566] loss: 0.00895\n",
      "[567] loss: 0.01032\n",
      "[568] loss: 0.00923\n",
      "[569] loss: 0.01100\n",
      "[570] loss: 0.01038\n",
      "[571] loss: 0.00891\n",
      "[572] loss: 0.00943\n",
      "[573] loss: 0.01104\n",
      "[574] loss: 0.00957\n",
      "[575] loss: 0.01052\n",
      "[576] loss: 0.00998\n",
      "[577] loss: 0.01054\n",
      "[578] loss: 0.00966\n",
      "[579] loss: 0.00973\n",
      "[580] loss: 0.00919\n",
      "[581] loss: 0.00921\n",
      "[582] loss: 0.00974\n",
      "[583] loss: 0.00936\n",
      "[584] loss: 0.00954\n",
      "[585] loss: 0.00914\n",
      "[586] loss: 0.00845\n",
      "[587] loss: 0.01024\n",
      "[588] loss: 0.00977\n",
      "[589] loss: 0.00957\n",
      "[590] loss: 0.00906\n",
      "[591] loss: 0.00889\n",
      "[592] loss: 0.00819\n",
      "[593] loss: 0.00954\n",
      "[594] loss: 0.00809\n",
      "[595] loss: 0.00888\n",
      "[596] loss: 0.00905\n",
      "[597] loss: 0.00980\n",
      "[598] loss: 0.00847\n",
      "[599] loss: 0.00831\n",
      "[600] loss: 0.00827\n",
      "[601] loss: 0.00845\n",
      "[602] loss: 0.00963\n",
      "[603] loss: 0.00851\n",
      "[604] loss: 0.00823\n",
      "[605] loss: 0.01087\n",
      "[606] loss: 0.00856\n",
      "[607] loss: 0.00885\n",
      "[608] loss: 0.00845\n",
      "[609] loss: 0.00854\n",
      "[610] loss: 0.00825\n",
      "[611] loss: 0.00880\n",
      "[612] loss: 0.00893\n",
      "[613] loss: 0.00789\n",
      "[614] loss: 0.00766\n",
      "[615] loss: 0.00860\n",
      "[616] loss: 0.00786\n",
      "[617] loss: 0.00923\n",
      "[618] loss: 0.00786\n",
      "[619] loss: 0.00829\n",
      "[620] loss: 0.00897\n",
      "[621] loss: 0.00810\n",
      "[622] loss: 0.00928\n",
      "[623] loss: 0.00994\n",
      "[624] loss: 0.01124\n",
      "[625] loss: 0.00755\n",
      "[626] loss: 0.00794\n",
      "[627] loss: 0.00802\n",
      "[628] loss: 0.00839\n",
      "[629] loss: 0.00842\n",
      "[630] loss: 0.00784\n",
      "[631] loss: 0.00840\n",
      "[632] loss: 0.00816\n",
      "[633] loss: 0.00880\n",
      "[634] loss: 0.00753\n",
      "[635] loss: 0.00812\n",
      "[636] loss: 0.00885\n",
      "[637] loss: 0.00832\n",
      "[638] loss: 0.00713\n",
      "[639] loss: 0.00808\n",
      "[640] loss: 0.00808\n",
      "[641] loss: 0.00763\n",
      "[642] loss: 0.00767\n",
      "[643] loss: 0.00783\n",
      "[644] loss: 0.00869\n",
      "[645] loss: 0.00761\n",
      "[646] loss: 0.00720\n",
      "[647] loss: 0.00723\n",
      "[648] loss: 0.00912\n",
      "[649] loss: 0.00763\n",
      "[650] loss: 0.00808\n",
      "[651] loss: 0.00840\n",
      "[652] loss: 0.00725\n",
      "[653] loss: 0.00845\n",
      "[654] loss: 0.00717\n",
      "[655] loss: 0.00767\n",
      "[656] loss: 0.00734\n",
      "[657] loss: 0.00932\n",
      "[658] loss: 0.00688\n",
      "[659] loss: 0.00757\n",
      "[660] loss: 0.00714\n",
      "[661] loss: 0.00722\n",
      "[662] loss: 0.00753\n",
      "[663] loss: 0.00729\n",
      "[664] loss: 0.00665\n",
      "[665] loss: 0.00681\n",
      "[666] loss: 0.00720\n",
      "[667] loss: 0.00752\n",
      "[668] loss: 0.00710\n",
      "[669] loss: 0.00672\n",
      "[670] loss: 0.00719\n",
      "[671] loss: 0.00660\n",
      "[672] loss: 0.00768\n",
      "[673] loss: 0.00737\n",
      "[674] loss: 0.00668\n",
      "[675] loss: 0.00687\n",
      "[676] loss: 0.00672\n",
      "[677] loss: 0.00689\n",
      "[678] loss: 0.00749\n",
      "[679] loss: 0.00735\n",
      "[680] loss: 0.00768\n",
      "[681] loss: 0.00726\n",
      "[682] loss: 0.00640\n",
      "[683] loss: 0.00692\n",
      "[684] loss: 0.00660\n",
      "[685] loss: 0.00644\n",
      "[686] loss: 0.00665\n",
      "[687] loss: 0.00776\n",
      "[688] loss: 0.00657\n",
      "[689] loss: 0.00625\n",
      "[690] loss: 0.00804\n",
      "[691] loss: 0.00738\n",
      "[692] loss: 0.00642\n",
      "[693] loss: 0.00659\n",
      "[694] loss: 0.00648\n",
      "[695] loss: 0.00697\n",
      "[696] loss: 0.00631\n",
      "[697] loss: 0.00648\n",
      "[698] loss: 0.00724\n",
      "[699] loss: 0.00683\n",
      "[700] loss: 0.00648\n",
      "[701] loss: 0.00688\n",
      "[702] loss: 0.00637\n",
      "[703] loss: 0.00720\n",
      "[704] loss: 0.00624\n",
      "[705] loss: 0.00714\n",
      "[706] loss: 0.00678\n",
      "[707] loss: 0.00631\n",
      "[708] loss: 0.00682\n",
      "[709] loss: 0.00620\n",
      "[710] loss: 0.00633\n",
      "[711] loss: 0.00612\n",
      "[712] loss: 0.00612\n",
      "[713] loss: 0.00716\n",
      "[714] loss: 0.00597\n",
      "[715] loss: 0.00607\n",
      "[716] loss: 0.00618\n",
      "[717] loss: 0.00593\n",
      "[718] loss: 0.00610\n",
      "[719] loss: 0.00613\n",
      "[720] loss: 0.00564\n",
      "[721] loss: 0.00612\n",
      "[722] loss: 0.00746\n",
      "[723] loss: 0.00623\n",
      "[724] loss: 0.00585\n",
      "[725] loss: 0.00610\n",
      "[726] loss: 0.00653\n",
      "[727] loss: 0.00651\n",
      "[728] loss: 0.00611\n",
      "[729] loss: 0.00629\n",
      "[730] loss: 0.00638\n",
      "[731] loss: 0.00613\n",
      "[732] loss: 0.00631\n",
      "[733] loss: 0.00622\n",
      "[734] loss: 0.00620\n",
      "[735] loss: 0.00617\n",
      "[736] loss: 0.00603\n",
      "[737] loss: 0.00548\n",
      "[738] loss: 0.00645\n",
      "[739] loss: 0.00627\n",
      "[740] loss: 0.00580\n",
      "[741] loss: 0.00592\n",
      "[742] loss: 0.00581\n",
      "[743] loss: 0.00673\n",
      "[744] loss: 0.00648\n",
      "[745] loss: 0.00616\n",
      "[746] loss: 0.00594\n",
      "[747] loss: 0.00531\n",
      "[748] loss: 0.00587\n",
      "[749] loss: 0.00591\n",
      "[750] loss: 0.00574\n",
      "[751] loss: 0.00576\n",
      "[752] loss: 0.00609\n",
      "[753] loss: 0.00550\n",
      "[754] loss: 0.00651\n",
      "[755] loss: 0.00639\n",
      "[756] loss: 0.00570\n",
      "[757] loss: 0.00667\n",
      "[758] loss: 0.00507\n",
      "[759] loss: 0.00532\n",
      "[760] loss: 0.00622\n",
      "[761] loss: 0.00684\n",
      "[762] loss: 0.00588\n",
      "[763] loss: 0.00579\n",
      "[764] loss: 0.00607\n",
      "[765] loss: 0.00615\n",
      "[766] loss: 0.00526\n",
      "[767] loss: 0.00699\n",
      "[768] loss: 0.00614\n",
      "[769] loss: 0.00542\n",
      "[770] loss: 0.00561\n",
      "[771] loss: 0.00546\n",
      "[772] loss: 0.00528\n",
      "[773] loss: 0.00648\n",
      "[774] loss: 0.00546\n",
      "[775] loss: 0.00622\n",
      "[776] loss: 0.00609\n",
      "[777] loss: 0.00501\n",
      "[778] loss: 0.00579\n",
      "[779] loss: 0.00539\n",
      "[780] loss: 0.00524\n",
      "[781] loss: 0.00558\n",
      "[782] loss: 0.00550\n",
      "[783] loss: 0.00578\n",
      "[784] loss: 0.00508\n",
      "[785] loss: 0.00549\n",
      "[786] loss: 0.00513\n",
      "[787] loss: 0.00487\n",
      "[788] loss: 0.00532\n",
      "[789] loss: 0.00533\n",
      "[790] loss: 0.00496\n",
      "[791] loss: 0.00476\n",
      "[792] loss: 0.00497\n",
      "[793] loss: 0.00530\n",
      "[794] loss: 0.00470\n",
      "[795] loss: 0.00540\n",
      "[796] loss: 0.00494\n",
      "[797] loss: 0.00639\n",
      "[798] loss: 0.00496\n",
      "[799] loss: 0.00550\n",
      "[800] loss: 0.00544\n",
      "[801] loss: 0.00487\n",
      "[802] loss: 0.00509\n",
      "[803] loss: 0.00557\n",
      "[804] loss: 0.00500\n",
      "[805] loss: 0.00531\n",
      "[806] loss: 0.00528\n",
      "[807] loss: 0.00514\n",
      "[808] loss: 0.00514\n",
      "[809] loss: 0.00454\n",
      "[810] loss: 0.00469\n",
      "[811] loss: 0.00498\n",
      "[812] loss: 0.00514\n",
      "[813] loss: 0.00560\n",
      "[814] loss: 0.00518\n",
      "[815] loss: 0.00491\n",
      "[816] loss: 0.00543\n",
      "[817] loss: 0.00482\n",
      "[818] loss: 0.00500\n",
      "[819] loss: 0.00511\n",
      "[820] loss: 0.00506\n",
      "[821] loss: 0.00504\n",
      "[822] loss: 0.00531\n",
      "[823] loss: 0.00507\n",
      "[824] loss: 0.00441\n",
      "[825] loss: 0.00486\n",
      "[826] loss: 0.00553\n",
      "[827] loss: 0.00466\n",
      "[828] loss: 0.00531\n",
      "[829] loss: 0.00453\n",
      "[830] loss: 0.00520\n",
      "[831] loss: 0.00489\n",
      "[832] loss: 0.00620\n",
      "[833] loss: 0.00480\n",
      "[834] loss: 0.00558\n",
      "[835] loss: 0.00475\n",
      "[836] loss: 0.00460\n",
      "[837] loss: 0.00480\n",
      "[838] loss: 0.00466\n",
      "[839] loss: 0.00543\n",
      "[840] loss: 0.00444\n",
      "[841] loss: 0.00552\n",
      "[842] loss: 0.00484\n",
      "[843] loss: 0.00538\n",
      "[844] loss: 0.00483\n",
      "[845] loss: 0.00500\n",
      "[846] loss: 0.00409\n",
      "[847] loss: 0.00448\n",
      "[848] loss: 0.00490\n",
      "[849] loss: 0.00453\n",
      "[850] loss: 0.00452\n",
      "[851] loss: 0.00458\n",
      "[852] loss: 0.00470\n",
      "[853] loss: 0.00460\n",
      "[854] loss: 0.00464\n",
      "[855] loss: 0.00460\n",
      "[856] loss: 0.00446\n",
      "[857] loss: 0.00464\n",
      "[858] loss: 0.00444\n",
      "[859] loss: 0.00441\n",
      "[860] loss: 0.00426\n",
      "[861] loss: 0.00423\n",
      "[862] loss: 0.00464\n",
      "[863] loss: 0.00448\n",
      "[864] loss: 0.00464\n",
      "[865] loss: 0.00495\n",
      "[866] loss: 0.00439\n",
      "[867] loss: 0.00419\n",
      "[868] loss: 0.00402\n",
      "[869] loss: 0.00419\n",
      "[870] loss: 0.00458\n",
      "[871] loss: 0.00452\n",
      "[872] loss: 0.00421\n",
      "[873] loss: 0.00429\n",
      "[874] loss: 0.00445\n",
      "[875] loss: 0.00529\n",
      "[876] loss: 0.00389\n",
      "[877] loss: 0.00445\n",
      "[878] loss: 0.00391\n",
      "[879] loss: 0.00412\n",
      "[880] loss: 0.00448\n",
      "[881] loss: 0.00411\n",
      "[882] loss: 0.00421\n",
      "[883] loss: 0.00413\n",
      "[884] loss: 0.00468\n",
      "[885] loss: 0.00425\n",
      "[886] loss: 0.00429\n",
      "[887] loss: 0.00413\n",
      "[888] loss: 0.00460\n",
      "[889] loss: 0.00562\n",
      "[890] loss: 0.00413\n",
      "[891] loss: 0.00370\n",
      "[892] loss: 0.00436\n",
      "[893] loss: 0.00457\n",
      "[894] loss: 0.00495\n",
      "[895] loss: 0.00480\n",
      "[896] loss: 0.00436\n",
      "[897] loss: 0.00406\n",
      "[898] loss: 0.00389\n",
      "[899] loss: 0.00372\n",
      "[900] loss: 0.00429\n",
      "[901] loss: 0.00429\n",
      "[902] loss: 0.00398\n",
      "[903] loss: 0.00404\n",
      "[904] loss: 0.00442\n",
      "[905] loss: 0.00382\n",
      "[906] loss: 0.00430\n",
      "[907] loss: 0.00434\n",
      "[908] loss: 0.00390\n",
      "[909] loss: 0.00370\n",
      "[910] loss: 0.00399\n",
      "[911] loss: 0.00373\n",
      "[912] loss: 0.00411\n",
      "[913] loss: 0.00396\n",
      "[914] loss: 0.00357\n",
      "[915] loss: 0.00418\n",
      "[916] loss: 0.00375\n",
      "[917] loss: 0.00452\n",
      "[918] loss: 0.00434\n",
      "[919] loss: 0.00469\n",
      "[920] loss: 0.00392\n",
      "[921] loss: 0.00408\n",
      "[922] loss: 0.00410\n",
      "[923] loss: 0.00378\n",
      "[924] loss: 0.00408\n",
      "[925] loss: 0.00404\n",
      "[926] loss: 0.00366\n",
      "[927] loss: 0.00423\n",
      "[928] loss: 0.00436\n",
      "[929] loss: 0.00409\n",
      "[930] loss: 0.00398\n",
      "[931] loss: 0.00397\n",
      "[932] loss: 0.00378\n",
      "[933] loss: 0.00396\n",
      "[934] loss: 0.00443\n",
      "[935] loss: 0.00414\n",
      "[936] loss: 0.00353\n",
      "[937] loss: 0.00383\n",
      "[938] loss: 0.00539\n",
      "[939] loss: 0.00420\n",
      "[940] loss: 0.00404\n",
      "[941] loss: 0.00348\n",
      "[942] loss: 0.00414\n",
      "[943] loss: 0.00407\n",
      "[944] loss: 0.00426\n",
      "[945] loss: 0.00398\n",
      "[946] loss: 0.00362\n",
      "[947] loss: 0.00380\n",
      "[948] loss: 0.00365\n",
      "[949] loss: 0.00391\n",
      "[950] loss: 0.00361\n",
      "[951] loss: 0.00383\n",
      "[952] loss: 0.00416\n",
      "[953] loss: 0.00352\n",
      "[954] loss: 0.00329\n",
      "[955] loss: 0.00382\n",
      "[956] loss: 0.00349\n",
      "[957] loss: 0.00376\n",
      "[958] loss: 0.00331\n",
      "[959] loss: 0.00368\n",
      "[960] loss: 0.00399\n",
      "[961] loss: 0.00351\n",
      "[962] loss: 0.00414\n",
      "[963] loss: 0.00352\n",
      "[964] loss: 0.00457\n",
      "[965] loss: 0.00350\n",
      "[966] loss: 0.00346\n",
      "[967] loss: 0.00416\n",
      "[968] loss: 0.00399\n",
      "[969] loss: 0.00353\n",
      "[970] loss: 0.00348\n",
      "[971] loss: 0.00348\n",
      "[972] loss: 0.00334\n",
      "[973] loss: 0.00418\n",
      "[974] loss: 0.00366\n",
      "[975] loss: 0.00382\n",
      "[976] loss: 0.00378\n",
      "[977] loss: 0.00329\n",
      "[978] loss: 0.00354\n",
      "[979] loss: 0.00332\n",
      "[980] loss: 0.00319\n",
      "[981] loss: 0.00361\n",
      "[982] loss: 0.00327\n",
      "[983] loss: 0.00337\n",
      "[984] loss: 0.00345\n",
      "[985] loss: 0.00336\n",
      "[986] loss: 0.00346\n",
      "[987] loss: 0.00341\n",
      "[988] loss: 0.00348\n",
      "[989] loss: 0.00333\n",
      "[990] loss: 0.00309\n",
      "[991] loss: 0.00319\n",
      "[992] loss: 0.00329\n",
      "[993] loss: 0.00338\n",
      "[994] loss: 0.00387\n",
      "[995] loss: 0.00359\n",
      "[996] loss: 0.00336\n",
      "[997] loss: 0.00393\n",
      "[998] loss: 0.00356\n",
      "[999] loss: 0.00322\n"
     ]
    }
   ],
   "source": [
    "# simplest train loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    y = net(tokens)\n",
    "    loss = criterion(y.view(-1, y.size(-1)), tokens.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"[{i}] loss: {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'helloooooo'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompt = \"hello\"\n",
    "tokens = tokenizer.encode(test_prompt)\n",
    "tokens = torch.tensor(tokens).unsqueeze(1).to(DEVICE)\n",
    "print(tokens.shape)\n",
    "\n",
    "net.eval()\n",
    "generated_tokens = net.generate(tokens, max_new_tokens=5, temperature=1.0, top_k=10)\n",
    "\n",
    "tokenizer.decode(generated_tokens.squeeze(1).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis-P7I560r2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
