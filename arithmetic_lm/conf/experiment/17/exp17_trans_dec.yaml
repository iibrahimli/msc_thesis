# @package _global_

defaults:
  - override /tokenizer: char
  - override /sampling: default
  - override /data: strlen_v1_1M
  - override /training: default
  - override /wandb: default
  - override /model: transformer_dec
  - _self_

data:
  train_ds_class: ArithmeticLMSequenceDataset
  format:
    pad: $
    reverse_ans: false
    filler_tokens_prompt: 0
    filler_tokens_ans: 0
    chain_of_thought: false

training:
  batch_size: 1024
  lr: 0.0001
  max_iters: 100000
  limit_test_examples: 1000 # since test set is large
  val_interval: 2000
  devices: [6]
  eval_func: "string_match_exact"

model:
  args:
    # n_layers: 6
    # n_embd: 768
    # n_head: 4
    n_layers: 1
    n_embd: 192
    n_head: 2

wandb:
  project: "strlen_v1"
  run_name: trans_dec_${model.args.n_layers}layers_${model.args.n_embd}embd_${model.args.n_head}head