batch_size: 256
lr: 0.001
weight_decay: 0.1
warmup_iters: 100
max_iters: 10000
num_dl_workers: 0
val_ratio: 0.1
val_interval: 500
limit_val_batches: null
limit_test_examples: null
reload_dataloaders_every_n_epochs: 0
only_answer_loss: false
devices: null

# ckpt to resume training run
resume_ckpt_path: null
# only load weights from ckpt, not training state (step, optim, etc.)
ckpt_weights_only: false

eval_func: "numeric"

# added by datasets (search for 'filler')
# suitable for char tokenizer (TODO: change if different tokenizer)
pause_token: "."