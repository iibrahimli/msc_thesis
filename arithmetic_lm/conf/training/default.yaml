batch_size: 256
lr: 0.001
weight_decay: 0.1
warmup_iters: 100
max_iters: 10000
num_dl_workers: 0
val_ratio: 0.1
val_interval: 500
limit_val_batches: null
limit_test_examples: null
devices: null

# ckpt to resume training run
resume_ckpt_path: null
# only load weights from ckpt, not training state (step, optim, etc.)
ckpt_weights_only: false

eval_func: "numeric"

# inject pause tokens (per original sequence length) like Goyal 2024 (0 to disable)
n_pause_tokens: 0
# suitable for char tokenizer (TODO: change if different tokenizer)
pause_token: p
# randomly insert ("pretrain"), otherwise only append pause tokens to the prefix ("finetune")
pause_type: pretrain