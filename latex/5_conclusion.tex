\chapter{Conclusion}\label{conclusion}

This thesis investigated the challenges of length generalization in integer addition tasks using small transformer models trained from scratch. The focus was on understanding why models with standard absolute positional encodings fail to generalize to sequences longer than those seen during training and exploring methods to improve generalization without altering the model architecture or introducing task-specific modifications.

It was confirmed that transformer models with absolute positional encodings struggle with digit alignment for longer sequences. The inability to effectively align digits beyond training lengths arises from the rigid nature of absolute positional encodings, which do not extrapolate to unseen sequence lengths. Experiments demonstrated that models with digit alignment-focused positional encodings, such as the Abacus encoding, significantly improve length generalization. This finding suggests that proper digit alignment is crucial for generalizing integer addition tasks.

Exploring data formatting techniques revealed that adding random spaces to input sequences marginally improves length generalization. The random spaces disrupt fixed positional patterns, encouraging the model to learn more robust representations less dependent on absolute positions. While this approach did not fully solve the generalization problem, it allowed models to interpolate within the training distribution and extrapolate to sequences slightly longer than those seen during training.

The inclusion of sub-task data—such as carry detection, digit-wise modular addition, reversing, and digit alignment—was investigated to enhance the model's compositionality. A comprehensive analysis across various model dimensions and dataset sizes revealed that incorporating sub-task data marginally improves length generalization, particularly for smaller models and smaller dataset sizes. Smaller models benefited more from mixed-task training, suggesting that limited capacity compels them to learn more generalizable algorithms with the aid of sub-tasks.

Mechanistic interpretability techniques were applied to understand the internal representations and failure modes of the models. Attention map analyses revealed that models with digit alignment-focused positional encodings exhibit crisp attention patterns that align with the correct addition algorithm, even for longer sequences. In contrast, models with absolute positional encodings showed diffuse and unclear attention patterns, indicating difficulties in digit alignment and carry propagation.

Overall, the findings isolate position-based indexing and digit alignment as the root cause of failure for length generalization. While methods like the Abacus encoding effectively address this issue, they involve task-specific modifications. The exploration of data formatting techniques and sub-task learning offers insights into potential approaches for improving generalization without altering the model architecture.

\section{Limitations}

This work focuses on small, specialized models trained from scratch, limiting the connection to large language models (LLMs) and pre-trained transformers. Consequently, it remains unclear how the findings translate to LLMs and what practical suggestions can be made to improve their performance on similar tasks. Additionally, other popular positional encodings beyond those mentioned were not explored, which could offer alternative solutions to the digit alignment problem.

The sub-task experiments, while hinting at interesting phenomena, are confounded by the number of training steps. Due to computational constraints, all models were trained for the same number of steps and may be under-trained. The results might change with extended training, potentially revealing different dynamics in sub-task learning and generalization.

Mechanistic interpretability methods to causally trace the origins of failure at the circuit level were not attempted. While it is not much explored in the literature how techniques like activation patching may be applied in algorithmic tasks, exploring other methods could provide understanding of the models' internal mechanisms.

\section{Future Work}

Future research can focus on developing and applying mechanistic interpretability methods specifically tailored for algorithmic tasks. Understanding the failure modes of transformers at a circuit level could inform the design of models that generalize better to longer sequences.

Exploring the application of these insights to data curation for fine-tuning pre-trained LLMs is another promising direction. Investigating how sub-task training or other architectural adjustments affect LLM performance on algorithmic tasks could bridge the gap between small specialized models and large pre-trained models.

Further studies on sub-task learning could involve experimenting with larger data scales and varying the amount of addition data while adding sub-tasks on top, rather than keeping the total dataset size constant. This approach could help determine if models can overfit or if sub-tasks assist in learning generalizable algorithms when models reach capacity.

Additionally, connections to active learning and curriculum learning could be explored. Implementing strategies where the model selects samples to learn from based on uncertainty or diversity might enhance learning efficiency. Incorporating curriculum learning with varying sub-task difficulty or digit lengths could improve generalization while avoiding catastrophic forgetting~\parencite{parisi_continual_2019}.
