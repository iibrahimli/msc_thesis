\chapter{Conclusion}\label{conclusion}

\TODO{conclude}

\section{Contributions}


\section{Limitations}
The work focuses on small, specialist models trained from scratch, and connection to LLMs and pre-trained models is not explored, but very interesting and relevant (not clear how findings translate to LLMs and what practical suggestions can be made to improve their performance). Also exploring other popular positional encodings is interesting and also missing.

The sub-task experiment hints at interesting stuff, but has the confounder of number of training steps (all models are trained for the same number of steps and are under-trained due to compute constraints). Thus, maybe the results change when training is done for much more steps.

Mechanistic interpretability methods to causally trace the origins of failure at circuit-level are not attempted, though from literature it is unclear whether the usual activation patching etc. can be useful in algotihmic tasks, and what other methods can be used.


\section{Future Work}
More mechanistic interpretability methods for algorithmic tasks can be conceived and explored, to understand the failure modes of transformers in these tasks.

Applying insights to data curation for finetuning on sub-tasks or other architectural aspects of pre-trained LLMs is interesting, and can be explored.

To investigate sub-task learning more might try more data scales, and try keeping all addition data and adding subtasks on top instead of decreasing the amount of addition data to keep the total constant, see if models can still overfit to that. Furthermore, can try to figure out the ``interpolation threshold'' (max number of samples it can overfit to) for a given model size, and then start with that amount of addition data, then see if subtasks help push that threshold further or not: idea is that if the model is already at capacity (can barely memorize more samples), the only way to keep loss on addition and improve on subtasks is to learn a generalizing algorithm and then collapse the circuits -> subtasks help. If the model is not at capacity, it can just memorize more samples and not learn the generalizing algorithm, so subtasks don't help.

Also, discuss connection to active learning, where the model can choose which samples to learn from, and how that can be applied here to select subtasks or samples. Different methods can be used to choose that, e.g. uncertainty, diversity, etc. Continual learning is also interesting here, can benefit from the many methods in literature to apply curriculum learning with varying subtask difficulty and digit length, avoid catastrophic forgetting, etc. \TODO{cite parisi continual 2019 Continual lifelong learning with neural networks A review}