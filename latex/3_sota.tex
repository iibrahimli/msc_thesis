\chapter{State of the Art}\label{sota}

In this chapter, a literature review is presented, focusing on the topics of transformers, learning arithmetic tasks, reasoning capabilities, compositional learning, and generalization to longer sequence lengths.

\section{Learning Arithmetics with Transformers}\label{sota_arithmetic_tasks}

The ability of transformer models \parencite{vaswani_attention_2017} to learn arithmetic operations such as integer addition has been a subject of significant research interest. Evaluations demonstrated that large pre-trained language models such as GPT-3 \parencite{brown_language_2020} can exhibit emergent capabilities across general-purpose tasks, including basic few-digit arithmetic, despite these tasks not being explicitly encoded by the unsupervised next-token prediction objective. However, even largest state-of-the-art models like GPT-4 \parencite{achiam_gpt-4_2023} struggle to robustly solve multi-digit addition and multiplication, especially when a larger number of digits are involved.

\cite{lee_teaching_2023} investigate how even small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition and multiplication. They show that training on chain-of-thought style data that includes intermediate step results significantly improves accuracy, sample complexity, and convergence speed, even in the absence of pretraining. This approach aligns with the experiments presented in this thesis on chain-of-thought training, where models are trained to output the steps involved in solving addition problems. One limitation of their work is that it limits each task to a fixed number of digits (e.g. 7 and 7 digit operands), using padding to ensure uniform input length. In contrast, this thesis extends the task to variable-length addition problems which is more difficult due to the need for models to learn to align digits as discussed in \ref{research_questions}.

Understanding how transformers learn arithmetic tasks is further explored by \cite{quirke_understanding_2023}, who present an in-depth mechanistic analysis of a one-layer transformer model trained for integer addition. They reveal that the model processes the problem in a non-intuitive way: it divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions, merging the results in the MLP layer. This work also restricts the operands to a fixed length of 5 digits and employs padding, apart from restricting the model to a single layer.

Length generalization is a critical challenge in training transformers for arithmetic tasks that comes up in numerous research works. \cite{jelassi_length_2023} examine how transformers cope with learning basic integer arithmetic and generalizing to longer sequences than seen during training. They find that relative position embeddings enable length generalization for simple tasks such as addition, allowing models trained on 5-digit numbers to perform 15-digit sums. However, this method fails for multiplication, leading them to propose "train set priming" by adding a few (10 to 50) longer sequences to the training set. Despite showing interesting capabilites of learning from few examples, this defeats the purpose of \emph{zero-shot} generalization to unseen lengths.

Similarly, \cite{duan_interpolation_2023} investigate the capabilities of transformers in learning arithmetic algorithms and introduce Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn proper attention biases linked to relative position encoding mechanisms. Using ABC, they achieve robust length generalization on certain arithmetic tasks. Despite promising results, this work is limited due to the attention bias intervention being task-specific and the need for a modified training with 2 stages (first training to perfect interpolation accuracy to learn the attention biases, then training another model with extracted attention biases). Conversely, the main research interest  in this domain lies in architectural modifications that apart from boosting algorithmic capabilities, also preserve or improve performance on other language tasks.

The limitations of transformers in performing counting tasks are highlighted by \cite{yehudai_when_2024}, who focus on simple counting tasks involving counting the number of times a token appears in a string. They show that transformers can solve this task when the dimension of the transformer state is linear in the context length, but this does not scale beyond this limit, providing theoretical arguments for the observed limitations.

Recent work by \cite{mcleish_transformers_2024} addresses the poor performance of transformers on arithmetic tasks by adding an embedding to each digit that encodes its position relative to the start of the number. This modification, along with architectural changes like input injection and recurrent layers, significantly improves performance, achieving up to 99\% accuracy on 100-digit addition problems.

Investigations into the symbolic capabilities of large language models by \cite{dave_investigating_2024} and the arithmetic properties in the space of language model prompts by \cite{krubinski_basic_2023} further contribute to understanding how transformers process arithmetic operations and the challenges involved in symbolic reasoning tasks.

In summary, the literature demonstrates that transformers can learn arithmetic tasks like integer addition, but challenges remain in achieving robust length generalization and understanding the underlying mechanisms by which these models perform arithmetic operations. These findings are directly relevant to our focus on why models fail and how they attempt to solve addition tasks, including mistake analysis and identifying failures in the addition algorithm.

\section{Reasoning in Transformers}\label{sota_reasoning_in_transformers}

Transformers have shown remarkable abilities in reasoning tasks, particularly when employing techniques such as chain-of-thought (CoT) prompting. \cite{li_chain_2024} provide a theoretical understanding of the power of CoT for decoder-only transformers, demonstrating that CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. This theoretical perspective supports our experiments involving chain-of-thought training to enhance the reasoning capabilities of models on arithmetic tasks.

\cite{wang_cot_2024} explore the idea that chain-of-thought reasoning paths can be elicited from pre-trained language models by simply altering the decoding process, rather than relying on specific prompting techniques. They find that CoT paths are frequently inherent in the models' alternative sequences and that the presence of a CoT in the decoding path correlates with higher confidence in the model's decoded answer.

Further extending the concept of self-generated reasoning, \cite{zelikman_quiet-star_2024} introduce Quiet-STaR, where language models learn to generate rationales at each token to explain future text, thereby improving their predictions. This approach generalizes previous work on self-taught reasoning by \cite{zelikman_star_2022}, highlighting the potential for models to internally reason without explicit prompting.

\cite{goyal_think_2024} propose training language models with pause tokens, allowing the model to process extra computation before outputting the next token. This method improves performance on reasoning tasks, including arithmetic, by enabling the model to ``think before speaking,'' which is conceptually similar to chain-of-thought processes.

Understanding how transformers learn causal structures is investigated by \cite{nichani_how_2024}, who introduce an in-context learning task requiring the learning of latent causal structures. They prove that gradient descent on a simplified two-layer transformer learns to solve this task by encoding the latent causal graph in the first attention layer, providing insights into the mechanisms by which transformers develop reasoning capabilities.

Overall, these studies illustrate the importance of internal reasoning processes in transformers and provide various methods to enhance reasoning abilities, which are pertinent to our work on training models to output steps and analyzing how they attempt to solve addition tasks.

\section{Compositional Learning}\label{sota_compositional_learning}

Compositionality refers to the ability of models to understand and generate new combinations of known components. \cite{dziri_faith_2023} investigate the limits of transformers on compositional tasks, such as multi-digit multiplication and logic grid puzzles. They find that transformers tend to solve compositional tasks by reducing multi-step reasoning into linearized subgraph matching rather than developing systematic problem-solving skills, suggesting limitations in compositional generalization.

\cite{press_measuring_2023} measure the compositionality gap in language models by evaluating how often models can correctly answer all sub-problems but fail to generate the overall solution. They find that as model size increases, single-hop question answering performance improves faster than multi-hop performance, indicating that larger models do not necessarily improve in their ability to perform compositional reasoning. They propose methods like chain-of-thought and self-ask prompting to narrow the compositionality gap.

The work of \cite{hupkes_compositionality_2020} provides a theoretical framework for understanding compositional generalization in neural networks. They propose tests to investigate whether models systematically recombine known parts and rules, generalize to longer sequences, and favor rules over exceptions during training. This framework is relevant to our analysis of how transformers learn and generalize in arithmetic tasks.

\cite{zhou_what_2023} propose the RASP-Generalization Conjecture, suggesting that transformers tend to length generalize on a task if it can be solved by a short RASP program that works for all input lengths. They use this framework to understand when and how transformers exhibit strong length generalization on algorithmic tasks, which is closely related to compositional learning.

In summary, while transformers have demonstrated some ability to perform compositional tasks, significant challenges remain in achieving systematic compositional generalization. These findings inform our focus on multi-task learning and understanding how models can be trained to perform compositional operations like digit alignment and modular sum.

\section{Generalization to Longer Sequence Lengths}\label{sota_generalization_to_longer_sequences}

Generalization to longer sequence lengths is a critical challenge in training transformers for tasks like integer addition. Positional encoding plays a significant role in length generalization. \cite{kazemnejad_impact_2023} conduct a systematic empirical study comparing different positional encoding approaches, including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, Rotary, and Transformers without positional encoding (NoPE). They find that explicit position embeddings are not essential for decoder-only transformers to generalize well to longer sequences and that models without positional encoding outperform others in length generalization.

\cite{ruoss_randomized_2023} introduce randomized positional encodings to boost length generalization of transformers. They demonstrate that their method allows transformers to generalize to sequences of unseen length by simulating the positions of longer sequences and randomly selecting an ordered subset to fit the sequence's length.

\cite{li_functional_2024} propose a novel functional relative position encoding with progressive interpolation (FIRE) to improve transformer generalization to longer contexts. They theoretically show that FIRE can represent popular relative position encodings and empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.

The success of length generalization is also linked to data format and position encoding type. \cite{zhou_transformers_2024} test the transformer's ability to generalize using the task of addition of two integers. They show that transformers can achieve length generalization but not robustly, as performance is significantly influenced by factors like random weight initialization and training data order.

\cite{mcleish_transformers_2024} address length generalization by adding an embedding to each digit that encodes its position relative to the start of the number. This fix, along with architectural modifications, allows transformers to generalize to larger and more complex arithmetic problems, achieving state-of-the-art performance on addition tasks with longer sequences.

In our own experiments, we explore different positional encodings and model sizes to improve length generalization in integer addition tasks. The literature suggests that carefully designed positional encodings and data formatting can significantly impact the ability of transformers to generalize to longer sequences.