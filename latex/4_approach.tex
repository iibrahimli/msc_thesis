\chapter{Approach}\label{approach}

\section{Overview}
\label{sec:overview}
Provide a concise overview of the experimental approach taken to address the research questions and test the hypotheses. Mention how the experiments are designed to explore the limitations of positional encodings, the impact of data formatting, and the role of sub-task learning in improving length generalization.

\section{Data Formatting Strategies}
\label{sec:data_formatting}
Discuss the various data formatting techniques employed to facilitate digit alignment and examine their effects on model generalization.

\subsection{Standard Format}
Describe the default representation of input sequences, such as how operands and the addition operator are arranged (e.g., "123+456").

\subsection{Reversing Operands}
Explain the method of reversing the digits of operands (e.g., "321+654") to align least significant digits and its intended impact on learning and generalization.

\subsection{Adding Random Spaces}
Describe how random spaces are introduced between digits and symbols in the input sequence. Discuss how this aims to disrupt fixed positional patterns and encourage the model to learn position-invariant representations.

\subsection{Zero Padding Operands}
Explain zero-padding operands to a fixed length (e.g., "00123+00456") to align digits. Discuss the benefits and limitations of this approach, including the necessity of knowing the maximum sequence length in advance.

\subsection{Scratchpad Technique}
Introduce the use of a scratchpad (chain-of-thought) where the model generates intermediate computation steps before the final answer. Explain how this promotes step-by-step reasoning and its potential impact on length generalization.

\subsection{Inclusion of Sub-Task Data}
Discuss the incorporation of sub-task data such as carry detection, digit-wise modular addition, reversing, and digit alignment. Explain how these sub-tasks are formatted and integrated into the training data to enhance compositional learning.

\section{Experimental Setup}
\label{sec:experimental_setup}
Provide details on how the experiments are structured to test the hypotheses, including references to the positional encodings and model architectures described in the Background chapter.

\subsection{Model Configurations}
Briefly mention that transformer models are used as described in the Background chapter. Highlight any specific configurations pertinent to the experiments, such as variations in model dimensions or the use of looped transformer decoders.

\subsection{Training Data Generation}
Describe how the training and test datasets are generated, including the range of digit lengths and operand values. Specify how out-of-distribution (OOD) test sets are constructed to assess length generalization.

\subsection{Training Parameters}
Outline the training procedures, including optimization parameters, learning rate schedules, batch sizes, number of epochs, and any regularization techniques used.

\subsection{Evaluation Metrics}
Define the metrics used to evaluate model performance, such as accuracy on in-distribution (ID) and OOD test sets, cross-entropy loss, and any task-specific metrics like carry prediction accuracy.

\section{Investigating the Limitations of Absolute Positional Encodings}
\label{sec:absolute_positional_limitations}
Assess the limitations of absolute positional encodings in enabling length generalization for integer addition tasks.

\subsection{Baseline Length Generalization Assessment}
Establish the baseline performance of models with absolute positional encodings on length generalization.

Describe how models are trained on addition tasks with operands of certain digit lengths and tested on both seen and longer unseen digit lengths. Highlight the expectation that models will struggle with sequences longer than those seen during training, supporting the hypothesis that absolute positional encodings limit generalization.

\subsection{Extending Observations to Longer Sequences}
Expand the assessment to include training on a broader range of digit lengths and testing on even longer sequences.

Explain how this reinforces the initial findings by demonstrating that even extensive training on a wide range of lengths does not enable generalization to unseen longer sequences.

\subsection{Broad Range Length Assessment}
Analyze the model's ability to generalize within and beyond the training length range when trained on a wide span of digit lengths excluding specific lengths.

Discuss how this experiment emphasizes the limitations of absolute positional encodings and supports the need for alternative approaches.

\section{Exploring Data Formatting Techniques for Improved Generalization}
\label{sec:data_formatting_improvement}
Investigate how different data formatting strategies impact the model's ability to generalize to longer sequences.

\subsection{Impact of Adding Random Spaces}
Examine the effect of introducing random spaces into input sequences on length generalization.

Discuss the hypothesis that adding randomness in spacing disrupts positional patterns that the model might overfit to, thereby encouraging it to learn more robust representations.

\subsection{Effect of Zero Padding Operands}
Assess how zero-padding operands to a fixed length influences generalization performance.

Highlight that while zero padding aligns digits and may improve performance on sequences up to the maximum padded length, it may not facilitate generalization to longer sequences without prior knowledge of maximum lengths.

\section{Enhancing Compositionality through Sub-Task Learning}
\label{sec:subtask_learning}
Explore the role of incorporating sub-task data in improving the model's compositionality and length generalization capabilities.

\subsection{Incorporating Sub-Task Data}
Describe the methodology for integrating sub-task data into the training process.

Explain how tasks such as carry detection, digit-wise modular addition, reversing, and digit alignment are formatted and prefixed in the training data to encourage the model to learn underlying algorithmic components.

\subsection{Analysis Across Model Scales}
Analyze the impact of sub-task learning across various model dimensions and dataset sizes.

Discuss expectations that models trained with sub-task data will exhibit improved length generalization and compositional understanding, addressing the hypothesis that sub-task learning enhances model performance on longer sequences.

\section{Mechanistic Interpretability Techniques}
\label{sec:mechanistic_interpretability}
Outline the application of mechanistic interpretability methods to understand the internal representations and failure modes of the models.

\subsection{Interpretability Methods}
Describe the specific techniques to be used, such as attention pattern analysis, probing individual neurons or layers, and circuit tracing.

\subsection{Application to Model Analysis}
Explain how these methods are applied to models from key experiments, particularly those with and without sub-task data.

Discuss how analyzing models with contrasting performances can reveal insights into the mechanisms underlying length generalization.

\subsection{Anticipated Insights}
Outline the expected findings from the interpretability analysis.

Suggest that this may reveal how different positional encoding schemes and data formatting strategies affect the model's ability to align digits and propagate carries, thereby influencing generalization performance.

Conclude the chapter by summarizing how the experimental approach is designed to systematically investigate the research questions and hypotheses through targeted experiments and analyses.

