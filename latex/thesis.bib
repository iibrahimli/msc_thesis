@inproceedings{achiam_gpt-4_2023,
  title    = {{GPT}-4 Technical Report},
  url      = {https://www.semanticscholar.org/paper/GPT-4-Technical-Report-Achiam-Adler/163b4d6a79a5b19af88b8585456363340d9efd04},
  abstract = {We report the development of {GPT}-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, {GPT}-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. {GPT}-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of {GPT}-4's performance based on models trained with no more than 1/1,000th the compute of {GPT}-4.},
  author   = {Achiam, {OpenAI} Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, S. and Balcom, Valerie and Baltescu, Paul and Bao, Haim-ing and Bavarian, Mo and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, B. and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Sim'on Posada and Forte, Juston and Fulford, Is-abella and Gao, Leo and Georges, Elie and Gibson, C. and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Raphael and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, S. and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, B. and Jun, Heewoo and Kaftan, Tomer and Kaiser, Lukasz and Kamali, Ali and Kanitscheider, I. and Keskar, N. and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Hendrik and Kiros, J. and Knight, Matthew and Kokotajlo, Daniel and Kondraciuk, Lukasz and Kondrich, A. and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, J. and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Ma-teusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, A. and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and {McGrew}, Bob and {McKinney}, S. and {McLeavey}, C. and {McMillan}, Paul and {McNeil}, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel P. and Mu, Tong and Murati, Mira and Murk, O. and M'ely, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Long, Ouyang and O'Keefe, Cullen and Pachocki, J. and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alexandre and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Pond√© de Oliveira and Pokorny, Michael and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack W. and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, M. and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin D. and Song, Yang and Staudacher, Natalie and Such, F. and Summers, Natalie and Sutskever, I. and Tang, Jie and Tezak, N. and Thompson, Madeleine and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cer'on and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll L. and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, P. and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qim-ing and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
  urldate  = {2024-09-22},
  date     = {2023-03-15},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/BKL27X2P/Achiam et al. - 2023 - GPT-4 Technical Report.pdf:application/pdf}
}


@inproceedings{tan_sparse_2023,
  address   = {Singapore},
  title     = {Sparse {Universal} {Transformer}},
  url       = {https://aclanthology.org/2023.emnlp-main.12},
  doi       = {10.18653/v1/2023.emnlp-main.12},
  urldate   = {2024-02-17},
  booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  publisher = {Association for Computational Linguistics},
  author    = {Tan, Shawn and Shen, Yikang and Chen, Zhenfang and Courville, Aaron and Gan, Chuang},
  editor    = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  month     = dec,
  year      = {2023},
  pages     = {169--179},
  file      = {Full Text PDF:/Users/imran/Zotero/storage/FBXD3LN4/Tan et al. - 2023 - Sparse Universal Transformer.pdf:application/pdf}
}

@inproceedings{brown_language_2020,
  title     = {Language Models are Few-Shot Learners},
  volume    = {33},
  url       = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  abstract  = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora.},
  pages     = {1877--1901},
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  urldate   = {2024-09-22},
  date      = {2020},
  file      = {Full Text PDF:/Users/imran/Zotero/storage/2CHSCG69/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf}
}

@misc{wang_cot_2024,
  title      = {Chain-of-Thought Reasoning Without Prompting},
  url        = {http://arxiv.org/abs/2402.10200},
  doi        = {10.48550/arXiv.2402.10200},
  abstract   = {In enhancing the reasoning capabilities of large language models ({LLMs}), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought ({CoT}) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can {LLMs} reason effectively without prompting? Our findings reveal that, intriguingly, {CoT} reasoning paths can be elicited from pre-trained {LLMs} by simply altering the {\textbackslash}textit\{decoding\} process. Rather than conventional greedy decoding, we investigate the top-\$k\$ alternative tokens, uncovering that {CoT} paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the {LLMs}' {\textbackslash}textit\{intrinsic\} reasoning abilities. Moreover, we observe that the presence of a {CoT} in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between {CoT} and non-{CoT} paths. Extensive empirical studies on various reasoning benchmarks show that the proposed {CoT}-decoding effectively elicits reasoning capabilities from language models, which were previously obscured by standard greedy decoding.},
  number     = {{arXiv}:2402.10200},
  publisher  = {{arXiv}},
  author     = {Wang, Xuezhi and Zhou, Denny},
  urldate    = {2024-09-22},
  date       = {2024-05-23},
  eprinttype = {arxiv},
  eprint     = {2402.10200 [cs]},
  keywords   = {Computer Science - Computation and Language},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/WG8Y37S9/Wang and Zhou - 2024 - Chain-of-Thought Reasoning Without Prompting.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/6L68HH6M/2402.html:text/html}
}


@inproceedings{dziri_faith_2023,
  title      = {Faith and {Fate}: {Limits} of {Transformers} on {Compositionality}},
  shorttitle = {Faith and {Fate}},
  url        = {https://openreview.net/forum?id=Fkckkr3ya8},
  language   = {en},
  urldate    = {2024-02-17},
  author     = {Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D. and Sanyal, Soumya and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin},
  month      = nov,
  year       = {2023},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/487IGXRD/Dziri et al. - 2023 - Faith and Fate Limits of Transformers on Composit.pdf:application/pdf}
}

@inproceedings{dehghani_universal_2018,
  title    = {Universal {Transformers}},
  url      = {https://openreview.net/forum?id=HyzdRiR9Y7},
  language = {en},
  urldate  = {2024-02-17},
  author   = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz},
  month    = sep,
  year     = {2018},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/NSQKNVF4/Dehghani et al. - 2018 - Universal Transformers.pdf:application/pdf}
}

@inproceedings{jaegle_perceiver_2021,
  title      = {Perceiver: {General} {Perception} with {Iterative} {Attention}},
  shorttitle = {Perceiver},
  url        = {https://proceedings.mlr.press/v139/jaegle21a.html},
  language   = {en},
  urldate    = {2024-02-17},
  booktitle  = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
  publisher  = {PMLR},
  author     = {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  month      = jul,
  year       = {2021},
  note       = {ISSN: 2640-3498},
  pages      = {4651--4664},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/RWGITQ7Q/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Atten.pdf:application/pdf}
}

@inproceedings{lee_teaching_2023,
  title    = {Teaching {Arithmetic} to {Small} {Transformers}},
  url      = {https://openreview.net/forum?id=YfhuG7xHQ8&referrer=%5Bthe%20profile%20of%20Jason%20D.%20Lee%5D(%2Fprofile%3Fid%3D~Jason_D._Lee1)},
  language = {en},
  urldate  = {2024-02-17},
  author   = {Lee, Nayoung and Sreenivasan, Kartik and Lee, Jason and Lee, Kangwook and Papailiopoulos, Dimitris},
  month    = oct,
  year     = {2023},
  file     = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/HHYDQNLM/Lee et al. - 2023 - Teaching Arithmetic to Small Transformers.pdf:application/pdf;Snapshot:/Users/imran/Zotero/storage/3WKE2A4U/forum.html:text/html}
}

@inproceedings{kazemnejad_impact_2023,
  title    = {The {Impact} of {Positional} {Encoding} on {Length} {Generalization} in {Transformers}},
  url      = {https://openreview.net/forum?id=Drrl2gcjzl},
  language = {en},
  urldate  = {2024-02-17},
  author   = {Kazemnejad, Amirhossein and Padhi, Inkit and Natesan, Karthikeyan and Das, Payel and Reddy, Siva},
  month    = nov,
  year     = {2023},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/9477TW63/Kazemnejad et al. - 2023 - The Impact of Positional Encoding on Length Genera.pdf:application/pdf}
}

@misc{jelassi_length_2023,
  title     = {Length {Generalization} in {Arithmetic} {Transformers}},
  url       = {http://arxiv.org/abs/2306.15400},
  doi       = {10.48550/arXiv.2306.15400},
  urldate   = {2024-02-17},
  publisher = {arXiv},
  author    = {Jelassi, Samy and d'Ascoli, St√©phane and Domingo-Enrich, Carles and Wu, Yuhuai and Li, Yuanzhi and Charton, Fran√ßois},
  month     = jun,
  year      = {2023},
  note      = {arXiv:2306.15400 [cs]},
  keywords  = {Computer Science - Machine Learning},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/I4USSMT4/Jelassi et al. - 2023 - Length Generalization in Arithmetic Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/QT5B6SHH/2306.html:text/html}
}

@misc{duan_interpolation_2023,
  title      = {From {Interpolation} to {Extrapolation}: {Complete} {Length} {Generalization} for {Arithmetic} {Transformers}},
  shorttitle = {From {Interpolation} to {Extrapolation}},
  url        = {http://arxiv.org/abs/2310.11984},
  doi        = {10.48550/arXiv.2310.11984},
  urldate    = {2024-02-17},
  publisher  = {arXiv},
  author     = {Duan, Shaoxiong and Shi, Yining},
  month      = oct,
  year       = {2023},
  note       = {arXiv:2310.11984 [cs]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/4SKLHNBB/Duan and Shi - 2023 - From Interpolation to Extrapolation Complete Leng.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/88IKFLLI/2310.html:text/html}
}

@misc{zhou_what_2023,
  title      = {What {Algorithms} can {Transformers} {Learn}? {A} {Study} in {Length} {Generalization}},
  shorttitle = {What {Algorithms} can {Transformers} {Learn}?},
  url        = {http://arxiv.org/abs/2310.16028},
  doi        = {10.48550/arXiv.2310.16028},
  urldate    = {2024-02-17},
  publisher  = {arXiv},
  author     = {Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  month      = oct,
  year       = {2023},
  note       = {arXiv:2310.16028 [cs, stat]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/QATAESL4/Zhou et al. - 2023 - What Algorithms can Transformers Learn A Study in.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/5TLRY5BC/2310.html:text/html}
}

@inproceedings{zhang_towards_2023,
  title      = {Towards {Best} {Practices} of {Activation} {Patching} in {Language} {Models}: {Metrics} and {Methods}},
  shorttitle = {Towards {Best} {Practices} of {Activation} {Patching} in {Language} {Models}},
  url        = {https://openreview.net/forum?id=Hf17y6u9BC},
  language   = {en},
  urldate    = {2024-02-17},
  author     = {Zhang, Fred and Nanda, Neel},
  month      = oct,
  year       = {2023},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/X7TERYJV/Zhang and Nanda - 2023 - Towards Best Practices of Activation Patching in L.pdf:application/pdf}
}

@misc{weng_transformer_2023,
  title    = {The {Transformer} {Family} {Version} 2.0},
  url      = {https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/},
  language = {en},
  urldate  = {2024-02-17},
  author   = {Weng, Lilian},
  month    = jan,
  year     = {2023},
  note     = {Section: posts},
  file     = {Snapshot:/Users/imran/Zotero/storage/LK7MU8FY/2023-01-27-the-transformer-family-v2.html:text/html}
}

@inproceedings{vaswani_attention_2017,
  title     = {Attention is {All} you {Need}},
  volume    = {30},
  urldate   = {2024-02-17},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, ≈Åukasz and Polosukhin, Illia},
  year      = {2017},
  file      = {Full Text PDF:/Users/imran/Zotero/storage/Y3ZZFQG3/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf}
}

@misc{zhou_transformers_2024,
  title     = {Transformers {Can} {Achieve} {Length} {Generalization} {But} {Not} {Robustly}},
  url       = {http://arxiv.org/abs/2402.09371},
  urldate   = {2024-02-17},
  publisher = {arXiv},
  author    = {Zhou, Yongchao and Alon, Uri and Chen, Xinyun and Wang, Xuezhi and Agarwal, Rishabh and Zhou, Denny},
  month     = feb,
  year      = {2024},
  note      = {arXiv:2402.09371 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/TCXLNJX2/Zhou et al. - 2024 - Transformers Can Achieve Length Generalization But.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/7JZCRFYR/2402.html:text/html}
}

@inproceedings{brody_expressivity_2023,
  address   = {Toronto, Canada},
  title     = {On the {Expressivity} {Role} of {LayerNorm} in {Transformers}' {Attention}},
  url       = {https://aclanthology.org/2023.findings-acl.895},
  doi       = {10.18653/v1/2023.findings-acl.895},
  urldate   = {2024-02-17},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
  publisher = {Association for Computational Linguistics},
  author    = {Brody, Shaked and Alon, Uri and Yahav, Eran},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = jul,
  year      = {2023},
  pages     = {14211--14221},
  file      = {Full Text PDF:/Users/imran/Zotero/storage/6XU5AY6G/Brody et al. - 2023 - On the Expressivity Role of LayerNorm in Transform.pdf:application/pdf}
}

@misc{noauthor_write_nodate,
  title   = {Write the {Paper} {First}},
  url     = {https://www.cs.jhu.edu/~jason/advice/write-the-paper-first.html},
  urldate = {2024-02-22},
  file    = {Write the Paper First:/Users/imran/Zotero/storage/6NWXYGVP/write-the-paper-first.html:text/html}
}

@article{kevinrowang_gears-level_nodate,
  title    = {Gears-{Level} {Mental} {Models} of {Transformer} {Interpretability}},
  url      = {https://www.alignmentforum.org/posts/X26ksz4p3wSyycKNB/gears-level-mental-models-of-transformer-interpretability},
  language = {en},
  urldate  = {2024-02-26},
  author   = {KevinRoWang},
  file     = {Snapshot:/Users/imran/Zotero/storage/PKLPXZ75/gears-level-mental-models-of-transformer-interpretability.html:text/html}
}

@misc{nichani_how_2024,
  title     = {How {Transformers} {Learn} {Causal} {Structure} with {Gradient} {Descent}},
  url       = {http://arxiv.org/abs/2402.14735},
  doi       = {10.48550/arXiv.2402.14735},
  urldate   = {2024-02-26},
  publisher = {arXiv},
  author    = {Nichani, Eshaan and Damian, Alex and Lee, Jason D.},
  month     = feb,
  year      = {2024},
  note      = {arXiv:2402.14735 [cs, math, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Theory},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/CP7N87IC/Nichani et al. - 2024 - How Transformers Learn Causal Structure with Gradient Descent.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/AI47H2KU/2402.html:text/html}
}

@inproceedings{geva_transformer_2021,
  address   = {Online and Punta Cana, Dominican Republic},
  title     = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
  url       = {https://aclanthology.org/2021.emnlp-main.446},
  doi       = {10.18653/v1/2021.emnlp-main.446},
  urldate   = {2024-02-26},
  booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  publisher = {Association for Computational Linguistics},
  author    = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  editor    = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
  month     = nov,
  year      = {2021},
  pages     = {5484--5495},
  file      = {Full Text PDF:/Users/imran/Zotero/storage/ASY2K6GZ/Geva et al. - 2021 - Transformer Feed-Forward Layers Are Key-Value Memories.pdf:application/pdf}
}

@inproceedings{quirke_understanding_2023,
  title    = {Understanding {Addition} in {Transformers}},
  url      = {https://openreview.net/forum?id=rIx1YXVWZb},
  language = {en},
  urldate  = {2024-03-11},
  author   = {Quirke, Philip and Barez, Fazl},
  month    = oct,
  year     = {2023},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/NJIS3P76/Quirke and Barez - 2023 - Understanding Addition in Transformers.pdf:application/pdf}
}

@misc{huang_music_2018,
  title     = {Music {Transformer}},
  url       = {http://arxiv.org/abs/1809.04281},
  doi       = {10.48550/arXiv.1809.04281},
  urldate   = {2024-04-03},
  publisher = {arXiv},
  author    = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Monica and Eck, Douglas},
  month     = dec,
  year      = {2018},
  note      = {arXiv:1809.04281 [cs, eess, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, relative positional encoding},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/NXBC33K5/Huang et al. - 2018 - Music Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/L39XF2PN/1809.html:text/html}
}

@inproceedings{shaw_self-attention_2018,
  address   = {New Orleans, Louisiana},
  title     = {Self-{Attention} with {Relative} {Position} {Representations}},
  url       = {https://aclanthology.org/N18-2074},
  doi       = {10.18653/v1/N18-2074},
  urldate   = {2024-04-03},
  booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 2 ({Short} {Papers})},
  publisher = {Association for Computational Linguistics},
  author    = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  editor    = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
  month     = jun,
  year      = {2018},
  keywords  = {relative positional encoding},
  pages     = {464--468},
  file      = {Full Text PDF:/Users/imran/Zotero/storage/MIHGM5NR/Shaw et al. - 2018 - Self-Attention with Relative Position Representations.pdf:application/pdf}
}

@misc{olsson_-context_2022,
  title     = {In-context {Learning} and {Induction} {Heads}},
  url       = {http://arxiv.org/abs/2209.11895},
  doi       = {10.48550/arXiv.2209.11895},
  urldate   = {2024-04-07},
  publisher = {arXiv},
  author    = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  month     = sep,
  year      = {2022},
  note      = {arXiv:2209.11895 [cs]},
  keywords  = {Computer Science - Machine Learning},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/84VXWPJ3/Olsson et al. - 2022 - In-context Learning and Induction Heads.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/943CWW47/2209.html:text/html}
}

@inproceedings{nanda_progress_2022,
  title    = {Progress measures for grokking via mechanistic interpretability},
  url      = {https://openreview.net/forum?id=9XFSbDPmdW},
  language = {en},
  urldate  = {2024-04-09},
  author   = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  month    = sep,
  year     = {2022},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/2YJWPRGB/Nanda et al. - 2022 - Progress measures for grokking via mechanistic interpretability.pdf:application/pdf}
}

@misc{burtsev_memory_2021,
  title     = {Memory {Transformer}},
  url       = {http://arxiv.org/abs/2006.11527},
  doi       = {10.48550/arXiv.2006.11527},
  urldate   = {2024-05-06},
  publisher = {arXiv},
  author    = {Burtsev, Mikhail S. and Kuratov, Yuri and Peganov, Anton and Sapunov, Grigory V.},
  month     = feb,
  year      = {2021},
  note      = {arXiv:2006.11527 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/98WTTXQ6/Burtsev et al. - 2021 - Memory Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/T6LCJEZA/2006.html:text/html}
}

@article{bulatov_recurrent_2022,
  title    = {Recurrent {Memory} {Transformer}},
  volume   = {35},
  language = {en},
  urldate  = {2024-05-06},
  journal  = {Advances in Neural Information Processing Systems},
  author   = {Bulatov, Aydar and Kuratov, Yury and Burtsev, Mikhail},
  month    = dec,
  year     = {2022},
  pages    = {11079--11091},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/8J658HLK/Bulatov et al. - 2022 - Recurrent Memory Transformer.pdf:application/pdf}
}

@inproceedings{darcet_vision_2023,
  title    = {Vision {Transformers} {Need} {Registers}},
  url      = {https://openreview.net/forum?id=2dnO3LLiJ1},
  language = {en},
  urldate  = {2024-05-06},
  author   = {Darcet, Timoth√©e and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  month    = oct,
  year     = {2023},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/6U4TV8FZ/Darcet et al. - 2023 - Vision Transformers Need Registers.pdf:application/pdf}
}

@misc{ferrando_primer_2024,
  title     = {A {Primer} on the {Inner} {Workings} of {Transformer}-based {Language} {Models}},
  url       = {http://arxiv.org/abs/2405.00208},
  doi       = {10.48550/arXiv.2405.00208},
  urldate   = {2024-05-06},
  publisher = {arXiv},
  author    = {Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and Costa-juss√†, Marta R.},
  month     = may,
  year      = {2024},
  note      = {arXiv:2405.00208 [cs]},
  keywords  = {Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/PTUTVIW2/Ferrando et al. - 2024 - A Primer on the Inner Workings of Transformer-based Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/4MRV79CY/2405.html:text/html}
}

@article{csordas_systematic_2023,
  title   = {Systematic generalization in connectionist models},
  url     = {https://sonar.ch/global/documents/326205},
  urldate = {2024-05-13},
  author  = {Csord√°s, R√≥bert},
  year    = {2023},
  file    = {Available Version (via Google Scholar):/Users/imran/Zotero/storage/HGTMNSJQ/Csord√°s - 2023 - Systematic generalization in connectionist models.pdf:application/pdf}
}

@misc{ruoss_randomized_2023,
  title     = {Randomized {Positional} {Encodings} {Boost} {Length} {Generalization} of {Transformers}},
  url       = {http://arxiv.org/abs/2305.16843},
  doi       = {10.48550/arXiv.2305.16843},
  urldate   = {2024-05-13},
  publisher = {arXiv},
  author    = {Ruoss, Anian and Del√©tang, Gr√©goire and Genewein, Tim and Grau-Moya, Jordi and Csord√°s, R√≥bert and Bennani, Mehdi and Legg, Shane and Veness, Joel},
  month     = may,
  year      = {2023},
  note      = {arXiv:2305.16843 [cs, stat]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/RFPSTUBR/Ruoss et al. - 2023 - Randomized Positional Encodings Boost Length Generalization of Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/PUAVCVQJ/2305.html:text/html}
}

@misc{pfau_lets_2024,
  title      = {Let's {Think} {Dot} by {Dot}: {Hidden} {Computation} in {Transformer} {Language} {Models}},
  shorttitle = {Let's {Think} {Dot} by {Dot}},
  url        = {http://arxiv.org/abs/2404.15758},
  doi        = {10.48550/arXiv.2404.15758},
  urldate    = {2024-05-13},
  publisher  = {arXiv},
  author     = {Pfau, Jacob and Merrill, William and Bowman, Samuel R.},
  month      = apr,
  year       = {2024},
  note       = {arXiv:2404.15758 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.6},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/5D8Y9BKR/Pfau et al. - 2024 - Let's Think Dot by Dot Hidden Computation in Transformer Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/WB3ADNXW/2404.html:text/html}
}

@misc{gurnee_finding_2023,
  title      = {Finding {Neurons} in a {Haystack}: {Case} {Studies} with {Sparse} {Probing}},
  shorttitle = {Finding {Neurons} in a {Haystack}},
  url        = {http://arxiv.org/abs/2305.01610},
  doi        = {10.48550/arXiv.2305.01610},
  urldate    = {2024-05-21},
  publisher  = {arXiv},
  author     = {Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
  month      = jun,
  year       = {2023},
  note       = {arXiv:2305.01610 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/5W4DH89V/Gurnee et al. - 2023 - Finding Neurons in a Haystack Case Studies with Sparse Probing.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/97XEDDY7/2305.html:text/html}
}

@misc{hase_unreasonable_2024,
  title     = {The {Unreasonable} {Effectiveness} of {Easy} {Training} {Data} for {Hard} {Tasks}},
  url       = {http://arxiv.org/abs/2401.06751},
  doi       = {10.48550/arXiv.2401.06751},
  urldate   = {2024-05-21},
  publisher = {arXiv},
  author    = {Hase, Peter and Bansal, Mohit and Clark, Peter and Wiegreffe, Sarah},
  month     = jan,
  year      = {2024},
  note      = {arXiv:2401.06751 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/8RMVJWFQ/Hase et al. - 2024 - The Unreasonable Effectiveness of Easy Training Data for Hard Tasks.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/V8I2Q9S4/2401.html:text/html}
}

@inproceedings{hanna_how_2023,
  title      = {How does {GPT}-2 compute greater-than?: {Interpreting} mathematical abilities in a pre-trained language model},
  shorttitle = {How does {GPT}-2 compute greater-than?},
  url        = {https://openreview.net/forum?id=p4PckNQR8k},
  language   = {en},
  urldate    = {2024-05-28},
  author     = {Hanna, Michael and Liu, Ollie and Variengien, Alexandre},
  month      = nov,
  year       = {2023},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/MR93QSSH/Hanna et al. - 2023 - How does GPT-2 compute greater-than Interpreting mathematical abilities in a pre-trained language.pdf:application/pdf}
}

@misc{mcleish_transformers_2024,
  title     = {Transformers {Can} {Do} {Arithmetic} with the {Right} {Embeddings}},
  url       = {http://arxiv.org/abs/2405.17399},
  doi       = {10.48550/arXiv.2405.17399},
  urldate   = {2024-06-02},
  publisher = {arXiv},
  author    = {McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and Goldstein, Tom},
  month     = may,
  year      = {2024},
  note      = {arXiv:2405.17399 [cs]
               version: 1},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, abacus},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/GN7JXQZU/McLeish et al. - 2024 - Transformers Can Do Arithmetic with the Right Embeddings.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/X526D7VQ/2405.html:text/html}
}

@misc{bounsi_transformers_2024,
  title     = {Transformers meet {Neural} {Algorithmic} {Reasoners}},
  url       = {http://arxiv.org/abs/2406.09308},
  doi       = {10.48550/arXiv.2406.09308},
  urldate   = {2024-06-14},
  publisher = {arXiv},
  author    = {Bounsi, Wilfried and Ibarz, Borja and Dudzik, Andrew and Hamrick, Jessica B. and Markeeva, Larisa and Vitvitskyi, Alex and Pascanu, Razvan and Veliƒçkoviƒá, Petar},
  month     = jun,
  year      = {2024},
  note      = {arXiv:2406.09308 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/KWRGY9NL/Bounsi et al. - 2024 - Transformers meet Neural Algorithmic Reasoners.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/IAHQJDWQ/2406.html:text/html}
}

@misc{li_functional_2024,
  title     = {Functional {Interpolation} for {Relative} {Positions} {Improves} {Long} {Context} {Transformers}},
  url       = {http://arxiv.org/abs/2310.04418},
  language  = {en},
  urldate   = {2024-06-14},
  publisher = {arXiv},
  author    = {Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh},
  month     = mar,
  year      = {2024},
  note      = {arXiv:2310.04418 [cs]},
  keywords  = {Computer Science - Machine Learning},
  file      = {Li et al. - 2024 - Functional Interpolation for Relative Positions Improves Long Context Transformers.pdf:/Users/imran/Zotero/storage/SH4FTD4F/Li et al. - 2024 - Functional Interpolation for Relative Positions Improves Long Context Transformers.pdf:application/pdf}
}

@inproceedings{dave_investigating_2024,
  title   = {Investigating {Symbolic} {Capabilities} of {Large} {Language} {Models}},
  url     = {https://www.semanticscholar.org/paper/Investigating-Symbolic-Capabilities-of-Large-Models-Dave-Kifer/cb1addf9cefe4e96763d28437f72a3d3cbfa7225},
  urldate = {2024-06-15},
  author  = {Dave, Neisarg and Kifer, Daniel and Giles, C. L. and Mali, A.},
  month   = may,
  year    = {2024},
  file    = {Full Text PDF:/Users/imran/Zotero/storage/NSCLGZIZ/Dave et al. - 2024 - Investigating Symbolic Capabilities of Large Language Models.pdf:application/pdf}
}

@article{su_roformer_2024,
  title      = {{RoFormer}: {Enhanced} transformer with {Rotary} {Position} {Embedding}},
  volume     = {568},
  issn       = {0925-2312},
  shorttitle = {{RoFormer}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0925231223011864},
  doi        = {10.1016/j.neucom.2023.127063},
  urldate    = {2024-06-17},
  journal    = {Neurocomputing},
  author     = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  month      = feb,
  year       = {2024},
  keywords   = {Natural language processing, Position information encoding, Pre-trained language models, Pre-training},
  pages      = {127063},
  file       = {ScienceDirect Snapshot:/Users/imran/Zotero/storage/4B4ZQLZX/S0925231223011864.html:text/html;Submitted Version:/Users/imran/Zotero/storage/SY4J7D58/Su et al. - 2024 - RoFormer Enhanced transformer with Rotary Position Embedding.pdf:application/pdf}
}

@misc{treutlein_connecting_2024,
  title      = {Connecting the {Dots}: {LLMs} can {Infer} and {Verbalize} {Latent} {Structure} from {Disparate} {Training} {Data}},
  shorttitle = {Connecting the {Dots}},
  url        = {http://arxiv.org/abs/2406.14546},
  doi        = {10.48550/arXiv.2406.14546},
  urldate    = {2024-06-22},
  publisher  = {arXiv},
  author     = {Treutlein, Johannes and Choi, Dami and Betley, Jan and Anil, Cem and Marks, Samuel and Grosse, Roger Baker and Evans, Owain},
  month      = jun,
  year       = {2024},
  note       = {arXiv:2406.14546 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/M8RT672I/Treutlein et al. - 2024 - Connecting the Dots LLMs can Infer and Verbalize Latent Structure from Disparate Training Data.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/ERUZBBKE/2406.html:text/html}
}

@misc{goyal_think_2024,
  title      = {Think before you speak: {Training} {Language} {Models} {With} {Pause} {Tokens}},
  shorttitle = {Think before you speak},
  url        = {http://arxiv.org/abs/2310.02226},
  doi        = {10.48550/arXiv.2310.02226},
  urldate    = {2024-07-15},
  publisher  = {arXiv},
  author     = {Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh},
  month      = apr,
  year       = {2024},
  note       = {arXiv:2310.02226 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/BCEY9NLB/Goyal et al. - 2024 - Think before you speak Training Language Models With Pause Tokens.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/VFUW5PJ5/2310.html:text/html}
}

@misc{zelikman_quiet-star_2024,
  title      = {Quiet-{STaR}: {Language} {Models} {Can} {Teach} {Themselves} to {Think} {Before} {Speaking}},
  shorttitle = {Quiet-{STaR}},
  url        = {http://arxiv.org/abs/2403.09629},
  doi        = {10.48550/arXiv.2403.09629},
  urldate    = {2024-07-15},
  publisher  = {arXiv},
  author     = {Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D.},
  month      = mar,
  year       = {2024},
  note       = {arXiv:2403.09629 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/73MUYPNX/Zelikman et al. - 2024 - Quiet-STaR Language Models Can Teach Themselves to Think Before Speaking.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/PMCMHD5G/2403.html:text/html}
}

@article{zelikman_star_2022,
  title      = {{STaR}: {Bootstrapping} {Reasoning} {With} {Reasoning}},
  volume     = {35},
  shorttitle = {{STaR}},
  language   = {en},
  urldate    = {2024-07-31},
  journal    = {Advances in Neural Information Processing Systems},
  author     = {Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  month      = dec,
  year       = {2022},
  pages      = {15476--15488},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/Q83E9B8A/Zelikman et al. - 2022 - STaR Bootstrapping Reasoning With Reasoning.pdf:application/pdf}
}

@inproceedings{ahuja_provable_2024,
  title    = {On {Provable} {Length} and {Compositional} {Generalization}},
  url      = {https://openreview.net/forum?id=xuwtmXiHMT},
  language = {en},
  urldate  = {2024-08-05},
  author   = {Ahuja, Kartik and Mansouri, Amin},
  month    = jul,
  year     = {2024},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/GJVWSS5A/Ahuja and Mansouri - 2024 - On Provable Length and Compositional Generalization.pdf:application/pdf}
}

@misc{quirke_increasing_2024,
  title     = {Increasing {Trust} in {Language} {Models} through the {Reuse} of {Verified} {Circuits}},
  url       = {http://arxiv.org/abs/2402.02619},
  doi       = {10.48550/arXiv.2402.02619},
  urldate   = {2024-08-05},
  publisher = {arXiv},
  author    = {Quirke, Philip and Neo, Clement and Barez, Fazl},
  month     = jul,
  year      = {2024},
  note      = {arXiv:2402.02619 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/TWUL6STU/Quirke et al. - 2024 - Increasing Trust in Language Models through the Reuse of Verified Circuits.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/YLWZRACN/2402.html:text/html}
}

@article{gao_going_2024,
  title      = {Going {Beyond} {XAI}: {A} {Systematic} {Survey} for {Explanation}-{Guided} {Learning}},
  volume     = {56},
  issn       = {0360-0300},
  shorttitle = {Going {Beyond} {XAI}},
  url        = {https://dl.acm.org/doi/10.1145/3644073},
  doi        = {10.1145/3644073},
  number     = {7},
  urldate    = {2024-09-03},
  journal    = {ACM Comput. Surv.},
  author     = {Gao, Yuyang and Gu, Siyi and Jiang, Junji and Hong, Sungsoo Ray and Yu, Dazhou and Zhao, Liang},
  month      = apr,
  year       = {2024},
  pages      = {188:1--188:39},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/PTK28IT3/Gao et al. - 2024 - Going Beyond XAI A Systematic Survey for Explanation-Guided Learning.pdf:application/pdf}
}

@misc{yehudai_when_2024,
  title     = {When {Can} {Transformers} {Count} to n?},
  url       = {http://arxiv.org/abs/2407.15160},
  doi       = {10.48550/arXiv.2407.15160},
  urldate   = {2024-09-06},
  publisher = {arXiv},
  author    = {Yehudai, Gilad and Kaplan, Haim and Ghandeharioun, Asma and Geva, Mor and Globerson, Amir},
  month     = jul,
  year      = {2024},
  note      = {arXiv:2407.15160 [cs]
               version: 1},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/P97JW2MA/Yehudai et al. - 2024 - When Can Transformers Count to n.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/FQ26M4RR/2407.html:text/html}
}

@misc{deng_explicit_2024,
  title      = {From {Explicit} {CoT} to {Implicit} {CoT}: {Learning} to {Internalize} {CoT} {Step} by {Step}},
  shorttitle = {From {Explicit} {CoT} to {Implicit} {CoT}},
  url        = {http://arxiv.org/abs/2405.14838},
  doi        = {10.48550/arXiv.2405.14838},
  urldate    = {2024-09-06},
  publisher  = {arXiv},
  author     = {Deng, Yuntian and Choi, Yejin and Shieber, Stuart},
  month      = may,
  year       = {2024},
  note       = {arXiv:2405.14838 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/W9PALGMI/Deng et al. - 2024 - From Explicit CoT to Implicit CoT Learning to Internalize CoT Step by Step.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/YTV4VD2H/2405.html:text/html}
}

@inproceedings{krubinski_basic_2023,
  title    = {Basic {Arithmetic} {Properties} in the {Space} of {Language} {Model} {Prompts}},
  url      = {https://openreview.net/forum?id=RCiRtdERCW},
  language = {en},
  urldate  = {2024-09-09},
  author   = {Krubi≈Ñski, Mateusz},
  month    = oct,
  year     = {2023},
  file     = {24.pdf:/Users/imran/Zotero/storage/GIP7WIBB/24.pdf:application/pdf;Snapshot:/Users/imran/Zotero/storage/JNWMLC89/forum.html:text/html}
}

@inproceedings{press_measuring_2023,
  address   = {Singapore},
  title     = {Measuring and {Narrowing} the {Compositionality} {Gap} in {Language} {Models}},
  url       = {https://aclanthology.org/2023.findings-emnlp.378},
  doi       = {10.18653/v1/2023.findings-emnlp.378},
  urldate   = {2024-09-09},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
  publisher = {Association for Computational Linguistics},
  author    = {Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah and Lewis, Mike},
  editor    = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  month     = dec,
  year      = {2023},
  pages     = {5687--5711},
  file      = {Full Text PDF:/Users/imran/Zotero/storage/WG8AISKZ/Press et al. - 2023 - Measuring and Narrowing the Compositionality Gap in Language Models.pdf:application/pdf}
}

@article{hupkes_compositionality_2020,
  title      = {Compositionality {Decomposed}: {How} do {Neural} {Networks} {Generalise}?},
  volume     = {67},
  copyright  = {Copyright (c)},
  issn       = {1076-9757},
  shorttitle = {Compositionality {Decomposed}},
  url        = {https://jair.org/index.php/jair/article/view/11674},
  doi        = {10.1613/jair.1.11674},
  language   = {en},
  urldate    = {2024-09-10},
  journal    = {Journal of Artificial Intelligence Research},
  author     = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  month      = apr,
  year       = {2020},
  keywords   = {machine learning, natural language, neural networks, rule learning},
  pages      = {757--795},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/47YV5B8P/Hupkes et al. - 2020 - Compositionality Decomposed How do Neural Networks Generalise.pdf:application/pdf}
}

@misc{li_chain_2024,
  title     = {Chain of {Thought} {Empowers} {Transformers} to {Solve} {Inherently} {Serial} {Problems}},
  url       = {http://arxiv.org/abs/2402.12875},
  doi       = {10.48550/arXiv.2402.12875},
  urldate   = {2024-09-16},
  publisher = {arXiv},
  author    = {Li, Zhiyuan and Liu, Hong and Zhou, Denny and Ma, Tengyu},
  month     = may,
  year      = {2024},
  note      = {arXiv:2402.12875 [cs, stat]},
  keywords  = {Computer Science - Computational Complexity, Computer Science - Machine Learning, Statistics - Machine Learning},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/MDST3MMG/Li et al. - 2024 - Chain of Thought Empowers Transformers to Solve Inherently Serial Problems.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/JFB64RP5/2402.html:text/html}
}
