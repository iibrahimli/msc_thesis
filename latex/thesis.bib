
@inproceedings{radford_improving_2018,
  title    = {Improving Language Understanding by Generative Pre-Training},
  url      = {https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering ({RACE}), and 1.5\% on textual entailment ({MultiNLI}).},
  author   = {Radford, Alec and Narasimhan, Karthik},
  urldate  = {2024-10-05},
  date     = {2018},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/IEFBSG99/Radford and Narasimhan - 2018 - Improving Language Understanding by Generative Pre-Training.pdf:application/pdf}
}

@article{he_deep_2016,
  title        = {Deep Residual Learning for Image Recognition},
  url          = {http://ieeexplore.ieee.org/document/7780459/},
  doi          = {10.1109/CVPR.2016.90},
  abstract     = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than {VGG} nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions1, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
  pages        = {770--778},
  journaltitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  urldate      = {2024-10-05},
  date         = {2016-06},
  note         = {Conference Name: 2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})
                  {ISBN}: 9781467388511
                  Place: Las Vegas, {NV}, {USA}
                  Publisher: {IEEE}},
  file         = {Accepted Version:/Users/imran/Zotero/storage/NWVCZCQI/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@misc{ba_layer_2016,
  title      = {Layer Normalization},
  url        = {http://arxiv.org/abs/1607.06450},
  doi        = {10.48550/arXiv.1607.06450},
  abstract   = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  number     = {{arXiv}:1607.06450},
  publisher  = {{arXiv}},
  author     = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  urldate    = {2024-10-04},
  date       = {2016-07-21},
  eprinttype = {arxiv},
  eprint     = {1607.06450 [cs, stat]},
  keywords   = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/RY5LZ2S5/Ba et al. - 2016 - Layer Normalization.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/AUWUJRPT/1607.html:text/html}
}

@inproceedings{loshchilov_decoupled_2018,
  title      = {Decoupled Weight Decay Regularization},
  url        = {https://openreview.net/forum?id=Bkg6RiCqY7},
  abstract   = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it ``weight decay'' in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard {SGD} and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with {SGD} with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in {TensorFlow} and {PyTorch}; the complete source code for our experiments is available at {\textbackslash}url\{https://github.com/loshchil/{AdamW}-and-{SGDW}\}},
  eventtitle = {International Conference on Learning Representations},
  author     = {Loshchilov, Ilya and Hutter, Frank},
  urldate    = {2024-10-04},
  date       = {2018-09-27},
  langid     = {english},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/9GZ2U65X/Loshchilov and Hutter - 2018 - Decoupled Weight Decay Regularization.pdf:application/pdf}
}

@article{bahdanau_neural_2014,
  title        = {Neural Machine Translation by Jointly Learning to Align and Translate},
  url          = {https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5},
  abstract     = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  journaltitle = {{CoRR}},
  author       = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  urldate      = {2024-10-03},
  date         = {2014-09-01},
  file         = {Full Text PDF:/Users/imran/Zotero/storage/XK5ZZ4W8/Bahdanau et al. - 2014 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf:application/pdf}
}

@inproceedings{yang_looped_2023,
  title      = {Looped Transformers are Better at Learning Learning Algorithms},
  url        = {https://openreview.net/forum?id=HHbRxoDTxE},
  abstract   = {Transformers have demonstrated effectiveness in in-context solving data-fitting problems from various (latent) models, as reported by Garg et al. (2022). However, the absence of an inherent iterative structure in the transformer architecture presents a challenge in emulating the iterative algorithms, which are commonly employed in traditional machine learning methods. To address this, we propose the utilization of looped transformer architecture and its associated training methodology, with the aim of incorporating iterative characteristics into the transformer architectures. Experimental results suggest that the looped transformer achieves performance comparable to the standard transformer in solving various data-fitting problems, while utilizing less than 10\% of the parameter count.},
  eventtitle = {The Twelfth International Conference on Learning Representations},
  author     = {Yang, Liu and Lee, Kangwook and Nowak, Robert D. and Papailiopoulos, Dimitris},
  urldate    = {2024-10-03},
  date       = {2023-10-13},
  langid     = {english},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/JZN3EVID/Yang et al. - 2023 - Looped Transformers are Better at Learning Learning Algorithms.pdf:application/pdf}
}

@inproceedings{zhong_clock_2023,
  title      = {The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks},
  url        = {https://openreview.net/forum?id=S5wmbQc1We},
  shorttitle = {The Clock and the Pizza},
  abstract   = {Do neural networks, trained on well-understood algorithmic tasks, reliably rediscover known algorithms? Several recent studies, on tasks ranging from group operations to in-context linear regression, have suggested that the answer is yes. Using modular addition as a prototypical problem, we show that algorithm discovery in neural networks is sometimes more complex: small changes to model hyperparameters and initializations can induce discovery of qualitatively different algorithms from a fixed training set, and even learning of multiple different solutions in parallel. In modular addition, we specifically show that models learn a known *Clock* algorithm, a previously undescribed, less intuitive, but comprehensible procedure we term the *Pizza* algorithm, and a variety of even more complex procedures. Our results show that even simple learning problems can admit a surprising diversity of solutions, motivating the development of new tools for mechanistically characterizing the behavior of neural networks across the algorithmic phase space.},
  eventtitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  author     = {Zhong, Ziqian and Liu, Ziming and Tegmark, Max and Andreas, Jacob},
  urldate    = {2024-09-29},
  date       = {2023-11-02},
  langid     = {english},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/Q6GCT74V/Zhong et al. - 2023 - The Clock and the Pizza Two Stories in Mechanistic Explanation of Neural Networks.pdf:application/pdf}
}

@article{nakkiran_deep_2021,
  title        = {Deep double descent: where bigger models and more data hurt*},
  volume       = {2021},
  issn         = {1742-5468},
  url          = {https://dx.doi.org/10.1088/1742-5468/ac3a74},
  doi          = {10.1088/1742-5468/ac3a74},
  shorttitle   = {Deep double descent},
  abstract     = {We show that a variety of modern deep learning tasks exhibit a ‘double-descent’ phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
  pages        = {124003},
  number       = {12},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  shortjournal = {J. Stat. Mech.},
  author       = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  urldate      = {2024-09-24},
  date         = {2021-12},
  langid       = {english},
  note         = {Publisher: {IOP} Publishing and {SISSA}},
  file         = {IOP Full Text PDF:/Users/imran/Zotero/storage/SX3UQ8XK/Nakkiran et al. - 2021 - Deep double descent where bigger models and more data hurt.pdf:application/pdf}
}

@misc{fan_looped_2024,
  title      = {Looped Transformers for Length Generalization},
  url        = {http://arxiv.org/abs/2409.15647},
  doi        = {10.48550/arXiv.2409.15647},
  abstract   = {Recent work has shown that Transformers trained from scratch can successfully solve various arithmetic and algorithmic tasks, such as adding numbers and computing parity. While these Transformers generalize well on unseen inputs of the same length, they struggle with length generalization, i.e., handling inputs of unseen lengths. In this work, we demonstrate that looped Transformers with an adaptive number of steps significantly improve length generalization. We focus on tasks with a known iterative solution, involving multiple iterations of a {RASP}-L operation - a length-generalizable operation that can be expressed by a finite-sized Transformer. We train looped Transformers using our proposed learning algorithm and observe that they learn highly length-generalizable solutions for various tasks.},
  number     = {{arXiv}:2409.15647},
  publisher  = {{arXiv}},
  author     = {Fan, Ying and Du, Yilun and Ramchandran, Kannan and Lee, Kangwook},
  urldate    = {2024-09-27},
  date       = {2024-09-25},
  eprinttype = {arxiv},
  eprint     = {2409.15647 [cs]},
  keywords   = {Computer Science - Machine Learning},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/ILBBUVQ9/Fan et al. - 2024 - Looped Transformers for Length Generalization.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/AN8RJGHE/2409.html:text/html}
}

@inproceedings{press_train_2021,
  title      = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  url        = {https://openreview.net/forum?id=R8sQPpGCv0},
  shorttitle = {Train Short, Test Long},
  abstract   = {Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases ({ALiBi}). {ALiBi} does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11\% faster and using 11\% less memory. {ALiBi}'s inductive bias towards recency also leads it to outperform multiple strong position methods on the {WikiText}-103 benchmark.},
  eventtitle = {International Conference on Learning Representations},
  author     = {Press, Ofir and Smith, Noah and Lewis, Mike},
  urldate    = {2024-09-27},
  date       = {2021-10-06},
  langid     = {english},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/ZPEISPUS/Press et al. - 2021 - Train Short, Test Long Attention with Linear Biases Enables Input Length Extrapolation.pdf:application/pdf}
}

@misc{zhou_pre-trained_2024,
  title      = {Pre-trained Large Language Models Use Fourier Features to Compute Addition},
  url        = {http://arxiv.org/abs/2406.03445},
  doi        = {10.48550/arXiv.2406.03445},
  abstract   = {Pre-trained large language models ({LLMs}) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained {LLMs} add numbers using Fourier features -- dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, {MLP} and attention layers use Fourier features in complementary ways: {MLP} layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features. Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy. Introducing pre-trained token embeddings to a randomly initialized model rescues its performance. Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.},
  number     = {{arXiv}:2406.03445},
  publisher  = {{arXiv}},
  author     = {Zhou, Tianyi and Fu, Deqing and Sharan, Vatsal and Jia, Robin},
  urldate    = {2024-09-27},
  date       = {2024-06-05},
  eprinttype = {arxiv},
  eprint     = {2406.03445 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/AABUENSD/Zhou et al. - 2024 - Pre-trained Large Language Models Use Fourier Features to Compute Addition.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/6ZPS45SZ/2406.html:text/html}
}

@article{belkin_reconciling_2019,
  title        = {Reconciling modern machine-learning practice and the classical bias–variance trade-off},
  volume       = {116},
  url          = {https://www.pnas.org/doi/10.1073/pnas.1903070116},
  doi          = {10.1073/pnas.1903070116},
  abstract     = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
  pages        = {15849--15854},
  number       = {32},
  journaltitle = {Proceedings of the National Academy of Sciences},
  author       = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  urldate      = {2024-09-29},
  date         = {2019-08-06},
  note         = {Publisher: Proceedings of the National Academy of Sciences},
  file         = {Full Text PDF:/Users/imran/Zotero/storage/MTEINQHT/Belkin et al. - 2019 - Reconciling modern machine-learning practice and the classical bias–variance trade-off.pdf:application/pdf}
}


@inproceedings{devlin_bert_2019,
  location   = {Minneapolis, Minnesota},
  title      = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url        = {https://aclanthology.org/N19-1423},
  doi        = {10.18653/v1/N19-1423},
  shorttitle = {{BERT}},
  abstract   = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5 (7.7 point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  eventtitle = {{NAACL}-{HLT} 2019},
  pages      = {4171--4186},
  booktitle  = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  publisher  = {Association for Computational Linguistics},
  author     = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  editor     = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  urldate    = {2024-09-29},
  date       = {2019-06}
}

@inproceedings{achiam_gpt-4_2023,
  title    = {{GPT}-4 Technical Report},
  url      = {https://www.semanticscholar.org/paper/GPT-4-Technical-Report-Achiam-Adler/163b4d6a79a5b19af88b8585456363340d9efd04},
  abstract = {We report the development of {GPT}-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, {GPT}-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. {GPT}-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of {GPT}-4's performance based on models trained with no more than 1/1,000th the compute of {GPT}-4.},
  author   = {Achiam, {OpenAI} Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, S. and Balcom, Valerie and Baltescu, Paul and Bao, Haim-ing and Bavarian, Mo and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, B. and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Sim'on Posada and Forte, Juston and Fulford, Is-abella and Gao, Leo and Georges, Elie and Gibson, C. and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Raphael and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, S. and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, B. and Jun, Heewoo and Kaftan, Tomer and Kaiser, Lukasz and Kamali, Ali and Kanitscheider, I. and Keskar, N. and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Hendrik and Kiros, J. and Knight, Matthew and Kokotajlo, Daniel and Kondraciuk, Lukasz and Kondrich, A. and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, J. and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Ma-teusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, A. and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and {McGrew}, Bob and {McKinney}, S. and {McLeavey}, C. and {McMillan}, Paul and {McNeil}, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel P. and Mu, Tong and Murati, Mira and Murk, O. and M'ely, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Long, Ouyang and O'Keefe, Cullen and Pachocki, J. and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alexandre and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Pondé de Oliveira and Pokorny, Michael and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack W. and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, M. and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin D. and Song, Yang and Staudacher, Natalie and Such, F. and Summers, Natalie and Sutskever, I. and Tang, Jie and Tezak, N. and Thompson, Madeleine and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cer'on and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll L. and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, P. and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qim-ing and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
  urldate  = {2024-09-22},
  date     = {2023-03-15},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/BKL27X2P/Achiam et al. - 2023 - GPT-4 Technical Report.pdf:application/pdf}
}


@inproceedings{tan_sparse_2023,
  address   = {Singapore},
  title     = {Sparse {Universal} {Transformer}},
  url       = {https://aclanthology.org/2023.emnlp-main.12},
  doi       = {10.18653/v1/2023.emnlp-main.12},
  urldate   = {2024-02-17},
  booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  publisher = {Association for Computational Linguistics},
  author    = {Tan, Shawn and Shen, Yikang and Chen, Zhenfang and Courville, Aaron and Gan, Chuang},
  editor    = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  month     = dec,
  year      = {2023},
  pages     = {169--179},
  file      = {Full Text PDF:/Users/imran/Zotero/storage/FBXD3LN4/Tan et al. - 2023 - Sparse Universal Transformer.pdf:application/pdf}
}

@inproceedings{brown_language_2020,
  title     = {Language Models are Few-Shot Learners},
  volume    = {33},
  url       = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  abstract  = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora.},
  pages     = {1877--1901},
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  urldate   = {2024-09-22},
  date      = {2020},
  file      = {Full Text PDF:/Users/imran/Zotero/storage/2CHSCG69/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf}
}

@misc{wang_cot_2024,
  title      = {Chain-of-Thought Reasoning Without Prompting},
  url        = {http://arxiv.org/abs/2402.10200},
  doi        = {10.48550/arXiv.2402.10200},
  abstract   = {In enhancing the reasoning capabilities of large language models ({LLMs}), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought ({CoT}) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can {LLMs} reason effectively without prompting? Our findings reveal that, intriguingly, {CoT} reasoning paths can be elicited from pre-trained {LLMs} by simply altering the {\textbackslash}textit\{decoding\} process. Rather than conventional greedy decoding, we investigate the top-\$k\$ alternative tokens, uncovering that {CoT} paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the {LLMs}' {\textbackslash}textit\{intrinsic\} reasoning abilities. Moreover, we observe that the presence of a {CoT} in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between {CoT} and non-{CoT} paths. Extensive empirical studies on various reasoning benchmarks show that the proposed {CoT}-decoding effectively elicits reasoning capabilities from language models, which were previously obscured by standard greedy decoding.},
  number     = {{arXiv}:2402.10200},
  publisher  = {{arXiv}},
  author     = {Wang, Xuezhi and Zhou, Denny},
  urldate    = {2024-09-22},
  date       = {2024-05-23},
  eprinttype = {arxiv},
  eprint     = {2402.10200 [cs]},
  keywords   = {Computer Science - Computation and Language},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/WG8Y37S9/Wang and Zhou - 2024 - Chain-of-Thought Reasoning Without Prompting.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/6L68HH6M/2402.html:text/html}
}


@inproceedings{dziri_faith_2023,
  title      = {Faith and {Fate}: {Limits} of {Transformers} on {Compositionality}},
  shorttitle = {Faith and {Fate}},
  url        = {https://openreview.net/forum?id=Fkckkr3ya8},
  language   = {en},
  urldate    = {2024-02-17},
  author     = {Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D. and Sanyal, Soumya and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin},
  month      = nov,
  year       = {2023},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/487IGXRD/Dziri et al. - 2023 - Faith and Fate Limits of Transformers on Composit.pdf:application/pdf}
}

@inproceedings{dehghani_universal_2018,
  title    = {Universal {Transformers}},
  url      = {https://openreview.net/forum?id=HyzdRiR9Y7},
  language = {en},
  urldate  = {2024-02-17},
  author   = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz},
  month    = sep,
  year     = {2018},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/NSQKNVF4/Dehghani et al. - 2018 - Universal Transformers.pdf:application/pdf}
}

@inproceedings{jaegle_perceiver_2021,
  title      = {Perceiver: {General} {Perception} with {Iterative} {Attention}},
  shorttitle = {Perceiver},
  url        = {https://proceedings.mlr.press/v139/jaegle21a.html},
  language   = {en},
  urldate    = {2024-02-17},
  booktitle  = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
  publisher  = {PMLR},
  author     = {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  month      = jul,
  year       = {2021},
  note       = {ISSN: 2640-3498},
  pages      = {4651--4664},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/RWGITQ7Q/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Atten.pdf:application/pdf}
}

@inproceedings{lee_teaching_2023,
  title    = {Teaching {Arithmetic} to {Small} {Transformers}},
  url      = {https://openreview.net/forum?id=YfhuG7xHQ8&referrer=%5Bthe%20profile%20of%20Jason%20D.%20Lee%5D(%2Fprofile%3Fid%3D~Jason_D._Lee1)},
  language = {en},
  urldate  = {2024-02-17},
  author   = {Lee, Nayoung and Sreenivasan, Kartik and Lee, Jason and Lee, Kangwook and Papailiopoulos, Dimitris},
  month    = oct,
  year     = {2023},
  file     = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/HHYDQNLM/Lee et al. - 2023 - Teaching Arithmetic to Small Transformers.pdf:application/pdf;Snapshot:/Users/imran/Zotero/storage/3WKE2A4U/forum.html:text/html}
}

@inproceedings{kazemnejad_impact_2023,
  title    = {The {Impact} of {Positional} {Encoding} on {Length} {Generalization} in {Transformers}},
  url      = {https://openreview.net/forum?id=Drrl2gcjzl},
  language = {en},
  urldate  = {2024-02-17},
  author   = {Kazemnejad, Amirhossein and Padhi, Inkit and Natesan, Karthikeyan and Das, Payel and Reddy, Siva},
  month    = nov,
  year     = {2023},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/9477TW63/Kazemnejad et al. - 2023 - The Impact of Positional Encoding on Length Genera.pdf:application/pdf}
}

@misc{jelassi_length_2023,
  title     = {Length {Generalization} in {Arithmetic} {Transformers}},
  url       = {http://arxiv.org/abs/2306.15400},
  doi       = {10.48550/arXiv.2306.15400},
  urldate   = {2024-02-17},
  publisher = {arXiv},
  author    = {Jelassi, Samy and d'Ascoli, Stéphane and Domingo-Enrich, Carles and Wu, Yuhuai and Li, Yuanzhi and Charton, François},
  month     = jun,
  year      = {2023},
  note      = {arXiv:2306.15400 [cs]},
  keywords  = {Computer Science - Machine Learning},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/I4USSMT4/Jelassi et al. - 2023 - Length Generalization in Arithmetic Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/QT5B6SHH/2306.html:text/html}
}

@misc{duan_interpolation_2023,
  title      = {From {Interpolation} to {Extrapolation}: {Complete} {Length} {Generalization} for {Arithmetic} {Transformers}},
  shorttitle = {From {Interpolation} to {Extrapolation}},
  url        = {http://arxiv.org/abs/2310.11984},
  doi        = {10.48550/arXiv.2310.11984},
  urldate    = {2024-02-17},
  publisher  = {arXiv},
  author     = {Duan, Shaoxiong and Shi, Yining},
  month      = oct,
  year       = {2023},
  note       = {arXiv:2310.11984 [cs]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/4SKLHNBB/Duan and Shi - 2023 - From Interpolation to Extrapolation Complete Leng.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/88IKFLLI/2310.html:text/html}
}

@misc{zhou_what_2023,
  title      = {What {Algorithms} can {Transformers} {Learn}? {A} {Study} in {Length} {Generalization}},
  shorttitle = {What {Algorithms} can {Transformers} {Learn}?},
  url        = {http://arxiv.org/abs/2310.16028},
  doi        = {10.48550/arXiv.2310.16028},
  urldate    = {2024-02-17},
  publisher  = {arXiv},
  author     = {Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  month      = oct,
  year       = {2023},
  note       = {arXiv:2310.16028 [cs, stat]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/QATAESL4/Zhou et al. - 2023 - What Algorithms can Transformers Learn A Study in.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/5TLRY5BC/2310.html:text/html}
}

@inproceedings{zhang_towards_2023,
  title      = {Towards {Best} {Practices} of {Activation} {Patching} in {Language} {Models}: {Metrics} and {Methods}},
  shorttitle = {Towards {Best} {Practices} of {Activation} {Patching} in {Language} {Models}},
  url        = {https://openreview.net/forum?id=Hf17y6u9BC},
  language   = {en},
  urldate    = {2024-02-17},
  author     = {Zhang, Fred and Nanda, Neel},
  month      = oct,
  year       = {2023},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/X7TERYJV/Zhang and Nanda - 2023 - Towards Best Practices of Activation Patching in L.pdf:application/pdf}
}

@misc{weng_transformer_2023,
  title    = {The {Transformer} {Family} {Version} 2.0},
  url      = {https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/},
  language = {en},
  urldate  = {2024-02-17},
  author   = {Weng, Lilian},
  month    = jan,
  year     = {2023},
  note     = {Section: posts},
  file     = {Snapshot:/Users/imran/Zotero/storage/LK7MU8FY/2023-01-27-the-transformer-family-v2.html:text/html}
}

@inproceedings{vaswani_attention_2017,
  title     = {Attention is {All} you {Need}},
  volume    = {30},
  urldate   = {2024-02-17},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  year      = {2017},
  file      = {Full Text PDF:/Users/imran/Zotero/storage/Y3ZZFQG3/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf}
}

@misc{zhou_transformers_2024,
  title     = {Transformers {Can} {Achieve} {Length} {Generalization} {But} {Not} {Robustly}},
  url       = {http://arxiv.org/abs/2402.09371},
  urldate   = {2024-02-17},
  publisher = {arXiv},
  author    = {Zhou, Yongchao and Alon, Uri and Chen, Xinyun and Wang, Xuezhi and Agarwal, Rishabh and Zhou, Denny},
  month     = feb,
  year      = {2024},
  note      = {arXiv:2402.09371 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/TCXLNJX2/Zhou et al. - 2024 - Transformers Can Achieve Length Generalization But.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/7JZCRFYR/2402.html:text/html}
}

@inproceedings{brody_expressivity_2023,
  address   = {Toronto, Canada},
  title     = {On the {Expressivity} {Role} of {LayerNorm} in {Transformers}' {Attention}},
  url       = {https://aclanthology.org/2023.findings-acl.895},
  doi       = {10.18653/v1/2023.findings-acl.895},
  urldate   = {2024-02-17},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
  publisher = {Association for Computational Linguistics},
  author    = {Brody, Shaked and Alon, Uri and Yahav, Eran},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = jul,
  year      = {2023},
  pages     = {14211--14221},
  file      = {Full Text PDF:/Users/imran/Zotero/storage/6XU5AY6G/Brody et al. - 2023 - On the Expressivity Role of LayerNorm in Transform.pdf:application/pdf}
}

@misc{noauthor_write_nodate,
  title   = {Write the {Paper} {First}},
  url     = {https://www.cs.jhu.edu/~jason/advice/write-the-paper-first.html},
  urldate = {2024-02-22},
  file    = {Write the Paper First:/Users/imran/Zotero/storage/6NWXYGVP/write-the-paper-first.html:text/html}
}

@article{kevinrowang_gears-level_nodate,
  title    = {Gears-{Level} {Mental} {Models} of {Transformer} {Interpretability}},
  url      = {https://www.alignmentforum.org/posts/X26ksz4p3wSyycKNB/gears-level-mental-models-of-transformer-interpretability},
  language = {en},
  urldate  = {2024-02-26},
  author   = {KevinRoWang},
  file     = {Snapshot:/Users/imran/Zotero/storage/PKLPXZ75/gears-level-mental-models-of-transformer-interpretability.html:text/html}
}

@misc{nichani_how_2024,
  title     = {How {Transformers} {Learn} {Causal} {Structure} with {Gradient} {Descent}},
  url       = {http://arxiv.org/abs/2402.14735},
  doi       = {10.48550/arXiv.2402.14735},
  urldate   = {2024-02-26},
  publisher = {arXiv},
  author    = {Nichani, Eshaan and Damian, Alex and Lee, Jason D.},
  month     = feb,
  year      = {2024},
  note      = {arXiv:2402.14735 [cs, math, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Theory},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/CP7N87IC/Nichani et al. - 2024 - How Transformers Learn Causal Structure with Gradient Descent.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/AI47H2KU/2402.html:text/html}
}

@inproceedings{geva_transformer_2021,
  address   = {Online and Punta Cana, Dominican Republic},
  title     = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
  url       = {https://aclanthology.org/2021.emnlp-main.446},
  doi       = {10.18653/v1/2021.emnlp-main.446},
  urldate   = {2024-02-26},
  booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  publisher = {Association for Computational Linguistics},
  author    = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  editor    = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
  month     = nov,
  year      = {2021},
  pages     = {5484--5495},
  file      = {Full Text PDF:/Users/imran/Zotero/storage/ASY2K6GZ/Geva et al. - 2021 - Transformer Feed-Forward Layers Are Key-Value Memories.pdf:application/pdf}
}

@inproceedings{quirke_understanding_2023,
  title    = {Understanding {Addition} in {Transformers}},
  url      = {https://openreview.net/forum?id=rIx1YXVWZb},
  language = {en},
  urldate  = {2024-03-11},
  author   = {Quirke, Philip and Barez, Fazl},
  month    = oct,
  year     = {2023},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/NJIS3P76/Quirke and Barez - 2023 - Understanding Addition in Transformers.pdf:application/pdf}
}

@misc{huang_music_2018,
  title     = {Music {Transformer}},
  url       = {http://arxiv.org/abs/1809.04281},
  doi       = {10.48550/arXiv.1809.04281},
  urldate   = {2024-04-03},
  publisher = {arXiv},
  author    = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Monica and Eck, Douglas},
  month     = dec,
  year      = {2018},
  note      = {arXiv:1809.04281 [cs, eess, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, relative positional encoding},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/NXBC33K5/Huang et al. - 2018 - Music Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/L39XF2PN/1809.html:text/html}
}

@inproceedings{shaw_self-attention_2018,
  address   = {New Orleans, Louisiana},
  title     = {Self-{Attention} with {Relative} {Position} {Representations}},
  url       = {https://aclanthology.org/N18-2074},
  doi       = {10.18653/v1/N18-2074},
  urldate   = {2024-04-03},
  booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 2 ({Short} {Papers})},
  publisher = {Association for Computational Linguistics},
  author    = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  editor    = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
  month     = jun,
  year      = {2018},
  keywords  = {relative positional encoding},
  pages     = {464--468},
  file      = {Full Text PDF:/Users/imran/Zotero/storage/MIHGM5NR/Shaw et al. - 2018 - Self-Attention with Relative Position Representations.pdf:application/pdf}
}

@misc{olsson_-context_2022,
  title     = {In-context {Learning} and {Induction} {Heads}},
  url       = {http://arxiv.org/abs/2209.11895},
  doi       = {10.48550/arXiv.2209.11895},
  urldate   = {2024-04-07},
  publisher = {arXiv},
  author    = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  month     = sep,
  year      = {2022},
  note      = {arXiv:2209.11895 [cs]},
  keywords  = {Computer Science - Machine Learning},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/84VXWPJ3/Olsson et al. - 2022 - In-context Learning and Induction Heads.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/943CWW47/2209.html:text/html}
}

@inproceedings{nanda_progress_2022,
  title    = {Progress measures for grokking via mechanistic interpretability},
  url      = {https://openreview.net/forum?id=9XFSbDPmdW},
  language = {en},
  urldate  = {2024-04-09},
  author   = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  month    = sep,
  year     = {2022},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/2YJWPRGB/Nanda et al. - 2022 - Progress measures for grokking via mechanistic interpretability.pdf:application/pdf}
}

@misc{burtsev_memory_2021,
  title     = {Memory {Transformer}},
  url       = {http://arxiv.org/abs/2006.11527},
  doi       = {10.48550/arXiv.2006.11527},
  urldate   = {2024-05-06},
  publisher = {arXiv},
  author    = {Burtsev, Mikhail S. and Kuratov, Yuri and Peganov, Anton and Sapunov, Grigory V.},
  month     = feb,
  year      = {2021},
  note      = {arXiv:2006.11527 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/98WTTXQ6/Burtsev et al. - 2021 - Memory Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/T6LCJEZA/2006.html:text/html}
}

@article{bulatov_recurrent_2022,
  title    = {Recurrent {Memory} {Transformer}},
  volume   = {35},
  language = {en},
  urldate  = {2024-05-06},
  journal  = {Advances in Neural Information Processing Systems},
  author   = {Bulatov, Aydar and Kuratov, Yury and Burtsev, Mikhail},
  month    = dec,
  year     = {2022},
  pages    = {11079--11091},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/8J658HLK/Bulatov et al. - 2022 - Recurrent Memory Transformer.pdf:application/pdf}
}

@inproceedings{darcet_vision_2023,
  title    = {Vision {Transformers} {Need} {Registers}},
  url      = {https://openreview.net/forum?id=2dnO3LLiJ1},
  language = {en},
  urldate  = {2024-05-06},
  author   = {Darcet, Timothée and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  month    = oct,
  year     = {2023},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/6U4TV8FZ/Darcet et al. - 2023 - Vision Transformers Need Registers.pdf:application/pdf}
}

@misc{ferrando_primer_2024,
  title     = {A {Primer} on the {Inner} {Workings} of {Transformer}-based {Language} {Models}},
  url       = {http://arxiv.org/abs/2405.00208},
  doi       = {10.48550/arXiv.2405.00208},
  urldate   = {2024-05-06},
  publisher = {arXiv},
  author    = {Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and Costa-jussà, Marta R.},
  month     = may,
  year      = {2024},
  note      = {arXiv:2405.00208 [cs]},
  keywords  = {Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/PTUTVIW2/Ferrando et al. - 2024 - A Primer on the Inner Workings of Transformer-based Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/4MRV79CY/2405.html:text/html}
}

@article{csordas_systematic_2023,
  title   = {Systematic generalization in connectionist models},
  url     = {https://sonar.ch/global/documents/326205},
  urldate = {2024-05-13},
  author  = {Csordás, Róbert},
  year    = {2023},
  file    = {Available Version (via Google Scholar):/Users/imran/Zotero/storage/HGTMNSJQ/Csordás - 2023 - Systematic generalization in connectionist models.pdf:application/pdf}
}

@misc{ruoss_randomized_2023,
  title     = {Randomized {Positional} {Encodings} {Boost} {Length} {Generalization} of {Transformers}},
  url       = {http://arxiv.org/abs/2305.16843},
  doi       = {10.48550/arXiv.2305.16843},
  urldate   = {2024-05-13},
  publisher = {arXiv},
  author    = {Ruoss, Anian and Delétang, Grégoire and Genewein, Tim and Grau-Moya, Jordi and Csordás, Róbert and Bennani, Mehdi and Legg, Shane and Veness, Joel},
  month     = may,
  year      = {2023},
  note      = {arXiv:2305.16843 [cs, stat]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/RFPSTUBR/Ruoss et al. - 2023 - Randomized Positional Encodings Boost Length Generalization of Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/PUAVCVQJ/2305.html:text/html}
}

@misc{pfau_lets_2024,
  title      = {Let's {Think} {Dot} by {Dot}: {Hidden} {Computation} in {Transformer} {Language} {Models}},
  shorttitle = {Let's {Think} {Dot} by {Dot}},
  url        = {http://arxiv.org/abs/2404.15758},
  doi        = {10.48550/arXiv.2404.15758},
  urldate    = {2024-05-13},
  publisher  = {arXiv},
  author     = {Pfau, Jacob and Merrill, William and Bowman, Samuel R.},
  month      = apr,
  year       = {2024},
  note       = {arXiv:2404.15758 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.6},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/5D8Y9BKR/Pfau et al. - 2024 - Let's Think Dot by Dot Hidden Computation in Transformer Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/WB3ADNXW/2404.html:text/html}
}

@misc{gurnee_finding_2023,
  title      = {Finding {Neurons} in a {Haystack}: {Case} {Studies} with {Sparse} {Probing}},
  shorttitle = {Finding {Neurons} in a {Haystack}},
  url        = {http://arxiv.org/abs/2305.01610},
  doi        = {10.48550/arXiv.2305.01610},
  urldate    = {2024-05-21},
  publisher  = {arXiv},
  author     = {Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
  month      = jun,
  year       = {2023},
  note       = {arXiv:2305.01610 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/5W4DH89V/Gurnee et al. - 2023 - Finding Neurons in a Haystack Case Studies with Sparse Probing.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/97XEDDY7/2305.html:text/html}
}

@misc{hase_unreasonable_2024,
  title     = {The {Unreasonable} {Effectiveness} of {Easy} {Training} {Data} for {Hard} {Tasks}},
  url       = {http://arxiv.org/abs/2401.06751},
  doi       = {10.48550/arXiv.2401.06751},
  urldate   = {2024-05-21},
  publisher = {arXiv},
  author    = {Hase, Peter and Bansal, Mohit and Clark, Peter and Wiegreffe, Sarah},
  month     = jan,
  year      = {2024},
  note      = {arXiv:2401.06751 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/8RMVJWFQ/Hase et al. - 2024 - The Unreasonable Effectiveness of Easy Training Data for Hard Tasks.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/V8I2Q9S4/2401.html:text/html}
}

@inproceedings{hanna_how_2023,
  title      = {How does {GPT}-2 compute greater-than?: {Interpreting} mathematical abilities in a pre-trained language model},
  shorttitle = {How does {GPT}-2 compute greater-than?},
  url        = {https://openreview.net/forum?id=p4PckNQR8k},
  language   = {en},
  urldate    = {2024-05-28},
  author     = {Hanna, Michael and Liu, Ollie and Variengien, Alexandre},
  month      = nov,
  year       = {2023},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/MR93QSSH/Hanna et al. - 2023 - How does GPT-2 compute greater-than Interpreting mathematical abilities in a pre-trained language.pdf:application/pdf}
}

@misc{mcleish_transformers_2024,
  title     = {Transformers {Can} {Do} {Arithmetic} with the {Right} {Embeddings}},
  url       = {http://arxiv.org/abs/2405.17399},
  doi       = {10.48550/arXiv.2405.17399},
  urldate   = {2024-06-02},
  publisher = {arXiv},
  author    = {McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and Goldstein, Tom},
  month     = may,
  year      = {2024},
  note      = {arXiv:2405.17399 [cs]
               version: 1},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, abacus},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/GN7JXQZU/McLeish et al. - 2024 - Transformers Can Do Arithmetic with the Right Embeddings.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/X526D7VQ/2405.html:text/html}
}

@misc{bounsi_transformers_2024,
  title     = {Transformers meet {Neural} {Algorithmic} {Reasoners}},
  url       = {http://arxiv.org/abs/2406.09308},
  doi       = {10.48550/arXiv.2406.09308},
  urldate   = {2024-06-14},
  publisher = {arXiv},
  author    = {Bounsi, Wilfried and Ibarz, Borja and Dudzik, Andrew and Hamrick, Jessica B. and Markeeva, Larisa and Vitvitskyi, Alex and Pascanu, Razvan and Veličković, Petar},
  month     = jun,
  year      = {2024},
  note      = {arXiv:2406.09308 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/KWRGY9NL/Bounsi et al. - 2024 - Transformers meet Neural Algorithmic Reasoners.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/IAHQJDWQ/2406.html:text/html}
}

@misc{li_functional_2024,
  title     = {Functional {Interpolation} for {Relative} {Positions} {Improves} {Long} {Context} {Transformers}},
  url       = {http://arxiv.org/abs/2310.04418},
  language  = {en},
  urldate   = {2024-06-14},
  publisher = {arXiv},
  author    = {Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh},
  month     = mar,
  year      = {2024},
  note      = {arXiv:2310.04418 [cs]},
  keywords  = {Computer Science - Machine Learning},
  file      = {Li et al. - 2024 - Functional Interpolation for Relative Positions Improves Long Context Transformers.pdf:/Users/imran/Zotero/storage/SH4FTD4F/Li et al. - 2024 - Functional Interpolation for Relative Positions Improves Long Context Transformers.pdf:application/pdf}
}

@inproceedings{dave_investigating_2024,
  title   = {Investigating {Symbolic} {Capabilities} of {Large} {Language} {Models}},
  url     = {https://www.semanticscholar.org/paper/Investigating-Symbolic-Capabilities-of-Large-Models-Dave-Kifer/cb1addf9cefe4e96763d28437f72a3d3cbfa7225},
  urldate = {2024-06-15},
  author  = {Dave, Neisarg and Kifer, Daniel and Giles, C. L. and Mali, A.},
  month   = may,
  year    = {2024},
  file    = {Full Text PDF:/Users/imran/Zotero/storage/NSCLGZIZ/Dave et al. - 2024 - Investigating Symbolic Capabilities of Large Language Models.pdf:application/pdf}
}

@article{su_roformer_2024,
  title      = {{RoFormer}: {Enhanced} transformer with {Rotary} {Position} {Embedding}},
  volume     = {568},
  issn       = {0925-2312},
  shorttitle = {{RoFormer}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0925231223011864},
  doi        = {10.1016/j.neucom.2023.127063},
  urldate    = {2024-06-17},
  journal    = {Neurocomputing},
  author     = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  month      = feb,
  year       = {2024},
  keywords   = {Natural language processing, Position information encoding, Pre-trained language models, Pre-training},
  pages      = {127063},
  file       = {ScienceDirect Snapshot:/Users/imran/Zotero/storage/4B4ZQLZX/S0925231223011864.html:text/html;Submitted Version:/Users/imran/Zotero/storage/SY4J7D58/Su et al. - 2024 - RoFormer Enhanced transformer with Rotary Position Embedding.pdf:application/pdf}
}

@misc{treutlein_connecting_2024,
  title      = {Connecting the {Dots}: {LLMs} can {Infer} and {Verbalize} {Latent} {Structure} from {Disparate} {Training} {Data}},
  shorttitle = {Connecting the {Dots}},
  url        = {http://arxiv.org/abs/2406.14546},
  doi        = {10.48550/arXiv.2406.14546},
  urldate    = {2024-06-22},
  publisher  = {arXiv},
  author     = {Treutlein, Johannes and Choi, Dami and Betley, Jan and Anil, Cem and Marks, Samuel and Grosse, Roger Baker and Evans, Owain},
  month      = jun,
  year       = {2024},
  note       = {arXiv:2406.14546 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/M8RT672I/Treutlein et al. - 2024 - Connecting the Dots LLMs can Infer and Verbalize Latent Structure from Disparate Training Data.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/ERUZBBKE/2406.html:text/html}
}

@misc{goyal_think_2024,
  title      = {Think before you speak: {Training} {Language} {Models} {With} {Pause} {Tokens}},
  shorttitle = {Think before you speak},
  url        = {http://arxiv.org/abs/2310.02226},
  doi        = {10.48550/arXiv.2310.02226},
  urldate    = {2024-07-15},
  publisher  = {arXiv},
  author     = {Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh},
  month      = apr,
  year       = {2024},
  note       = {arXiv:2310.02226 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/BCEY9NLB/Goyal et al. - 2024 - Think before you speak Training Language Models With Pause Tokens.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/VFUW5PJ5/2310.html:text/html}
}

@misc{zelikman_quiet-star_2024,
  title      = {Quiet-{STaR}: {Language} {Models} {Can} {Teach} {Themselves} to {Think} {Before} {Speaking}},
  shorttitle = {Quiet-{STaR}},
  url        = {http://arxiv.org/abs/2403.09629},
  doi        = {10.48550/arXiv.2403.09629},
  urldate    = {2024-07-15},
  publisher  = {arXiv},
  author     = {Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D.},
  month      = mar,
  year       = {2024},
  note       = {arXiv:2403.09629 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/73MUYPNX/Zelikman et al. - 2024 - Quiet-STaR Language Models Can Teach Themselves to Think Before Speaking.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/PMCMHD5G/2403.html:text/html}
}

@article{zelikman_star_2022,
  title      = {{STaR}: {Bootstrapping} {Reasoning} {With} {Reasoning}},
  volume     = {35},
  shorttitle = {{STaR}},
  language   = {en},
  urldate    = {2024-07-31},
  journal    = {Advances in Neural Information Processing Systems},
  author     = {Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  month      = dec,
  year       = {2022},
  pages      = {15476--15488},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/Q83E9B8A/Zelikman et al. - 2022 - STaR Bootstrapping Reasoning With Reasoning.pdf:application/pdf}
}

@inproceedings{ahuja_provable_2024,
  title    = {On {Provable} {Length} and {Compositional} {Generalization}},
  url      = {https://openreview.net/forum?id=xuwtmXiHMT},
  language = {en},
  urldate  = {2024-08-05},
  author   = {Ahuja, Kartik and Mansouri, Amin},
  month    = jul,
  year     = {2024},
  file     = {Full Text PDF:/Users/imran/Zotero/storage/GJVWSS5A/Ahuja and Mansouri - 2024 - On Provable Length and Compositional Generalization.pdf:application/pdf}
}

@misc{quirke_increasing_2024,
  title     = {Increasing {Trust} in {Language} {Models} through the {Reuse} of {Verified} {Circuits}},
  url       = {http://arxiv.org/abs/2402.02619},
  doi       = {10.48550/arXiv.2402.02619},
  urldate   = {2024-08-05},
  publisher = {arXiv},
  author    = {Quirke, Philip and Neo, Clement and Barez, Fazl},
  month     = jul,
  year      = {2024},
  note      = {arXiv:2402.02619 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/TWUL6STU/Quirke et al. - 2024 - Increasing Trust in Language Models through the Reuse of Verified Circuits.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/YLWZRACN/2402.html:text/html}
}

@article{gao_going_2024,
  title      = {Going {Beyond} {XAI}: {A} {Systematic} {Survey} for {Explanation}-{Guided} {Learning}},
  volume     = {56},
  issn       = {0360-0300},
  shorttitle = {Going {Beyond} {XAI}},
  url        = {https://dl.acm.org/doi/10.1145/3644073},
  doi        = {10.1145/3644073},
  number     = {7},
  urldate    = {2024-09-03},
  journal    = {ACM Comput. Surv.},
  author     = {Gao, Yuyang and Gu, Siyi and Jiang, Junji and Hong, Sungsoo Ray and Yu, Dazhou and Zhao, Liang},
  month      = apr,
  year       = {2024},
  pages      = {188:1--188:39},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/PTK28IT3/Gao et al. - 2024 - Going Beyond XAI A Systematic Survey for Explanation-Guided Learning.pdf:application/pdf}
}

@misc{yehudai_when_2024,
  title     = {When {Can} {Transformers} {Count} to n?},
  url       = {http://arxiv.org/abs/2407.15160},
  doi       = {10.48550/arXiv.2407.15160},
  urldate   = {2024-09-06},
  publisher = {arXiv},
  author    = {Yehudai, Gilad and Kaplan, Haim and Ghandeharioun, Asma and Geva, Mor and Globerson, Amir},
  month     = jul,
  year      = {2024},
  note      = {arXiv:2407.15160 [cs]
               version: 1},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/P97JW2MA/Yehudai et al. - 2024 - When Can Transformers Count to n.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/FQ26M4RR/2407.html:text/html}
}

@misc{deng_explicit_2024,
  title      = {From {Explicit} {CoT} to {Implicit} {CoT}: {Learning} to {Internalize} {CoT} {Step} by {Step}},
  shorttitle = {From {Explicit} {CoT} to {Implicit} {CoT}},
  url        = {http://arxiv.org/abs/2405.14838},
  doi        = {10.48550/arXiv.2405.14838},
  urldate    = {2024-09-06},
  publisher  = {arXiv},
  author     = {Deng, Yuntian and Choi, Yejin and Shieber, Stuart},
  month      = may,
  year       = {2024},
  note       = {arXiv:2405.14838 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
  file       = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/W9PALGMI/Deng et al. - 2024 - From Explicit CoT to Implicit CoT Learning to Internalize CoT Step by Step.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/YTV4VD2H/2405.html:text/html}
}

@inproceedings{krubinski_basic_2023,
  title    = {Basic {Arithmetic} {Properties} in the {Space} of {Language} {Model} {Prompts}},
  url      = {https://openreview.net/forum?id=RCiRtdERCW},
  language = {en},
  urldate  = {2024-09-09},
  author   = {Krubiński, Mateusz},
  month    = oct,
  year     = {2023},
  file     = {24.pdf:/Users/imran/Zotero/storage/GIP7WIBB/24.pdf:application/pdf;Snapshot:/Users/imran/Zotero/storage/JNWMLC89/forum.html:text/html}
}

@inproceedings{press_measuring_2023,
  address   = {Singapore},
  title     = {Measuring and {Narrowing} the {Compositionality} {Gap} in {Language} {Models}},
  url       = {https://aclanthology.org/2023.findings-emnlp.378},
  doi       = {10.18653/v1/2023.findings-emnlp.378},
  urldate   = {2024-09-09},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
  publisher = {Association for Computational Linguistics},
  author    = {Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah and Lewis, Mike},
  editor    = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  month     = dec,
  year      = {2023},
  pages     = {5687--5711},
  file      = {Full Text PDF:/Users/imran/Zotero/storage/WG8AISKZ/Press et al. - 2023 - Measuring and Narrowing the Compositionality Gap in Language Models.pdf:application/pdf}
}

@article{hupkes_compositionality_2020,
  title      = {Compositionality {Decomposed}: {How} do {Neural} {Networks} {Generalise}?},
  volume     = {67},
  copyright  = {Copyright (c)},
  issn       = {1076-9757},
  shorttitle = {Compositionality {Decomposed}},
  url        = {https://jair.org/index.php/jair/article/view/11674},
  doi        = {10.1613/jair.1.11674},
  language   = {en},
  urldate    = {2024-09-10},
  journal    = {Journal of Artificial Intelligence Research},
  author     = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  month      = apr,
  year       = {2020},
  keywords   = {machine learning, natural language, neural networks, rule learning},
  pages      = {757--795},
  file       = {Full Text PDF:/Users/imran/Zotero/storage/47YV5B8P/Hupkes et al. - 2020 - Compositionality Decomposed How do Neural Networks Generalise.pdf:application/pdf}
}

@misc{li_chain_2024,
  title     = {Chain of {Thought} {Empowers} {Transformers} to {Solve} {Inherently} {Serial} {Problems}},
  url       = {http://arxiv.org/abs/2402.12875},
  doi       = {10.48550/arXiv.2402.12875},
  urldate   = {2024-09-16},
  publisher = {arXiv},
  author    = {Li, Zhiyuan and Liu, Hong and Zhou, Denny and Ma, Tengyu},
  month     = may,
  year      = {2024},
  note      = {arXiv:2402.12875 [cs, stat]},
  keywords  = {Computer Science - Computational Complexity, Computer Science - Machine Learning, Statistics - Machine Learning},
  file      = {arXiv Fulltext PDF:/Users/imran/Zotero/storage/MDST3MMG/Li et al. - 2024 - Chain of Thought Empowers Transformers to Solve Inherently Serial Problems.pdf:application/pdf;arXiv.org Snapshot:/Users/imran/Zotero/storage/JFB64RP5/2402.html:text/html}
}
