\chapter{Background}\label{background}

\TODO{lay out how these all are connected}

\section{Transformer}\label{sec:transformer_arch}

\paragraph{Encoder-Decoder}
\paragraph{Encoder-only}
\paragraph{Decoder-only}


\subsection{Positional Encoding Schemes}\label{subsec:positional_encoding}



\subsection{Training and Inference}\label{subsec:training_inference}


\section{Mechanistic Interpretability}\label{sec:mech_interp}

Residual stream, activation patching, logitlens, etc.

\section{Realizability?}\label{sec:realizability}

Complexity classes? RASP?
- RASP-L proves that transformers can learn addition (expand more, paper WHAT ALGORITHMS CAN TRANSFORMERS LEARN)

\section{Deep double descent}\label{sec:interpretability}