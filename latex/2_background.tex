\chapter{Background}\label{background}

\TODO{rewrite}

This chapter provides an overview of the Transformer architecture, focusing on its core components, different variants, positional encoding schemes, and the processes of training and inference. It also discusses mechanistic interpretability and the deep double descent phenomenon.

\section*{Notation}\label{sec:notation}

\begin{center}
    \begin{tabular}{cl}
        $\mathcal{V}$ \qquad & Vocabulary (set) of input tokens.                                           \\
        $n$                  & Length of the input sequence.                                               \\
        $b$                  & Batch size for batched input sequences.                                     \\
        $\mathbf{x}$         & Input sequence of tokens, $\mathbf{x} \in \mathcal{V}^{n}$                  \\
        $x_i$                & $i$-th token in the input sequence, $x_i \in \mathcal{V}$.                  \\
        $d$                  & Dimensionality of the token embedding space.                                \\
        $E_i$                & $d$-dimensional embedding vector of token $x_i$, $E_i \in \mathbb{R}^d$.    \\
        $E$                  & Embedding matrix, $E \in \mathbb{R}^{|\mathcal{V}| \times d}$.              \\
        $H$                  & Matrix of a sequence of embedding vectors, $H \in \mathbb{R}^{n \times d}$. \\
        $H_i$                & $i$-th vector in the sequence of embeddings, $H_i \in \mathbb{R}^d$.        \\
        $O$                  & Output matrix from a Transformer layer, $O \in \mathbb{R}^{n \times d}$.    \\
        $Q$                  & Queries matrix, $Q \in \mathbb{R}^{n \times d}$.                            \\
        $K$                  & Keys matrix, $K \in \mathbb{R}^{n \times d}$.                               \\
        $V$                  & Values matrix, $V \in \mathbb{R}^{n \times d}$.                             \\
        $\theta$             & Model parameters (weights and biases).                                      \\
    \end{tabular}
\end{center}

\section{Transformer}\label{sec:transformer_arch}

The Transformer architecture, introduced by \textcite{vaswani_attention_2017}, performs sequence modeling by relying entirely on self-attention mechanism, instead of using convolution or recurrence.

Let $\mathcal{V}$ denote the vocabulary of input tokens. While the tokens can represent anything, in language modeling tasks they are usually learned subword units. In this work, however, a simple character tokenization scheme is used that is suitable for algorithmic tasks, so each token corresponds to a single character (letter, digit or symbol) in the sequence. An input sequence is represented as $\mathbf{x} = [x_1, x_2, \dots, x_n]$, where $x_i \in \mathcal{V}$ and $n$ is the sequence length. Each token $x_i$ is mapped to a $d$-dimensional embedding vector $E_i \in \mathbb{R}^d$ using an embedding matrix $E \in \mathbb{R}^{|\mathcal{V}| \times d}$. Thus, each row of $E$ corresponds to the embedding of a token in the vocabulary.

The input matrix to a Transformer layer is a sequence of vector embeddings (also called the \emph{latent} or \emph{hidden representation} for intermediate layer inputs), denoted $H \in \mathbb{R}^{n \times d}$, where $H = [H_1^\top, H_2^\top, \dots, H_n^\top]^\top$. The output of a Transformer layer is also a sequence of vectors with the same sequence length, denoted $O \in \mathbb{R}^{n \times d}$.

In practice, the inputs to the Transformer are batched, so the input has an additional dimension for the batch size, denoted $b$, with $H \in \mathbb{R}^{b \times n \times d}$. This results in the first (batch) dimension being added throughout the intermediate representation and the output, but has no bearing on the description of the Transformer model.

\subsection{Elements}\label{subsec:elements_transformers}

The Transformer is composed of several key components to model dependencies in sequential data: multi-head attention, feed-forward networks, layer normalization, and residual connections. Informally, the attention mechanism transfers information \emph{between} tokens, while the feed-forward networks process information \emph{within} tokens. These components are stacked in each layer of the Transformer as shown in Figure \ref{fig:transformer_layer} (a).

\paragraph{Token Embeddings}
The discrete input tokens are first converted into continuous embeddings using an embedding matrix $E \in \mathbb{R}^{|\mathcal{V}| \times d}$, where $|\mathcal{V}|$ is the size of the vocabulary and $d$ is the embedding dimension. The embedding matrix is learned during training, and the embeddings are used as input to the Transformer. After applying the Transformer layers, the output embeddings are passed through a linear \emph{unembedding} layer to get the \emph{logits} (unnormalized log-probabilities) for the next token.

\paragraph{Attention Mechanism}
The attention mechanism \parencite{bahdanau_neural_2014} allows the model to weigh the relevance of different positions in the input sequence. For this purpose, it computes \emph{queries}, \emph{keys}, and \emph{values} from the input embeddings and uses them to calculate attention scores. The output is a weighted sum of the values, where the weights are determined by the attention scores. The names ``queries'', ``keys'', and ``values'' are derived from the context of information retrieval, where the queries are the elements being searched for, the keys are the elements being searched, and the values are the elements being retrieved. In the context of the Transformer, intuitively, the query represents what the current token is ``looking for'' in the sequence, the keys represent what the token at a given position ``offers'' to the current token, and the values are the actual information that the current token ``receives'' from the other tokens.

First, the queries, keys, and values are computed as:
\begin{align*}
    Q & = H W^Q, \\
    K & = H W^K, \\
    V & = H W^V,
\end{align*}
where $W^Q, W^K, W^V \in \mathbb{R}^{d \times d_k}$ are the weight matrices, and $d_k$ is the dimension of the queries, keys, and values. In practice, the convention is to set $d_k = d$, which is the case for the models in this work. Thus, $Q, K, V \in \mathbb{R}^{n \times d}$.

Given queries $Q$, keys $K$, and values $V$, the attention output is computed as:
\begin{equation*}
    O_{att} = \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^\top}{\sqrt{d}} \right) V.
\end{equation*}
where the softmax function is applied along the last dimension, and is defined as:
\begin{equation*}
    \text{softmax}(\mathbf{x})_i = \frac{e^{x_i}}{\sum_j e^{x_j}}.
\end{equation*}

Note that the output of the attention mechanism has the same shape as the input, $O_{att} \in \mathbb{R}^{n \times d}$.

\paragraph{Multi-Head Attention}
Multi-head attention extends the attention mechanism with multiple independent \emph{heads} to allow the model to focus on information from different representation subspaces. So, instead of applying attention to the $d$-dimensional queries, keys, and values directly, they are projected into $h$ different $d_{head}$-dimensional subspaces, where $h$ is the number of heads. In this work, the head dimension $d_{head}$ is set to $d/h$, so that the total dimensionality remains $d$. The outputs the heads are concatenated and linearly transformed to the original dimensionality:
\begin{equation*}
    \text{MultiHeadAttention}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h) W^O,
\end{equation*}
where $W^O \in \mathbb{R}^{d \times d}$ is the learned output weight matrix, and each head is computed as:
\begin{equation*}
    \text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V),
\end{equation*}
and $W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d \times d_{head}}$ are the weight matrices for the $i$-th head.

\paragraph{Feed-Forward Networks}

Position-wise feed-forward networks (FFN), also called the Multi-layer Perceptron (MLP), are applied independently to each position in the sequence:
\begin{equation*}
    \text{FFN}(H_i) = \sigma(H_i W_1 + \mathbf{b}_1) W_2 + \mathbf{b}_2,
\end{equation*}
where $H_i \in \mathbb{R}^d$ is the input vector, $W_1 \in \mathbb{R}^{d \times d_{\text{ff}}}$, $W_2 \in \mathbb{R}^{d_{\text{ff}} \times d}$, and $\sigma$ is an activation function such as Rectified Linear Unit (ReLU). The intermediate dimensionality $d_{\text{ff}}$ is usually set to $4d$ in the original Transformer model and in all experiments presented in this work.

\paragraph{Layer Normalization}
Layer normalization \parencite{ba_layer_2016} is applied after each sub-layer over the last (feature) dimension. The $\text{LayerNorm}$ function for a vector $v \in \mathbb{R}^d$ is defined as:
\begin{equation*}
    \text{LayerNorm}(v) = \frac{v - \mu}{\sigma}\gamma + \beta,
\end{equation*}
where the scale $\gamma$ and bias vector $\beta$ are learned scaling and shifting parameters, and $\mu$ and $\sigma$ are the mean and standard deviation of $v$,computed as follows:
\begin{align*}
    \mu    & = \frac{1}{d} \sum_{i=1}^{d} v_i,                  \\
    \sigma & = \sqrt{\frac{1}{d} \sum_{i=1}^{d} (v_i - \mu)^2}.
\end{align*}

\paragraph{Residual Connections}
Residual connections \parencite{he_deep_2016} are usually applied to ease gradient flow and enable the training of deeper networks. In Transformer models, the residual connections are applied after each sub-layer (self-attention and MLP), followed by a layer normalization. Thus, the output of each sub-layer is:
\begin{equation*}
    \text{SubLayerOutput} = \text{LayerNorm}(\mathbf{x} + \text{SubLayer}(\mathbf{x})).
\end{equation*}

The residual connections-based view of the model also enables the concept of a \emph{residual stream}, which is important in mechanistic interpretability. In this alternative view of the model, the residual connections are the main backbone of information flow through the model, with the sub-layers processing the hidden representation tensor $H$ and adding it back to the residual stream.

\paragraph{Block Structure}
The original Transformer introduced in \cite{vaswani_attention_2017} consists of stacked encoder and decoder blocks, each containing multi-head attention and feed-forward networks, along with residual connections and layer normalization. However, different modified architectures have been proposed, such as the encoder-only BERT model \parencite{devlin_bert_2019} and the decoder-only GPT model \parencite{radford_improving_2018}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/transformer_layer.pdf}
    \caption{(a) A single Transformer layer, consisting of multi-head self-attention, feed-forward network (MLP), and layer normalization. (b) A decoder-only transformer model. In the decoder, the self-attention mechanism has a causal mask to prevent attending to future tokens.}
    \label{fig:transformer_layer}
\end{figure}


\subsection{Encoder and Decoder Architectures}\label{subsec:types_transformers}

In this section, different Transformer architectures are summarized: encoder-decoder, encoder-only, and decoder-only models. The original Transformer~\parencite{vaswani_attention_2017} employs an encoder-decoder structure, where the encoder transforms input sequences into continuous representations, and the decoder generates output sequences based on these representations and previously generated tokens. Encoder-only models like BERT~\parencite{devlin_bert_2019} focus solely on encoding input sequences into contextual embeddings, making them well-suited for understanding tasks such as text classification and question answering. Decoder-only models, like GPT~\parencite{radford_improving_2018}, generate sequences by predicting the next token based on prior tokens, primarily used for text generation. While both encoder-decoder and decoder-only architectures can perform autoregressive sequence generation, decoder-only models are the focus of this work.

\paragraph{Encoder-Decoder}
The original Transformer model introduced by Vaswani et al.~\cite{vaswani_attention_2017} employs an encoder-decoder architecture. In this architecture, the encoder processes an input sequence $\mathbf{x} = (x_1, x_2, \dots, x_n)$ into a sequence of continuous representations $\mathbf{z} = (z_1, z_2, \dots, z_n)$. The decoder then generates an output sequence $\mathbf{y} = (y_1, y_2, \dots, y_m)$ by predicting the next token $y_t$ based on the encoder's output and the previously generated tokens.

The encoder consists of a stack of $N$ identical layers, each containing two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The decoder has a similar structure but includes a third sub-layer: the \emph{cross-attention}, also called \emph{encoder-decoder attention}, where the queries come from the previous decoder layer, and the keys and values come from the output of the encoder. Hence, the difference between the self-attention and cross-attention mechanisms is that self-attention is usually applied to the same sequence, while cross-attention is applied between two different sequences (e.g. one from the encoder, and one from the decoder). Moreover, the self-attention mechanism in the decoder has a causal mask to prevent attending to ``future tokens'', ensuring output is generated autoregressively. The T5 model \parencite{raffel_exploring_2020} is an example of a large encoder-decoder Transformer model capable of performing many NLP tasks such as translation, summarization, and question answering.

\paragraph{Encoder-only}
Encoder-only models focus exclusively on encoding the input sequence into a contextual representation without a decoder component. BERT (Bidirectional Encoder Representations from Transformers)~\cite{devlin_bert_2019} is a prominent example of this architecture. BERT utilizes a stack of Transformer encoder layers to produce deep bidirectional representations by jointly conditioning on both left and right context. This makes encoder-only models particularly well-suited for tasks that require a comprehensive understanding of the input, such as text classification, named entity recognition, and question answering.

These models are typically pre-trained on large unlabeled text corpora using self-supervised objectives like masked language modeling (MLM) and next sentence prediction (NSP). The pre-trained models can then be fine-tuned on specific downstream tasks.

\paragraph{Decoder-only}
Decoder-only models generate sequences based on prior tokens and are designed primarily for autoregressive language modeling and text generation tasks. GPT (Generative Pre-trained Transformer)~\cite{radford_improving_2018} is a canonical example of a decoder-only architecture. In these models, the Transformer decoder predicts the next token in a sequence by attending to the previous tokens without an encoder component. The encoder is not needed since in a decoder-only Transformer, the input sequence $\mathbf{x}$ is prepended to the decoder input sequence $\mathbf{y}$, and only passed through the decoder layers to autoregressively generate the output sequence. Recent research on language modeling has mostly focused decoder-only models, since they can also be used on other language tasks through prompting, few-shot learning, and fine-tuning. In particular, majority of the latest state-of-the-art large language models (LLMs) are decoder-only transformers pre-trained on large text corpora.

\paragraph{Encoder-Decoder vs. Decoder-Only} Both model types are capable of autoregressive sequence generation and can be used for a wide range of tasks. The decoder-only models are favored in recent works due to them being simpler and having less inductive bias. However, encoder-decoder models are still widely used in machine translation, robotics, and multi-modal learning tasks. The additional structure in encoder-decoder models, as compared to using a decoder-only model with would-be encoder input sequence prepended to the decoder's input can be summarized as:
\begin{itemize}
    \item The input to the encoder passes through more layers (encoder layers) before reaching the decoder.
    \item It is assumed that input and output sequences are sufficiently different to justify using separate parameters for them (encoder and decoder).
\end{itemize}

With large language models and massive datasets, the difference between the two architecture becomes less relevant. In summary, the choice of Transformer architecture depends on the specific requirements of the task, though decoder-only models have been more widely used in recent research and are the focus of this work.

\subsection{Recurrent and Looping Architectures}\label{subsec:recurrent_looping}

Multiple Transformer extensions have been proposed that incorporate iterative application of weight-sharing layers, particularly suitable for algorithmic tasks that require reasoning over sequences. The Universal Transformer and Looped Transformer are two such examples that introduce recurrence into the Transformer architecture. There are other modifications the Transformer architecture, a few examples of which include the Memory Transformer \parencite{burtsev_memory_2021}, Recurrent Memory Transformer \parencite{bulatov_recurrent_2022}, and Neural Data Router \parencite{csordas_neural_2021}, but these architectures are not widely adopted in pretrained language models, and are not tested in this work.

\paragraph{Universal Transformer}
The Universal Transformer \parencite{dehghani_universal_2018} introduces recurrence into the Transformer architecture by repeatedly applying the same transformer layers multiple times, in both encoder and decoder parts:
\begin{equation*}
    H^{(t+1)} = \text{TransformerLayer}(H^{(t)}),
\end{equation*}
where $t$ denotes the iteration step. An adaptive computation time (ACT) mechanism is used to dynamically adjust the number of iterations based on the input sequence. The Universal Transformer has been shown to achieve better performance on algorithmic tasks compared to the original Transformer. It is also an interesting extension of the Transformer architecture from theoretical point of view, since it makes the model Turing-complete under certain conditions \parencite{dehghani_universal_2018}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/looped_transformer.pdf}
    \caption{(a) A block of multiple transformer layers used in the looped transformer. (b) Looped decoder-only transformer architecture. The input injection mechanism adds skip connections (arrow around blocks) from the original input sequence to the input of each block.}
    \label{fig:looped_transformer}
\end{figure}

\paragraph{Looped Transformer}
Similar to the Universal Transformer, the Looped Transformer \parencite{yang_looped_2023} extends the Transformer by incorporating iterative application of a block of transformer layers. This modification has been shown to perform better than the original, non-recurrent Transformer on algorithmic tasks \parencite{csordas_systematic_2023,yang_looped_2023}. In particular, looped transformer achieves better length generalization on the binary addition task as shown by \cite{fan_looped_2024}. The looped decoder-only transformer architecture is illustrated in Figure \ref{fig:looped_transformer}. However, unlike the Universal Transformer, the Looped Transformer is not necessarily encoder-decoder, and might not use the adaptive computation time mechanism.

\subsection{Positional Encoding Schemes}\label{subsec:positional_encoding}

Since the Transformer lacks inherent sequential order, positional encodings are added to input embeddings to provide position information. Though, several recent works have shown that the causal attention mechanism in a decoder-only transformer can also implicitly learn to encode positional information in the absence of explicit positional encodings \parencite{haviv_transformer_2022,zuo_breaking_2024,zhou_transformers_2024}. Positional encoding methods are also crucial for algorithmic tasks, especially multi-digit integer addition \parencite{shen_positional_2023,kazemnejad_impact_2023,ruoss_randomized_2023}

\paragraph{Absolute Positional Encoding}\label{subsec:absolute_pos_enc}
The original Transformer \parencite{vaswani_attention_2017} uses additive vectors of same dimensionality as the embeddings to encode the \emph{absolute} positions of the tokens. These vectors could be learned (from a random initialization), or \emph{sinusoidal}, where the latter are defined as:
\begin{align*}
    \mathbf{p}_{i,2k}   & = \sin\left( \frac{i}{10000^{2k/d}} \right), \\
    \mathbf{p}_{i,2k+1} & = \cos\left( \frac{i}{10000^{2k/d}} \right),
\end{align*}
for position in the sequence $i$ and dimension $k$. In \cite{vaswani_attention_2017}, the performance difference between learned and sinusoidal positional encodings was found to be insignificant, and the use of sinusoidal encodings is justified by possibility of generalization to longer sequences. In practice, however, absolute positional encodings do not generalize well to sequences longer than the ones seen during training \parencite{press_train_2021}.

\paragraph{Randomized Positional Encodings}\label{subsec:random_pos_enc}
Randomized positional encodings \cite{ruoss_randomized_2023} aim to improve length generalization by simulating longer sequences during training. Similarly to absolute positional encodings, they are added to the input embeddings, and have separate vectors for each position in the sequence. However, instead of using sequential positions $1, 2, \dots, n$, the positions are randomly sampled (keeping order) from a range $[1, n_{\text{max}}]$, where $n_{\text{max}}$ is the maximum sequence length. Thus, the model is exposed to a wider range of positional encodings during training, which helps improve generalization to longer sequences. A related method is to randomly insert spaces between tokens in the input sequence (e.g. \texttt{123 + 456} might become \texttt{12 3  + 45 6}), which disrupts the model's dependence on absolute position information and encourages it to learn more robust representations \parencite{shen_positional_2023}. It is important to distinguish this method from the one introduced by \cite{shen_positional_2023} named Random Embedding, where a random Gaussian ``tag'' is added to a subset of embedding dimensions.

\paragraph{Relative Position Encoding}\label{subsec:relative_pos_enc}
Relative position representations \parencite{shaw_self-attention_2018} encode the relative distances between sequence elements directly into the attention mechanism. In RPE, a vector $\mathbf{a}_{i, j} \in \mathbb{R}^d$ is learned for each pair of positions $(i, j)$, and added to the keys before computing the attention scores:
\begin{equation*}
    A_{ij} = \frac{\mathbf{q}_i (\mathbf{k}_i + \mathbf{a}_{ij}^K)^\top}{\sqrt{d}}
\end{equation*}
where $\mathbf{q}_i$ and $\mathbf{k}_j$ are the query and key vectors for positions $i$ and $j$, respectively. Relative position encodings have been shown to improve performance on tasks requiring long-range dependencies \parencite{shaw_self-attention_2018}.

\paragraph{Attention with Linear Biases (ALiBi)}\label{subsec:alibi}
ALiBi~\parencite{press_train_2021} introduces a linear bias to the attention scores based on the relative positions. For this additive method, the computation of the (pre-softmax) attention logits is modified as:
\begin{equation*}
    A_{\text{ALiBi}}(X) = Q K^\top + B,
\end{equation*}
where the bias matrix $B \in \mathbb{R}^{n \times n}$ is computed by the position encoding function $b : \mathbb{N}^2 \to \mathbb{R}$, such that the $(i,j)$-th entry of $B$ is $b(i,j)$. The bias function for the relative position encoding is defined as:
\begin{equation*}
    b(i,j) = -r|i - j|
\end{equation*}
where the $r$ is a fixed slope pre-computed for each head.

\paragraph{Rotary Position Encoding (RoPE)}\label{subsec:rope}
RoPE \cite{su_roformer_2024} encodes positions using rotations of the query and key vectors with an angle proportional to their absolute positions before the dot product attention, which results in attention being a function of the relative distance between the tokens, capturing the relative positional information. It is a non-additive relative positional encoding. Works such as \cite{press_train_2021,kazemnejad_impact_2023} show that RoPE also has poor length generalization in addition tasks.

\subsection{Training and Inference}\label{subsec:training_inference}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/training_and_inference.pdf}
    \caption{Training and inference setup for Transformer models on the arithmetic task. During training (subfigure a), the padded input batch is passed through the model, and the loss is computed by comparing the predicted logits with the target sequence (\emph{teacher forcing}). Unlike regular unsupervised learning, the loss from non-answer tokens is masked out in experiments. During inference (subfigure b), the model generates outputs by greedily sampling tokens one by one until maximum output length is reached, or the end-of-sequence token \texttt{\$} is generated.}
    \label{fig:transformer_training_inference}
\end{figure}

\paragraph{Training}
Training involves minimizing a loss function, typically the \emph{cross-entropy loss} for language tasks, using an optimization algorithm like Stochastic Gradient Descent (SGD) or AdamW~\parencite{loshchilov_decoupled_2018}. The model learns the parameters $\theta$ by backpropagating the loss through the network layers. The cross-entropy loss is defined as:
\begin{equation*}
    \mathcal{L} = -\frac{1}{b} \sum_{i=1}^{b} \sum_{j=1}^{n} \log p(x_{i,j} | x_{i,<j}),
\end{equation*}
where $b$ is the batch size, $n$ is the sequence length, and $p(x_{i,j} | x_{i,<j})$ is the probability of token $x_{i,j}$ given the previous tokens $x_{i,<j}$.

\paragraph{Answer Loss Masking}
For tasks like integer addition, the loss is modified to focus only on the answer tokens, where a binary mask \( m_{i,j} \in \{0, 1\} \) is applied to denote whether a token at position \( j \) in sequence \( i \) corresponds to an answer. The loss becomes:

\begin{equation*}
    \mathcal{L}_{\text{answer-only}} = -\frac{1}{b} \sum_{i=1}^{b} \sum_{j=1}^{n} m_{i,j} \log p(x_{i,j} | x_{i,<j}),
\end{equation*}

where $b$ is the batch size, $n$ is the sequence length, $m_{i,j} = 1$ if token \( x_{i,j} \) is an answer token, otherwise \( m_{i,j} = 0 \), and \( p(x_{i,j} | x_{i,<j}) \) is the predicted probability of token \( x_{i,j} \) given the preceding tokens \( x_{i,<j} \).

This formulation ensures that the loss is computed only over the answer tokens, with non-answer tokens effectively ignored (since \( m_{i,j} = 0 \) for those positions). Without answer loss masking, the cross-entropy loss would penalize the model for incorrectly predicting randomly generated operands as well (which are not possible to predict in the first place), thus adding noise to the training process never reaching 0 loss.

\paragraph{Inference}
During inference, the trained model generates outputs by iteratively computing the forward pass through the network and adding the sampled tokens to the prompt sequence each time. In autoregressive models, tokens are generated one by one, conditioned on previous outputs. The token at each step is selected using \emph{greedy sampling}, where:
\begin{equation*}
    x_j = \arg\max_{x \in \mathcal{V}} p_{\theta}(x | x_{<j}),
\end{equation*}
where $\mathcal{V}$ is the vocabulary, $p_{\theta}(x | x_{<j})$ are the logits computed by the model, and $\text{softmax}$ is applied to obtain the probabilities from non-normalized logits. The input sequence for the next step is updated by appending the predicted token to the previous sequence:
\begin{equation*}
    x^{t+1} = [x^t, x_j].
\end{equation*}
The sampling process continues until a predefined stopping criterion is met, such as reaching a maximum sequence length or generating a special end-of-sequence token. Greedy sampling is computationally efficient but does not explore alternative sequences. However, it is sufficient for tasks like integer addition, where the output is deterministic.


\section{Mechanistic Interpretability}\label{sec:mech_interp}

Understanding how Transformers make decisions is crucial for interpretability and debugging. Relevant concepts include the \emph{residual stream}, \emph{activation patching}, and the \emph{logit lens}, which provide insights into the model's internal representations and decision-making processes.

\paragraph{Residual Stream}
The residual stream in a Transformer is enabled by the residual connections around operations (as described in Subsection \ref{subsec:elements_transformers}) and serves as a shared memory, accumulating information across layers via residual connections. Both attention heads and feedforward layers read from and write to this stream, which ensures the propagation of information throughout the model. Since each layer can only read from earlier layers, analyzing this stream is essential for understanding how information is transformed and stored across layers \parencite{elhage2021mathematical}. The residual stream can be studied by techniques such as activation patching to trace information flow.

\paragraph{Circuits}
Circuits in Transformers are a set of elements (i.e. attention heads, feed-forward layers) that are responsible for specific behaviors. Currently, while multiple methods exist that attempt to discover circuits, there is no structured way to identify all circuits of interest, nor is it certain that human-interpretable circuits exist in the first place in any given model \parencite{ferrando_primer_2024}. Some works succeeded in identifying specific high-level circuits such as ones performing modular addition \parencite{nanda_progress_2022,zhong_clock_2023} and indirect object identification \parencite{wang_interpretability_2022}. Discovering and explaining circuits is helpful for learning algorithmic tasks and compositionality, like multi-digit integer addition. For instance, a number of circuits might emerge in the model that perform the sub-tasks for integer addition (such as digit-wise modular addition, and carry propagation), and the answer might be generated by composing their outputs.

\paragraph{Activation Patching}
Activation patching \parencite{zhang_towards_2023} is a causal analysis technique where activations from a run of the model are replaced by activations from a different context, allowing to observe how this change affects the model's predictions. By selectively patching different layers or tokens, it becomes possible to identify which parts of the model are responsible for specific behaviors, such as copying repeated sequences or in-context learning. For instance, patching activations in the residual stream can reveal the contribution of circuits like \emph{induction heads}, which are attention heads that predict repeated sequences \parencite{olsson_-context_2022}.

\paragraph{Logit Lens}
The logit lens method \parencite{nostalgebraist_interpreting_2020} involves inspecting the logits (pre-softmax outputs) of the model at various layers to interpret intermediate representations. By projecting the activations of each layer onto the output logits, it allows the predictions to be tracked as they are refined through the layers of the network.

\section{Realizability}\label{sec:realizability}

In the context of Transformers, realizability refers to whether a given task or behavior can be reliably implemented using the model's learned representations. One framework for analyzing this is \emph{RASP}, a restricted-access sequence processing language \parencite{weiss_thinking_2021} that mimics the operations of transformers in a more interpretable manner. Moreover, \cite{fan_looped_2024} show using RASP that looped transformers can generalize to longer sequence lengths for binary addition task, along with other algorithmic tasks.

A key aspect of Transformers is their ability to generalize compositionally in some tasks, as highlighted by \cite{hupkes_compositionality_2020}. This suggests that, under the right conditions, transformers are capable of learning systematic recombination of components, enabling them to generalize beyond seen examples in a structured manner. This compositional behavior is crucial for algorithmic tasks, where the model must apply learned rules consistently across different sequence lengths. However, it is not clear how it can be proven that a Transformer model of specific architecture and size can learn a particular task, especially for complex tasks like multi-digit integer addition.

\section{Deep Double Descent}\label{sec:deep_double_descent}

The phenomenon of \emph{deep double descent} describes how increasing model capacity or training duration can initially lead to worse generalization before ultimately improving it. This was initially believed to challenge the traditional bias-variance tradeoff, where increasing model complexity was believed to always result in overfitting \parencite{belkin_reconciling_2019, nakkiran_deep_2021}. Deep double descent arises when models, especially overparameterized ones, first memorize the training data, but later discover more generalizable features, leading to improved performance on the test set and zero training loss.

This phenomenon is still relevant in integer addition tasks, since the interpolation threshold â€” the point where the model transitions from memorization to generalization might change with respect to model and dataset size.
