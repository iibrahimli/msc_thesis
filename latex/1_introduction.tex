\chapter{Introduction}\label{introduction}

\section{Motivation}

Length generalization in sequence modeling tasks is a fundamental challenge in deep learning, particularly in algorithmic problems such as integer addition. While humans can effortlessly generalize addition to numbers of arbitrary length, transformer models often struggle with out-of-distribution (OOD) generalization to sequences longer than those seen during training. Understanding the underlying reasons for this limitation is crucial for advancing the capabilities of neural networks in algorithmic reasoning and compositional generalization.

\section{Problem Statement}

Despite the success of transformers in various domains, their ability to perform multi-digit integer addition with length generalization remains limited. Prior work suggests that positional encoding schemes play a significant role in this failure. Standard absolute positional encodings do not provide the necessary alignment cues for the model to correctly match digits by their place value, leading to errors when extrapolating to longer sequences. Alternative methods, such as the Abacus positional encoding, have been proposed but often involve task-specific modifications that may not generalize to other problems.

\section{Research Questions}

The primary goal of this thesis is to investigate the reasons behind the failure of transformer models to generalize integer addition to longer sequences and to understand how different positional encoding schemes and data formatting strategies impact this ability. Specifically, we aim to address the following research questions:

\begin{itemize}
    \item \textbf{Why} do transformer models with standard absolute positional encodings fail to generalize integer addition to sequences longer than those seen during training?
    \item \textbf{How} do different positional encoding schemes affect the model's ability to perform addition correctly on longer sequences?
    \item \textbf{How} does the inclusion of sub-task data (e.g., carry detection, digit alignment) influence the model's compositional learning and generalization capabilities?
    \item \textbf{How} can mechanistic interpretability techniques be applied to understand the internal representations and failure modes of transformer models in the context of integer addition?
\end{itemize}

By exploring these questions, we aim to gain insights into the limitations of current transformer architectures and positional encoding methods, and to identify principles that could lead to improved generalization in algorithmic tasks.