\chapter{Introduction}\label{introduction}

\section{Motivation}

Robust length generalization in sequence modeling tasks remains a significant challenge in deep learning, particularly for algorithmic problems that require precise manipulation of sequential data. Integer addition serves as a canonical example of such a task, where the ability to add numbers of arbitrary length is trivial for humans but non-trivial for neural networks. The transformer architecture \cite{vaswani_attention_2017}, known for its success in natural language processing, computer vision, and other domains, often fails to learn generalizable algorithms that also correctly process sequences longer than those encountered during training. This limitation highlights a fundamental gap in our understanding of how neural networks learn and represent algorithmic processes. Understanding the mechanisms underlying this failure is essential and has broader implications for developing neural networks capable of robust algorithmic reasoning in various domains.

Large language models (LLMs) exhibit impressive emergent capabilites and state-of-the-art performance on many benchmarks, but nonetheless struggle with algorithmic tasks that require compositional reasoning and precise manipulation of structured data. Even small transformer models trained from scratch on a specific task share this failure mode. It is hypothesized that the limitation stems from models struggling to effectively use \emph{position-based addressing} to focus on structure of the sequence, in contrast to their strength in \emph{content-based addressing} as seen in natural language processing tasks \parencite{ebrahimi_your_2024}. In line with this, recent studies have pointed towards positional encoding as a key factor influencing length generalization in transformers \cite{mcleish_transformers_2024,zhou_transformers_2024}. Alternative positional encoding methods, such as the Abacus positional encoding \cite{mcleish_transformers_2024}, have shown improved generalization but often involve task-specific modifications that may not extend to other algorithmic problems. This raises questions about the universality and flexibility of currently used positional encoding schemes and especially their impact on algorithmic task performance. Furthermore, despite many research efforts directed towards novel architectures and positional encoding methods, the underlying reasons for the failure of standard transformers in algorithmic tasks remain underexplored in the literature.

\section{Problem Statement}

Despite the widespread success of transformer models in various domains, their ability to perform multi-digit integer addition with length generalization remains limited. Standard absolute positional encodings, integral to the transformer architecture, fail to provide the necessary alignment cues for correctly matching digits by their place value.

The core issue lies in the transformer's difficulty in aligning digits of different operands based on their positional significance across different sequence lengths. In multi-digit addition, each digit must be correctly matched with its corresponding digit in the other operand, and the positional encoding must facilitate this alignment. Without correct position-based retrieval of the correct digit from the sequence, the model cannot learn the necessary compositional structure to generalize addition to longer sequences.

This thesis seeks to investigate whether transformers can achieve length generalization in integer addition without resorting to task-specific positional encodings or architectural modifications. The goal is to understand the limitations of standard transformer models and identify principles that enable better generalization by exploring various positional encoding schemes and training datasets, while minimizing the architectural changes to standard unsupervised language modeling with transformers.

\section{Research Questions}\label{sec:research_questions}

The primary goal of this thesis is to investigate the reasons behind the failure of transformer models to generalize integer addition to longer sequences and to understand how different positional encoding schemes and data formatting strategies impact this ability. Specifically, the following research questions are addressed:

\begin{itemize}
    \item \textbf{RQ1: Why do transformer models with standard absolute positional encodings fail to generalize integer addition to sequences longer than those seen during training?}
          
          Focus is to analyze the limitations of absolute positional encodings in facilitating digit alignment and carry propagation over longer sequences. Understanding the fundamental reasons for this failure might inform the development of more flexible positional encoding schemes.
          
    \item \textbf{RQ2: How does the inclusion of sub-task data (carry detection, digit-wise modular addition, reversing, digit alignment) influence the model's compositionality and length generalization capabilities?}
          
          Incorporating sub-task data may help the model learn the underlying algorithmic components of addition. This work explores whether training on sub-tasks enables the model to compose these functions and generalize to longer sequences.
          
    \item \textbf{RQ3: How can mechanistic interpretability techniques be applied to understand the internal representations and failure modes of transformer models in the context of integer addition?}
          
          Mechanistic interpretability methods allow to analyze the learned representations and identify mechanisms that lead to success or failure in length generalization.
          
\end{itemize}

Through exploring these questions, the goal is to gain insights into the limitations of current transformer architecture and positional encoding methods, and to identify principles that could boost length generalization in algorithmic tasks. This thesis focuses on maintaining the standard transformer architecture without introducing task-specific modifications, thereby seeking solutions that are generalizable and applicable to a broader range of problems.

\section{Thesis Structure}

The remainder of this thesis is organized as follows:

\begin{itemize}
    \item \textbf{Chapter \ref{background}} provides background information on transformer model, positional encoding schemes, and mechanistic interpretability.
    \item \textbf{Chapter \ref{related_work}} reviews related work domains of integer addition, length generalization, and reasoning in transformers.
    \item \textbf{Chapter \ref{approach}} describes the experimental approach, including data formatting strategies and training setups used in this research.
    \item \textbf{Chapter \ref{conclusion}} summarizes the findings of the thesis, discusses the implications of the results, and outlines directions for future research.
    \item \textbf{Appendices} include additional experimental results, technical details, and supplementary material relevant to the thesis.
\end{itemize}
